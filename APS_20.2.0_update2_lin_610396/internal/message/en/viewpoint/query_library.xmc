<?xml version="1.0" encoding="utf-8" ?>
<!--

 Copyright Â© 2009-2020 Intel Corporation. All rights reserved.

 The information contained herein is the exclusive property of
 Intel Corporation and may not be disclosed, examined, or reproduced in
 whole or in part without explicit written authorization from the Company.

-->
<xmc version="1.0">
  <catalog name="viewpoint" lang="en">
    <msg name="%RootGrouping">Total</msg>
    <msg name="%AssemblyContent">Assembly</msg>
    <msg name="%BasicBlock">Basic Block</msg>
    <msg name="%BasicBlockSize">BB Size</msg>
    <msg name="%BasicBlockNumInstr">Number of BB Instructions</msg>
    <msg name="%BasicBlockBranchType">BB Branch Type</msg>
    <msg name="%CPI">CPI Rate</msg>
    <msg name="%CPIDescription">Cycles per Instruction Retired, or CPI, is a fundamental performance metric indicating approximately how much time each executed instruction took, in units of cycles.  Modern superscalar processors issue up to four instructions per cycle, suggesting a theoretical best CPI of 0.25. But various effects (long-latency memory, floating-point, or SIMD operations; non-retired instructions due to branch mispredictions; instruction starvation in the front-end) tend to pull the observed CPI up. A CPI &lt; 1 is typical for instruction bound code, while a CPI &gt; 1 may show up for a stall cycle bound application, also likely memory bound. CPI is an excellent metric for judging an overall potential for application performance tuning.</msg>
    <msg name="%IPC">IPC</msg>
    <msg name="%IPCDescription">Instructions Retired per Cycle, or IPC shows average number of retired instructions per cycle.  Modern superscalar processors issue up to four instructions per cycle, suggesting a theoretical best IPC of 4. But various effects (long-latency memory, floating-point, or SIMD operations; non-retired instructions due to branch mispredictions; instruction starvation in the front-end) tend to pull the observed IPC down. A IPC &gt; 1 is typical for instruction bound code, while a IPC &lt; 1 may show up for a stall cycle bound application, also likely memory bound. IPC is an excellent metric for judging an overall potential for application performance tuning.</msg>
    <msg name="%CallCountDescription">Statistical estimation of call counts based on hardware events. Values of this metric are not aggregated per call stack filter mode.</msg>
    <msg name="%ExactCallCountDescription">Call counts based on hardware tracing. Values of this metric are not aggregated per call stack filter mode.</msg>
    <msg name="%TotalIterationCountDescription">Statistical estimation of the total loop iteration count. Values of this metric are not aggregated per call stack filter mode.</msg>
    <msg name="%ExactTotalIterationCountDescription">Total loop iteration count based on hardware tracing. Values of this metric are not aggregated per call stack filter mode.</msg>
    <msg name="%LoopEntryCountDescription">Statistical estimation of the number of times the loop was entered from the outside. Values of this metric are not aggregated per call stack filter mode.</msg>
    <msg name="%AverageTripCountDescription">Statistical estimation of average trip count for the loop.</msg>
    <msg name="%BBExecCountDescription">Statistical estimation of the basic block execution count.</msg>
    <msg name="%KNCCPIDescription">Cycles per Instruction Retired is a fundamental performance metric indicating an average amount of time each instruction took to execute.  It is measured in cycles and calculated as an average across hardware threads.  This view displays CPI for a hardware thread, but CPI per core is also a useful metric and can be calculated as CPI per thread / the number of hardware threads used per core.  The theoretical best CPI per hardware thread is 2.0.  CPIs over 4.0 in a hot function or hot loop may warrant further investigation.  High CPI values usually indicate latency in the system that could be reduced.</msg>
    <msg name="%AtomCPIDescription">Cycles per Instructions Retired is a fundamental performance metric indicating an average amount of time each instruction took to execute, in units of cycles. For this processor, the theoretical best CPI per thread is 0.50, but CPI's over 2.0 warrant investigation.  High CPI values may indicate latency in the system that could be reduced such as long-latency memory, floating-point operations, non-retired instructions due to branch mispredictions, or instruction starvation in the front-end. Beware that some optimizations such as SIMD will use less instructions per cycle (increasing CPI), and debug code can use redundant instructions creating more instructions per cycle (decreasing CPI).</msg>
    <msg name="%GoldmontCPIDescription">Cycles per Instructions Retired is a fundamental performance metric indicating an average amount of time each instruction took to execute, in units of cycles. For this processor the theoretical best CPI per thread is 0.33, but CPI's over 2.0 warrant investigation.  High CPI values may indicate latency in the system that could be reduced such as long-latency memory, floating-point operations, non-retired instructions due to branch mispredictions, or instruction starvation in the front-end. Beware that some optimizations such as SIMD will use less instructions per cycle (increasing CPI), and debug code can use redundant instructions creating more instructions per cycle (decreasing CPI).</msg>
    <msg name="%SoFIA3GCPIDescription">Cycles per Instructions Retired is a fundamental performance metric indicating an average amount of time each instruction took to execute, in units of cycles. For SoFIA 3G platforms, the theoretical best CPI per thread is 4.50, but CPI's over 18.0 warrant investigation.  High CPI values may indicate latency in the system that could be reduced such as long-latency memory, floating-point operations, non-retired instructions due to branch mispredictions, or instruction starvation in the front-end. Beware that some optimizations such as SIMD will use less instructions per cycle (increasing CPI), and debug code can use redundant instructions creating more instructions per cycle (decreasing CPI).</msg>
    <msg name="%SoFIALTECPIDescription">Cycles per Instructions Retired is a fundamental performance metric indicating an average amount of time each instruction took to execute, in units of cycles. For SoFIA LTE platforms, the theoretical best CPI per thread is 1.80, but CPI's over 7.2 warrant investigation.  High CPI values may indicate latency in the system that could be reduced such as long-latency memory, floating-point operations, non-retired instructions due to branch mispredictions, or instruction starvation in the front-end. Beware that some optimizations such as SIMD will use less instructions per cycle (increasing CPI), and debug code can use redundant instructions creating more instructions per cycle (decreasing CPI).</msg>
    <msg name="%CPIIssueText">The CPI may  be too high. This could be caused  by issues such as memory stalls, instruction starvation, branch misprediction or long latency instructions. Explore the other hardware-related metrics to identify what is causing high CPI.</msg>
    <msg name="%IPCIssueText">The IPC may  be too low. This could be caused  by issues such as memory stalls, instruction starvation, branch misprediction or long latency instructions. Explore the other hardware-related metrics to identify what is causing low IPC.</msg>
    <msg name="%MicroarchitectureUsage">Microarchitecture Usage</msg>
    <msg name="%MicroarchitectureUsageDescription">Microarchitecture Usage metric is a key indicator that helps estimate (in %) how effectively your code runs on the current microarchitecture. Microarchitecture Usage can be impacted by long-latency memory, floating-point, or SIMD operations; non-retired instructions due to branch mispredictions; instruction starvation in the front-end.</msg>
    <msg name="%MicroarchitectureUsageIssueText"><![CDATA[<p>You code efficiency on this platform is too low.</p><p><b>Possible cause</b>: memory stalls, instruction starvation, branch misprediction or long latency instructions.</p><p><b>Next steps</b>: Run Microarchitecture Exploration analysis to identify the cause of the low microarchitecture usage efficiency.</p>]]></msg>
    <msg name="%CPUSample">CPU Sample</msg>
    <msg name="%CPUSampleCount">CPU Sample Count</msg>
    <msg name="%CPUIoStatePercentage">CPU State</msg>
    <msg name="%CPUIoState">CPU State</msg>
    <msg name="%CPUMetricConfidenceText">The amount of collected CPU samples is too low to reliably calculate this metric.</msg>
    <msg name="%NotEnoughSamples">The amount of collected samples is too low to reliably calculate this metric.</msg>
    <msg name="%CPUTime">CPU Time</msg>
    <msg name="%CPUTimeDescription">CPU Time is time during which the CPU is actively executing your application.</msg>
    <msg name="%CPUTimePercentage">Percentage of Total CPU Time</msg>
    <msg name="%CPUTimeDiff">CPU Time, sorted by abs. difference</msg>
    <msg name="%CPUUsage">Simultaneously Utilized Logical CPUs</msg>
    <msg name="%CPUUsageUtilization">Utilization</msg>
    <msg name="%CallStack">Call Stack</msg>
    <msg name="%FunctionStack">Function Stack</msg>
    <msg name="%SourceFunctionStack">Source Function Stack</msg>
    <msg name="%AllocStack">Object Allocation Stack</msg>
    <msg name="%CallCount">Estimated Call Count</msg>
    <msg name="%ExactCallCount">Call Count</msg>
    <msg name="%CPUKernelModeTime">Kernel Time</msg>
    <msg name="%CPUKernelModeTimeShort">Kernel</msg>
    <msg name="%CPUKernelModeTimeDescription">Kernel Time is the time during which the CPU is actively executing in the kernel mode.</msg>
    <msg name="%CPUUserModeTime">User Time</msg>
    <msg name="%CPUUserModeTimeShort">User</msg>
    <msg name="%CPUUserModeTimeDescription">User Time is the time during which the CPU is actively executing in the user mode.</msg>
    <msg name="%CPUUnknownTime">Unknown Time</msg>
    <msg name="%CPUUnknownTimeShort">Unknown</msg>
    <msg name="%TotalIterationCount">Total Iteration Count</msg>
    <msg name="%AverageTripCount">Average Loop Trip Count</msg>
    <msg name="%LoopEntryCount">Loop Entry Count</msg>
    <msg name="%EstimatedBBExecCount">Estimated BB Execution Count</msg>
    <msg name="%TypeCsFunctionType">Call Site Type</msg>
    <msg name="%Class">Class</msg>
    <msg name="%Clockticks">Clockticks</msg>
    <msg name="%Concurrency">Concurrency</msg>
    <msg name="%ConcurrencyUtilization">Thread Concurrency</msg>
    <msg name="%ElapsedTime">Elapsed Time</msg>
    <msg name="%ElapsedTimeDescription">Elapsed time is the wall time from the beginning to the end of collection.</msg>
    <msg name="%ProcessElapsedTime">Process Elapsed Time</msg>
    <msg name="%ElapsedTimePerThread">Elapsed Time Per Thread</msg>
    <msg name="%FlatProfileCallees">Callees</msg>
    <msg name="%FlatProfileCallers">Callers</msg>
    <msg name="%Frame">Frame</msg>
    <msg name="%FrameCount">Frame Count</msg>
    <msg name="%FrameDomain">Frame Domain</msg>
    <msg name="%FrameDuration">Frame Duration</msg>
    <msg name="%FrameRate">Frame Rate</msg>
    <msg name="%FrameTime">Frame Time</msg>
    <msg name="%FrameTimeDescription">Frame time is wall time during which frames were active. When applied to individual frames, it reflects the duration of those frames. When applied to a frame domain, it reflects the accumulated time during which frames in that domain were active.</msg>
    <msg name="%FrameType">Frame Duration Type</msg>
    <msg name="%Region">Region Instance</msg>
    <msg name="%RegionCount">Instance Count</msg>
    <msg name="%RegionDomain">OpenMP Region</msg>
    <msg name="%BarrierDomain">OpenMP Barrier-to-Barrier Segment</msg>
    <msg name="%BarrierType">OpenMP Barrier-to-Barrier Segment Type</msg>
    <msg name="%loop">Loop barrier segments</msg>
    <msg name="%single">Single barrier segments</msg>
    <msg name="%reduction">Reduction barrier segments</msg>
    <msg name="%AliasUnknownBarrierType">Other barrier segments</msg>
    <msg name="%BarrierScheduleType">OpenMP Loop Schedule Type</msg>
    <msg name="%static">Static</msg>
    <msg name="%dynamic">Dynamic</msg>
    <msg name="%guided">Guided</msg>
    <msg name="%custom">Custom</msg>
    <msg name="%BarrierChunk">OpenMP Loop Chunk</msg>
    <msg name="%BarrierIterationsCount">OpenMP Loop Iteration Count</msg>
    <msg name="%BarrierIterationsCountMin">Min OpenMP Loop Iteration Count</msg>
    <msg name="%BarrierIterationsCountMax">Max OpenMP Loop Iteration Count</msg>
    <msg name="%BarrierIterationsCountAvg">Avg OpenMP Loop Iteration Count</msg>
    <msg name="%BarrierCount">Instance Count</msg>
    <msg name="%RegionDuration">Duration Type (sec)</msg>
    <msg name="%RegionTime">OpenMP Region Time</msg>
    <msg name="%RegionTimeDescription">OpenMP Region Time is a duration of all the lexical region instances.</msg>
    <msg name="%RegionType">OpenMP Region Duration Type</msg>
    <msg name="%ImbalanceTsc">Imbalance tsc</msg>
    <msg name="%ImbalanceTime">Imbalance</msg>
    <msg name="%ImbalanceTimePercentsElapced">Imbalance (%)</msg>
    <msg name="%ImbalancePotentialGainDescription">OpenMP Potential Gain Imbalance shows maximum elapsed time that could be saved if the OpenMP construct is optimized to have no imbalance. It is calculated as summary of CPU time by all OpenMP threads spinning on a barrier divided by the number of OpenMP threads.</msg>
    <msg name="%ImbalancePotentialGainIssueText">Significant time spent waiting on an OpenMP barrier inside of a parallel region can be a result of load imbalance. Consider using dynamic work scheduling to reduce the imbalance, where possible.</msg>
    <msg name="%ImbalancePotentialGainPercentsElapsedDescription">OpenMP Potential Gain Imbalance shows maximum elapsed time that could be saved if the OpenMP construct is optimized to have no imbalance. It is calculated as summary of CPU time by all OpenMP threads spinning on a barrier divided by the number of OpenMP threads. Percent value is based on Collection Time.</msg>
    <msg name="%SerialTime">Serial Time (outside parallel regions)</msg>
    <msg name="%ParallelTime">Parallel Time</msg>
    <msg name="%SerialTimeShort">Serial Time</msg>
    <msg name="%SerialTimeDescription">Serial Time is the time spent by the application outside any OpenMP region in the master thread during collection. It directly impacts application Collection Time and scaling. High values signal a performance problem to be solved via code parallelization or algorithm tuning.</msg>
    <msg name="%SerialTimePercentElapsedDescription">Serial Time is the wall time spent by the application outside any OpenMP region in the master thread during collection. It directly impacts application Collection Time and scaling. High values signal a performance problem to be solved via code parallelization or algorithm tuning. Percent value is based on Collection Time.</msg>
    <msg name="%SerialTimeIssueText">Serial Time of your application is high. It directly impacts application Elapsed Time and scalability. Explore options for parallelization, algorithm or microarchitecture tuning of the serial part of the application.</msg>
    <msg name="%ParallelExecutionWallTime">Parallel Region Time</msg>
    <msg name="%ParallelExecutionWallTimeDescription">Parallel Region Time is the total duration for all instances of all lexical parallel regions.</msg>
    <msg name="%ParallelExecutionWallTimePercentElapsedDescription">Parallel Region Time is the total duration for all instances of all lexical parallel regions. Percent value is based on Collection Time.</msg>
    <msg name="%IdealRegionTime">Estimated Ideal Time</msg>
    <msg name="%IdealSerialTime">Effective Time</msg>
    <msg name="%IdealRegionTimeDescription">Ideal Time is the estimated time for all parallel regions potentially load-balanced with zero OpenMP runtime overhead according to the formula: Total User CPU time in all regions/Number of OpenMP threads.</msg>
    <msg name="%IdealRegionTimePercentElapsedDescription">Ideal Time is the estimated time for all parallel regions potentially load-balanced with zero OpenMP runtime overhead according to the formula: Total User CPU time in all regions/Number of OpenMP threads. Percent value is based on Collection Time.</msg>
    <msg name="%RegionPotentialGain">OpenMP Potential Gain</msg>
    <msg name="%RegionPotentialGainShort">OpenMP Potential Gain</msg>
    <msg name="%RegionPotentialGainDescription">Potential Gain shows the maximum time that could be saved if the OpenMP region is optimized to have no load imbalance assuming no runtime overhead (Parallel Region Time minus Region Ideal Time). If the Potential Gain is large, make sure the workload for this region is enough and the loop schedule is optimal.</msg>
    <msg name="%RegionPotentialGainPercentElapsedDescription">Potential Gain shows the maximum time that could be saved if the OpenMP region is optimized to have no load imbalance assuming no runtime overhead (Parallel Region Time minus Region Ideal Time). If the Potential Gain is large, make sure the workload for this region is enough and the loop schedule is optimal. Percent value is based on Collection Time.</msg>
    <msg name="%RegionPotentialGainPercent">OpenMP Potential Gain (% of Collection Time)</msg>
    <msg name="%RegionPotentialGainPercentShort">&#32;&#32;(%)</msg>
    <msg name="%SerialTimePercentShort">&#32;&#32;&#32;(%)</msg>
    <msg name="%RegionPotentialGainPercentDescription">Potential Gain percent of Collection Time per parallel region.</msg>
    <msg name="%RegionPotentialGainForSummaryDescription">Potential Gain shows the maximum time that could be saved if all OpenMP regions were optimized to have no load imbalance assuming no runtime overhead (Collection Time minus Ideal Time). If the Potential Gain is large, consider tuning top OpenMP regions based on the metric values in the table below.</msg>
    <msg name="%RegionPotentialGainForSummaryIssueText">The time wasted on load imbalance or parallel work arrangement is significant and negatively impacts the application performance and scalability. Explore OpenMP regions with the highest metric values. Make sure the workload of the regions is enough and the loop schedule is optimal.</msg>
    <msg name='%OpenMPSectionHeader'>OpenMP Analysis. Collection Time</msg>
    <msg name='%OpenMPSectionHeaderDescription'>Collection Time is wall time from the beginning to the end of collection, excluding Paused Time.</msg>
    <msg name='%UserTime'>Effective Time</msg>
    <msg name='%UserTimeDescription'>Effective Time is CPU time spent in the user code. This metric does not include Spin and Overhead time.</msg>
    <msg name='%UserTimeInsideRegions'>Effective Time inside OpenMP regions</msg>
    <msg name="%OpenMPThreadCount">Number of OpenMP threads</msg>
    <msg name='%OpenMPThreadCountCumulative'>Total number of OpenMP threads for all instances</msg>
    <msg name='%OpenMPThreadCountDescription'>Number of active OpenMP worker threads for this lexical region including OpenMP master thread.</msg>
    <msg name="%GlobalElapsedTime">Elapsed Time</msg>
    <msg name="%Function">Function</msg>
    <msg name="%FunctionType">Function Type</msg>
    <msg name="%FunctionFull">Function (Full)</msg>
    <msg name="%FunctionRange">Function Range</msg>
    <msg name="%FunctionRangeSize">Function Range Size</msg>
    <msg name="%FunctionStartAddress">Start Address</msg>
    <msg name="%Executor">Work Executor</msg>
    <msg name="%HWContext">Logical Core</msg>
    <msg name="%InstructionsRetired">Instructions Retired</msg>
    <msg name="%PTInstructions">Instructions Traced</msg>
    <msg name="%PTBranches">Branches Traced</msg>
    <msg name="%Latency">Latency</msg>
    <msg name="%MarksGlobal">User Marks</msg>
    <msg name="%Module">Module</msg>
    <msg name="%FunctionModule">Module</msg>
    <msg name="%ModulePath">Module Path</msg>
    <msg name="%VectInstSet">Vector Instruction Set</msg>
    <msg name="%VectInstSetDescription">Displays using Vector Instruction Set</msg>
    <msg name="%VectInstClass">Vectorization Traits</msg>
    <msg name="%VectWidths">Vector Widths</msg>
    <msg name="%VectDataTypes">Vector Data Types</msg>
    <msg name="%LoopCharacterization">Loop Characterization</msg>
    <msg name="%LoopCharacterizationDescription">Displays a loop type based on the Intel compiler optreport.</msg>
    <msg name="%Overhead">Overhead</msg>
    <msg name="%SystemOverhead">System Overhead</msg>
    <msg name="%OverheadTime">Overhead Time</msg>
    <msg name="%OverheadAndSpinTime">Overhead and Spin Time</msg>
    <msg name="%SpinAndOverheadTime">Spin and Overhead Time</msg>
    <msg name="%SpinAndOverheadTimePercentage">(% from CPU Time)</msg>
    <msg name="%OverheadTimeDescription">Overhead time is CPU time spent on the overhead of known synchronization and threading libraries, such as system synchronization APIs, Intel TBB, and OpenMP.</msg>
    <msg name="%OverheadTimeIssueText">A significant portion of CPU time is spent in synchronization or threading overhead. Consider increasing task granularity or the scope of data synchronization.</msg>
    <msg name="%ThreadOversubscription">Thread Oversubscription</msg>
    <msg name="%ThreadOversubscriptionIssueText">Significant amount of time application spent in thread oversubscription. This can negatively impact parallel performance because of thread preemption and context switch cost.</msg>
    <msg name="%ThreadOversubscriptionWithSignificantWaitTimeIssueText">The number of threads that increases the number of logical cores on the system leads to thread oversubscription and significant wait time because of thread preemption. Consider decreasing the thread number corresponding to the number of logical cores.</msg>
    <msg name="%ThreadOversubscriptionDescription">Thread Oversubscription indicates time spent in the code with the number of simultaneously working threads more than the number of available logical cores on the system.</msg>
    <msg name="%ThreadUndersubscriptionIssueText">The number of threads in the application is significantly lower than the number of logical cores on the machine. Check if it is resulted by thread number hard coding that limits application scalability.</msg>
    <msg name="%OverheadWorkForkingTime">Creation</msg>
    <msg name="%OverheadWorkForkingWallTime">Creation</msg>
    <msg name="%OverheadWorkForkingTimePercentsElapsed">Creation (%)</msg>
    <msg name="%OverheadWorkForkingTimeDescription">Creation time is CPU time that a runtime library spends on organizing parallel work.</msg>
    <msg name="%OverheadWorkForkingWallTimeDescription">OpenMP Potential Gain Creation shows elapsed time cost of parallel work arrangement by OpenMP runtime library. If the time is significant consider parallelizing outer looks rather than inner loops where possible.</msg>
    <msg name="%OverheadWorkForkingWallTimePercentsElapsedDescription">OpenMP Potential Gain Creation shows elapsed time cost of parallel work arrangement by OpenMP runtime library. If the time is significant consider parallelizing outer looks rather than inner loops where possible. Percent value is based on Collection Time.</msg>
    <msg name="%OverheadWorkForkingTimeIssueText">CPU time spent on parallel work arrangement can be a result of too fine-grain parallelism. Try parallelizing outer loops, rather than inner loops, to reduce the work arrangement overhead.</msg>
    <msg name="%OverheadWorkSchedulingTime">Scheduling</msg>
    <msg name="%OverheadWorkSchedulingWallTime">Scheduling</msg>
    <msg name="%OverheadWorkSchedulingTimePercentsElapsed">Scheduling (%)</msg>
    <msg name="%OverheadWorkSchedulingTimeDescription">Scheduling time is CPU time that a runtime library spends on work assignment for threads. If the time is significant, consider using coarse-grain work chunking.</msg>
    <msg name="%OverheadWorkSchedulingWallTimeDescription">OpenMP Potential Gain Scheduling shows elapsed time cost of work assignment for threads by OpenMP runtime library. If the time is significant, consider using coarse-grain work chunking.</msg>
    <msg name="%OverheadWorkSchedulingWallTimePercentsElapsedDescription">OpenMP Potential Gain Scheduling shows elapsed time cost of work assignment for threads by OpenMP runtime library. If the time is significant, consider using coarse-grain work chunking. Percent value is based on Collection Time.</msg>
    <msg name="%OverheadWorkSchedulingTimeIssueText">The thread scheduling threading runtime function consumed a significant amount of CPU time. This occurs when the threads are frequently returning to the scheduler for more work, which can indicate an inefficient work chunk size. To reduce scheduling overhead, increase the size of tasks or iteration chunks being performed by working threads.</msg>
    <msg name="%OverheadReductionTime">Reduction</msg>
    <msg name="%OverheadReductionWallTime">Reduction</msg>
    <msg name="%OverheadReductionTimePercentsElapsed">Reduction (%)</msg>
    <msg name="%OverheadReductionTimeDescription">Reduction time is CPU time that a runtime library spends on loop or region reduction operations.</msg>
    <msg name="%OverheadReductionWallTimeDescription">OpenMP Potential Gain Reduction shows elapsed time cost of loop or region reduction operations.</msg>
    <msg name="%OverheadReductionWallTimePercentsElapsedDescription">OpenMP Potential Gain Reduction shows elapsed time cost of loop or region reduction operations. Percent value is based on Collection Time.</msg>
    <msg name="%OverheadReductionTimeIssueText">A significant portion of CPU time is spent on doing reduction.</msg>
    <msg name="%OverheadAtomicsTime">Atomics</msg>
    <msg name="%OverheadAtomicsWallTime">Atomics</msg>
    <msg name="%OverheadAtomicsTimePercentsElapsed">Atomics (%)</msg>
    <msg name="%OverheadAtomicsTimeDescription">Atomics time is CPU time that a runtime library spends on atomic operations.</msg>
    <msg name="%OverheadAtomicsWallTimeDescription">OpenMP Atomics Potential Gain shows elapsed time cost of atomic operations executed by OpenMP runtime library.</msg>
    <msg name="%OverheadAtomicsWallTimePercentsElapsedDescription">OpenMP Atomics Potential Gain shows elapsed time cost of atomic operations executed by OpenMP runtime library. Percent value is based on Collection Time.</msg>
    <msg name="%OverheadAtomicsTimeIssueText">CPU time spent on atomic operations is significant. Consider using reduction operations where possible.</msg>
    <msg name="%OverheadOMPTaskingTime">Tasking (OpenMP)</msg>
    <msg name="%OverheadOMPTaskingWallTime">Tasking</msg>
    <msg name="%OverheadOMPTaskingTimePercentsElapsed">Tasking (%)</msg>
    <msg name="%OverheadOMPTaskingWallTimeDescription">OpenMP Tasking Potential Gain shows elapsed time cost of allocating and completing the tasks executed by the OpenMP runtime library.</msg>
    <msg name="%OverheadOMPTaskingWallTimePercentsElapsedDescription">OpenMP Tasking Potential Gain shows elapsed time cost of allocating and completeing a task executed by the OpenMP runtime library. Percent value is based on the Collection Time.</msg>
    <msg name="%OverheadOMPTaskingTimeDescription">Tasking time is CPU time that an OpenMP runtime library spends on allocating and completing tasks.</msg>
    <msg name="%OverheadOMPTaskingTimeIssueText">CPU time spent on allocating and completing a task is significant. This can be a result of too fine-grain tasks. Consider increasing the task granularity to avoid this kind of overhead.</msg>
    <msg name="%OverheadOtherTime">Other</msg>
    <msg name="%OverheadOtherTimePercentsElapsed">Other (%)</msg>
    <msg name="%OverheadOtherTimeDescription">This metric shows unclassified Overhead time spent in a threading runtime library.</msg>
    <msg name="%SpinOverheadOtherTimeDescription">OpenMP Potential Gain Other shows elapsed time cost of unclassified Spin and Overhead time spent in the OpenMP runtime library.</msg>
    <msg name="%SpinOverheadOtherTimePercentsElapsedDescription">OpenMP Potential Gain Other shows elapsed time cost of unclassified Spin and Overhead time spent in the OpenMP runtime library. Percent value is based on Collection Time.</msg>
    <msg name="%SpinBusyWaitOnBarrierTime">Imbalance or Serial Spinning</msg>
    <msg name="%SpinBusyWaitOnBarrierWallTime">Imbalance or Serial Spinning</msg>
    <msg name="%SpinBusyWaitOnBarrierTimeDescription">Imbalance or Serial Spinnig time is CPU time when working threads are spinning on a synchronization barrier consuming CPU resources. This can be caused by load imbalance, insufficient concurrency for all working threads or waits on a barrier in the case of serialized execution.</msg>
    <msg name="%SpinBusyWaitOnBarrierWallTimeDescription">Imbalance or Serial Spinnig time is wall time when working threads are spinning on a synchronization barrier consuming CPU resources. This can be caused by load imbalance, insufficient concurrency for all working threads or waits on a barrier in the case of serialized execution.</msg>
    <msg name="%SpinBusyWaitOnBarrierTimeIssueText">The threading runtime function related to time spent on imbalance or serial spinning consumed a significant amount of CPU time. This can be caused by a load imbalance, insufficient concurrency for all working threads, or busy waits of worker threads while serial code is executed. If there is an imbalance, apply dynamic work scheduling or reduce the size of work chunks or tasks. If there is insufficient concurrency, consider collapsing the outer and inner loops. If there is a wait for completion of serial code, explore options for parallelization with Intel Advisor, algorithm, or microarchitecture tuning of the application's serial code with PRODUCT_LEGAL_SHORT_NAME Basic Hotspots or Microarchitecture Exploration analysis respectively. For OpenMP* applications, use the Per-Barrier OpenMP Potential Gain metric set in the HPC Performance Characterization analysis to discover the reason for high imbalance or serial spin time.</msg>
    <msg name="%SpinBusyWaitOnLockTime">Lock Contention</msg>
    <msg name="%SpinBusyWaitOnLockWallTime">Lock Contention</msg>
    <msg name="%SpinBusyWaitOnLockTimePercentsElapsed">Lock Contention (%)</msg>
    <msg name="%SpinBusyWaitOnLockTimeDescription">Lock Contention time is CPU time when working threads are spinning on a lock consuming CPU resources. High metric value may signal inefficient parallelization with highly contended synchronization objects. To avoid intensive synchronization, consider using reduction, atomic operations or thread local variables where possible.</msg>
    <msg name="%SpinBusyWaitOnLockWallTimeDescription">OpenMP Potential Gain Lock Contention shows elapsed time cost of  OpenMP locks and ordered synchronization. High metric value may signal inefficient parallelization with highly contended synchronization objects. To avoid intensive synchronization, consider using reduction, atomic operations or thread local variables where possible. This metric is based on the CPU sampling and does not include passive waits.</msg>
    <msg name="%SpinBusyWaitOnLockWallTimePercentsElapsedDescription">OpenMP Potential Gain Lock Contention shows elapsed time cost of  OpenMP locks and ordered synchronization. High metric value may signal inefficient parallelization with highly contended synchronization objects. To avoid intensive synchronization, consider using reduction, atomic operations or thread local variables where possible. This metric is based on the CPU sampling and does not include passive waits. Percent value is based on Collection Time.</msg>
    <msg name="%SpinBusyWaitOnLockTimeIssueText">When synchronization objects are used inside a parallel region, threads can spend CPU time waiting on a lock release, contending with other threads for a shared resource. Where possible, reduce synchronization by using reduction or atomic operations, or minimize the amount of code executed inside the critical section.</msg>
    <msg name="%SpinBusyWaitOnMPISpinningTime">Communication (MPI)</msg>
    <msg name="%SpinBusyWaitOnMPISpinningTime2">MPI Busy Wait Time</msg>
    <msg name="%SpinBusyWaitOnMPISpinningTimeForTimeline">MPI Busy Wait Time</msg>
    <msg name="%SpinBusyWaitOnMPISpinningTimeDescription">MPI Busy Wait Time is CPU time when MPI runtime library is spinning on waits in communication operations. High metric value can be caused by load imbalance between ranks, active communications or nonoptimal settings of MPI library. Explore details on communication inefficiencies with Intel Trace Analyzer and Collector.</msg>
    <msg name="%SpinBusyWaitOnMPISpinningTimeIssueText">CPU time spent on waits for MPI communication operations is significant and can negatively impact the application performance and scalability. This can be caused by load imbalance between ranks, active communications or non-optimal settings of MPI library. Explore details on communication inefficiencies with Intel Trace Analyzer and Collector.</msg>
    <msg name="%SpinBusyWaitOnMPISpinningTimePercentElapsed">&#32;&#32;&#32;&#32;(%)</msg>
    <msg name="%SpinOtherTime">Other</msg>
    <msg name="%SpinOtherTimeDescription">This metric shows unclassified Spin time spent in a threading runtime library.</msg>
    <msg name="%TurboFreqRate">CPU Frequency Ratio</msg>
    <msg name="%TurboFreqRateDescription">The ratio between the actual and the nominal CPU frequencies. Values above 1.0 indicate that CPU is operating in a turbo boost mode.</msg>
    <msg name="%AverageFrequency">Average CPU Frequency</msg>
    <msg name="%AverageFrequencyDescription">Average actual CPU frequency. Values above nominal frequency indicate that the CPU is operating in a turbo boost mode.</msg>
    <msg name="%PMUDataOfInterest">Hardware Event Data Of Interest</msg>
    <msg name="%PMUEventCount">Hardware Event Count</msg>
    <msg name="%PMUEventType">Hardware Event Type</msg>
    <msg name="%PMUEventsPerSample">Events Per Sample</msg>
    <msg name="%PMUEventIsPrecise">Precise</msg>
    <msg name="%PMUHotspot">PMU Hotspot</msg>
    <msg name="%PMUHotspotDescription"></msg>
    <msg name="%PMUSample">Hardware Event Sample</msg>
    <msg name="%PMUSampleCount">Hardware Event Sample Count</msg>
    <msg name="%PackageFrequency">Frequency</msg>
    <msg name="%Package">Package</msg>
    <msg name="%ParentCallStack">Call Stack</msg>
    <msg name="%PausedTime">Paused Time</msg>
    <msg name="%PausedTimeDescription">Paused time is the amount of Elapsed time during which the analysis was paused using either the GUI, CLI commands, or user API.</msg>
    <msg name="%Process">Process</msg>
    <msg name="%ProcessID">PID</msg>
    <msg name="%Thread">Thread</msg>
    <msg name="%ThreadID">TID</msg>
    <msg name="%ThreadAffinity">Affinity</msg>
    <msg name="%VM">VM</msg>
    <msg name="%RVA">Code Location</msg>
    <msg name="%InternalAddress">Address</msg>
    <msg name="%RefTime_">CPU Time</msg>
    <msg name="%RefTimeDescription">CPU Time is time during which the CPU is actively executing your application.</msg>
    <msg name="%Region">Region</msg>
    <msg name="%RegionDomain">Region Domain</msg>
    <msg name="%RunnableTime">Runnable Time</msg>
    <msg name="%SchedTime">Context Switch Time</msg>
    <msg name="%SourceContent">Source</msg>
    <msg name="%ParentSourceFileAndLine" meta="unofficial">Source File and Line</msg>
    <msg name="%SourceFile">Source File</msg>
    <msg name="%SourceFilePath">Source File Path</msg>
    <msg name="%SourceFunction">Source Function</msg>
    <msg name="%SourceFunctionFull">Source Function (Full)</msg>
    <msg name="%SourceLine">Source Line</msg>
    <msg name="%SourceColumn">Source Column</msg>
    <msg name="%GPUSourceLine">Compute Task Source Line</msg>
    <msg name="%SpinTime">Spin Time</msg>
    <msg name="%SpinTimeDescription">Spin time is Wait Time during which the CPU is busy.  This often occurs when a synchronization API causes the CPU to poll while the software thread is waiting. Some Spin Time may be preferable to the alternative of increased thread context switches. Too much Spin Time, however, can reflect lost opportunity for productive work.</msg>
    <msg name="%SpinTimeIssueText">A significant portion of CPU time is spent waiting. Use this metric to discover which synchronizations are spinning. Consider adjusting spin wait parameters, changing the lock implementation (for example, by backing off then descheduling), or adjusting the synchronization granularity.</msg>
    <msg name="%Sync">Synchronization</msg>
    <msg name="%TargetConcurrency">Target Concurrency</msg>
    <msg name="%TargetUtilization">Target Utilization</msg>
    <msg name="%Time_">CPU Time</msg>
    <msg name="%TotalElapsedTime">Elapsed Time</msg>
    <msg name="%TotalThreadCount">Total Thread Count</msg>
    <msg name="%LogicalCPUCount">Logical CPU Count</msg>
    <msg name="%PhysicalCoreCount">Physical Core Count</msg>
    <msg name="%OsName">Operating System</msg>
    <msg name="%OsDetailedName">OS Detailed Name</msg>
    <msg name="%OsKernelRelease">OS Kernel Release</msg>
    <msg name="%MpiRank">MPI Process Rank</msg>
    <msg name="%CommandLine">Application Command Line</msg>
    <msg name="%CollectorTypeDecription">Collector Type</msg>
    <msg name="%EnvironmentVars">Environment Variables</msg>
    <msg name="%ComputerName">Computer Name</msg>
    <msg name="%ProcessorName">CPU Name</msg>
    <msg name="%ResultSize">Result Size</msg>
    <msg name="%ReferenceFrequency">Frequency</msg>
    <msg name="%CollectionStart">Collection start time</msg>
    <msg name="%CollectionStop">Collection stop time</msg>
    <msg name="%UserName">User Name</msg>
    <msg name="%UnusedCPU">Unused CPU</msg>
    <msg name="%UserTasks">User Tasks</msg>
    <msg name="%WaitCount">Wait Count</msg>
    <msg name="%WaitCountDescription">Wait Count measures the number of times software threads wait due to APIs that block or cause synchronization.</msg>
    <msg name="%WaitModuleFunction">Wait Module and Function</msg>
    <msg name="%WaitSourceFileLine">Wait File and Line</msg>
    <msg name="%WaitSignalCallStack">Signal Call Stack</msg>
    <msg name="%WaitSignalParentCallStack">Signal Parent Call Stack</msg>
    <msg name="%WaitSignalRVA">Signal Code Location</msg>
    <msg name="%WaitSignalSourceFile">Signal Source File</msg>
    <msg name="%WaitSignalSourceLine">Signal Source Line</msg>
    <msg name="%WaitSyncObj">Sync Object</msg>
    <msg name="%WaitSyncObjCreation">Object Creation</msg>
    <msg name="%WaitSyncObjCreationFunction">Object Creation Function</msg>
    <msg name="%WaitSyncObjCreationModule">Object Creation Module</msg>
    <msg name="%WaitSyncObjCreationRVA">Object Creation RVA</msg>
    <msg name="%WaitSyncObjCreationSourceFile">Object Creation File</msg>
    <msg name="%WaitSyncObjCreationSourceLine">Object Creation Line</msg>
    <msg name="%WaitSyncObjCreationModuleFunction">Object Creation Module and Function</msg>
    <msg name="%WaitSyncObjCreationSourceFileLine">Object Creation File and Line</msg>
    <msg name="%WaitSyncObjType">Object Type</msg>
    <msg name="%WaitThreadState">Wait Thread State</msg>
    <msg name="%WaitTime">Wait Time</msg>
    <msg name="%WaitTimeDescription">Wait Time occurs when software threads are waiting due to APIs that block or cause synchronization. Wait Time is per-thread, therefore the total Wait Time can exceed the application Elapsed Time.</msg>
    <msg name="%Waits">Waits</msg>
    <msg name="%WaitSignal">Signal</msg>
    <msg name='%CounterCount'>Thread Counter</msg>
    <msg name='%GlobalCounterCount'>Global Counter</msg>
    <msg name='%CounterMetrics'>Counter Rate</msg>
    <msg name='%InstantValue'>Instant Value</msg>
    <msg name='%GlobalCounterCountMetrics'>Metric Count</msg>
    <msg name='%CounterDomain'>Counter Domain</msg>
    <msg name='%CounterType'>Counter Type</msg>
    <msg name='%CounterCallStack'>Counter Call Stack</msg>
    <msg name='%GlobalInstantValueCount'>Global Instant Value Count</msg>
    <msg name='%ThreadInstantValueCount'>Instant Value Count</msg>
    <msg name='%DeviceCounterCount'>Device Counter</msg>
    <msg name='%DeviceInstantValueCount'>Device Instant Counter</msg>
    <msg name='%DeviceInstantValue'>Device Instant Value</msg>
    <msg name='%AliasUnknownClass'>[Not part of any known object class]</msg>
    <msg name='%AliasUnknownFrame'>[Outside any frame]</msg>
    <msg name='%AliasUnknownFrameType'>[No frame type - Outside any frame]</msg>
    <msg name='%AliasUnknownFrameDomain'>[No frame domain - Outside any frame]</msg>
    <msg name='%AliasUnknownRegion'>[Outside any region]</msg>
    <msg name='%AliasUnknownBarrier'>[Outside any barrier-to-barrier segment]</msg>
    <msg name='%AliasUnknownRegionType'>[No region type - Outside any region]</msg>
    <msg name='%AliasUnknownRegionDomain'>[Serial - outside parallel regions]</msg>
    <msg name='%AliasUnknownTaskType'>[Outside any task]</msg>
    <msg name='%AliasUnknownInterrupt'>[Outside any interrupt handler]</msg>
    <msg name='%AliasUnknownCallStack'>[No call stack information]</msg>
    <msg name='%AliasUnknownSourceFile'>[Unknown source file]</msg>
    <msg name='%AliasUnknownPacketType'>[Outside any packet]</msg>
    <msg name='%AliasUnknownModule'>[Outside any module]</msg>
    <msg name='%DataTransferred'>Data Transferred</msg>
    <msg name='%GPUComputeMemoryTransferSizeOut'>Data Transferred: Out</msg>
    <msg name='%GPUComputeMemoryTransferSizeIn'>Data Transferred: In</msg>
    <msg name='%GPUComputeMemoryTransferSizeTotal'>Data Transferred: Total</msg>
    <msg name='%GPUComputeBufferId'>Buffer ID</msg>
    <msg name='%DataTransferredGB'>Data Transferred, GB</msg>
    <msg name='%DataReadGB'>Data Read, GB</msg>
    <msg name='%NonDataReadGB'>Non Data Read, GB</msg>
    <msg name='%UncoreEventType'>Uncore Event Type</msg>
    <msg name='%UncoreEventCount'>Uncore Event Count</msg>
    <msg name='%UncoreEventCountMetrics'>Metric Count</msg>
    <msg name='%DataWrittenGB'>Data Written, GB</msg>
    <msg name='%NonDataWrittenGB'>Non Data Written, GB</msg>
    <msg name='%DataTransferredMB'>Data Transferred, MB</msg>
    <msg name='%DataReadMB'>Data Read, MB</msg>
    <msg name='%DataWrittenMB'>Data Written, MB</msg>
    <msg name='%DRAMTotalGB'>DRAM Data Transferred, GB</msg>
    <msg name='%PCIETotalMB'>PCIe Data Transferred, MB</msg>
    <msg name='%BandwidthMBperSec'>Average Bandwidth, MB/sec</msg>
    <msg name='%ReadBandwidthMBperSec'>Average Bandwidth Read, MB/sec</msg>
    <msg name='%DataTxGB'>Interconnect Data Transferred, GB</msg>
    <msg name='%QPITotalBandwidthGBperSec'>Total Interconnect Transmit Bandwidth, GB/sec</msg>
    <msg name="%SOCMod0BusAgentBandwidth">Module 0 Bus Agent Bandwidth, MB/s</msg>
    <msg name="%SOCMod1BusAgentBandwidth">Module 1 Bus Agent Bandwidth, MB/s</msg>
    <msg name="%SOCMod2BusAgentBandwidth">Module 2 Bus Agent Bandwidth, MB/s</msg>
    <msg name="%SOCMod3BusAgentBandwidth">Module 3 Bus Agent Bandwidth, MB/s</msg>
    <msg name="%SOCGraphicsBandwidth">Graphics Controller Bandwidth, MB/s</msg>
    <msg name="%SOCImagingBandwidth">Imaging Controller Bandwidth, MB/s</msg>
    <msg name="%SOCDisplayBandwidth">Display Controller Bandwidth, MB/s</msg>
    <msg name="%SOCVEDBandwidth">Video Encode/Decode Controller Bandwidth, MB/s</msg>
    <msg name="%SOCIOBandwidth">I/O Bandwidth, MB/s</msg>
    <msg name="%SOCHighSpeedPFBandwidth">High Speed Peripheral Fabric Bandwidth, MB/s</msg>
    <msg name="%SOCSystemAgentBandwidth">System Agent Bandwidth, MB/s</msg>
    <msg name="%TID">ThreadID</msg>
    <msg name="%PID">ProcessID</msg>
    <msg name="%Core">Physical Core</msg>
    <msg name="%CoreType">Core Type</msg>
    <msg name='%CPUCore'>CPU Core</msg>
    <msg name='%OSCore'>OS Core</msg>
    <msg name='%HWCore'>HW Core</msg>
    <msg name='%CoreFrequency'>Core Frequency</msg>
    <msg name='%CPUFrequency'>CPU Frequency</msg>
    <msg name='%GPUFrequency'>GPU Frequency</msg>
    <msg name='%ModuleFrequency'>Module Frequency</msg>
    <msg name='%CStateCount'>Wake-up Count</msg>
    <msg name='%CStateWakeUpName'>Wake-ups</msg>
    <msg name='%CStateWakeUpCount'>Total Wake-up Count</msg>
    <msg name='%CStateWakeUpCountDescription'>Total number of core wake-ups over all cores.</msg>
    <msg name='%CStateWakeUpCountPercent'>Wake-ups</msg>
    <msg name='%CStateWakeUpCountPercentDescription'>Percentage of core wake-ups over all cores.</msg>
    <msg name='%CStateTime'>C-States Time</msg>
    <msg name='%CoreCStateTime'>Core C-States Time</msg>
    <msg name='%HWCStateTime'>HW Core C-States Time</msg>
    <msg name='%HWCState'>Core Sleep State</msg>
    <msg name='%PackageCStateTime'>Package C-States Time</msg>
    <msg name='%ModuleCStateTime'>Module C-States Time</msg>
    <msg name='%GfxCStateTime'>Graphics C-States Time</msg>
    <msg name='%GfxActiveTime'>Graphics Active Time</msg>
    <msg name='%VmSwitch'>VM Switch</msg>
    <msg name='%VmSwitchTime'>VM Switch Time</msg>
    <msg name='%VmSwitchOs'>VM Switch Os</msg>
    <msg name='%VmSwitchOsVMM'>VMM</msg>
    <msg name='%VmSwitchOsModem'>Modem</msg>
    <msg name='%VmSwitchOsAndroid'>Android</msg>
    <msg name='%VmSwitchOsSecVM'>SecVM</msg>
    <msg name='%VmSwitchExitCause'>Exit Cause</msg>
    <msg name='%VmSwitchVcpuIdName'>VCPU ID</msg>
    <msg name='%CStateTimeDescription'>Time spent in core states C0-Cx.</msg>
    <msg name='%CStateWakeUpReason'>Wake-up Reason</msg>
    <msg name='%CStateWakeUpObjectName'>Wake-up Object</msg>
    <msg name='%CStateTimerType'>Timer Type</msg>
    <msg name='%CStateTimerCallStack'>Timer Source</msg>
    <msg name='%CStateProcessName'>Process Name</msg>
    <msg name='%CStateModule'>Module Name</msg>
    <msg name='%CStateTimerCallSite'>Timer Call Site</msg>
    <msg name='%CStateTransition'>C-state Transition</msg>
    <msg name='%CStateTransitionCause'>C-state Transition Cause</msg>
    <msg name='%ModulePState'>Module Frequency</msg>
    <msg name='%PStateCount'>Frequency Transition Count</msg>
    <msg name='%PStateTime'>Core P-States Time</msg>
    <msg name='%ModulePStateTime'>Module P-States Time</msg>
    <msg name='%PStateTimeDescription'>Time spent in different frequency modes.</msg>
    <msg name='%ModulePStateTimeDescription'>Module time spent in different frequency modes.</msg>
    <msg name='%PStateTimePercent'>Time (%)</msg>
    <msg name='%PStateTimePercentDescription'>Percentage of time spent in different frequency modes.</msg>
    <msg name='%PStateFreq'>Core Frequency</msg>
    <msg name='%ModulePStateFreq'>Module Frequency</msg>
    <msg name='%GPXPStateFreq'>Graphics Frequency</msg>
    <msg name='%PStateType'>Frequency Type</msg>
    <msg name='%MinMaxRequester'>Mininum or Maximum Policy OMP Requester</msg>
    <msg name='%MinRequester'>Min Policy OMP Requester</msg>
    <msg name='%MaxRequester'>Max Policy OMP Requester</msg>
    <msg name='%CPU'>CPU</msg>
    <msg name='%TotalTimeNotInC0'>Total Time in Non-C0 States</msg>
    <msg name='%TotalTimeNotInC0Description'>Total time in sleep states C1-Cx over all cores.</msg>
    <msg name='%TotalTimeInC0'>Total Time in C0 State</msg>
    <msg name='%TotalTimeInC0Description'>Total time spent in the active C0 state over all cores.</msg>
    <msg name='%WakeUpsPerSecond'>Wake-ups/sec</msg>
    <msg name='%WakeUpsPerSecondPerCore'>Wake-ups/sec per Core</msg>
    <msg name='%WakeUpsPerSecondPerCPU'>Wake-ups/sec per CPU</msg>
    <msg name='%WakeUpsPerSecondDescription'>Rate of wake-ups.</msg>
    <msg name='%CStateIRQNum'>IRQ</msg>
    <msg name='%AliasUnknownTimerType'>[Non-timer wake-ups]</msg>
    <msg name='%TimerResolutionInfo'>Timer Resolution</msg>
    <msg name='%TimerResolutionTime'>Time</msg>
    <msg name='%TimerResolutionTimeDescription'>Time spent in different timer resolution.</msg>
    <msg name='%TimerResolutionRequestAppName'>Application</msg>
    <msg name='%TimerResolutionRequest'>Requested Timer Resolution</msg>
    <msg name='%DeviceCStateTimeDescription'>Time spent in different sleep states.</msg>
    <msg name='%ThermalTime'>Device Thermal Time</msg>
    <msg name='%CPUInactiveTime'>CPU Inactive</msg>
    <msg name='%VCPUUtilization'>Virtual Core Utilization (%)</msg>
    <msg name='%VCPUInactiveTime'>Virtual Core Inactive</msg>
    <msg name='%VCPUActiveTime'>Virtual Core Active</msg>
    <msg name='%VCoreWakeUpsPerSecond'>Wakeups per second</msg>
    <msg name='%VCoreWakeUpsPerSecondPerVCore'>Wakeups per second per Virtual Core</msg>
    <msg name='%SStateWakeUpCount'>S-State Wakeup Count</msg>
    <msg name='%IstpCStateWakeUpCount'>C-State Wakeup Count</msg>
    <msg name='%IstpCState'>C-State</msg>
    <msg name='%IstpHWSignalState'>Value</msg>
    <msg name='%IstpHWSignalName'>Name</msg>
    <msg name='%IstpHWSignalInstanceCount'>Count</msg>
    <msg name='%MonitoringSignals'>Monitoring Signals</msg>
    <msg name="%Task">Task</msg>
    <msg name="%TaskDomain">Task Domain</msg>
    <msg name="%TaskEndCallStack">Task End Call Stack</msg>
    <msg name="%TaskRegion">Task Region</msg>
    <msg name="%TaskRegionDomain">Task Region Domain</msg>
    <msg name="%TaskType">Task Type</msg>
    <msg name="%TaskTime">Task Time</msg>
    <msg name="%TaskTimeDescription">Total amount of time spent within a task.</msg>
    <msg name="%UserTaskTime">Task Time</msg>
    <msg name="%UserTaskTimeDescription">Total amount of time spent within a task.</msg>
    <msg name="%TaskInstanceCount">Task Count</msg>
    <msg name="%TaskInstanceCountDescription">Total number of times a task is run.</msg>
    <msg name="%TaskAvgTime">Average Task Time</msg>
    <msg name="%TaskAvgTimeDescription">Average amount of time spent in the task.</msg>
    <msg name="%TaskParameter">Task Parameter</msg>
    <msg name="%TaskTransition">Task Transition</msg>
    <msg name="%FPGAComputeTaskType">Computing Task (FPGA)</msg>
    <msg name="%FPGAComputeTaskCount">Computing Task Count</msg>
    <msg name="%FPGAComputeTaskCountDescription">Total number of times a task is run.</msg>
    <msg name="%FPGASourceComputeTask">Source Computing Task (FPGA)</msg>
    <msg name="%FPGAMemoryTransferSizeDescription">Amount of memory processed on an FPGA.</msg>
    <msg name="%FPGAMemoryTransferBandwidthDescription">An average bandwidth of the data transferred between a CPU and an FPGA. In some cases (for example, clEnqueueMapBuffer), there may be transfers generating high bandwidth values because memory is not copied but shared via L3 cache.</msg>
    <msg name="%FPGAComputingTaskTime">Computing Task Time</msg>
    <msg name="%FPGAComputingTaskUtilization">FPGA Utilization</msg>
    <msg name="%ComputingTask">Computing Task</msg>
    <msg name="%GpuComputeTaskInstance">Instance</msg>
    <msg name="%GpuSourceComputeTask">Source Computing Task (GPU)</msg>
    <msg name="%GpuComputingTaskTimeTotal">Total Time</msg>
    <msg name="%GpuComputingTaskTimeTotalTransfer">Total Transfer Time (f)</msg>
    <msg name="%GpuComputingTaskTimeTotalCompute">Total Compute Time</msg>
    <msg name="%GpuComputingTaskTimeFull">Computing Task Time (GPU)</msg>
    <msg name="%ComputingTaskQueueTime">Computing Queue Time</msg>
    <msg name="%GpuComputeTaskCount">Instance Count</msg>
    <msg name="%GpuComputeTaskDuration">Average Time</msg>
    <msg name="%ComputingSpaceSize">Work Size</msg>
    <msg name="%ComputingGlobalSizeShort">Global</msg>
    <msg name="%ComputingGlobalSize">Global Size</msg>
    <msg name="%ComputingGlobalSizeDescription">Total working size of a computing task.</msg>
    <msg name="%ComputingLocalSizeShort">Local</msg>
    <msg name="%ComputingLocalSize">Local Size</msg>
    <msg name="%ComputingLocalSizeDescription">Local space size of a computing task. For example, for an OpenCL kernel, it is a working group size.</msg>
    <msg name="%ComputingSimdWidthShort">SIMD Width</msg>
    <msg name="%ComputingSimdWidth">SIMD Width</msg>
    <msg name="%ComputingSimdWidthDescription">The number of working items processed by a GPU thread.</msg>
    <msg name="%ComputingTaskPurpose">Computing Task Purpose</msg>
    <msg name="%ComputingTaskPurposeShortName">Purpose</msg>
    <msg name="%ComputingTransferSize">Transfer Size</msg>
    <msg name="%ComputingTransferSizeShort">Size</msg>
    <msg name="%GPUMemoryTransferSizeDescription">Amount of memory processed on a GPU.</msg>
    <msg name="%GPUMemoryTransferBandwidthDescription">Average bandwidth of data transfer between a CPU and a GPU. In some cases (for example, clEnqueueMapBuffer), there may be transfers generating high bandwidth values because memory is not copied but shared via L3 cache.</msg>
    <msg name="%ComputeSVMUsageType">SVM Usage Type</msg>
    <msg name="%ContextSwitches">Context Switches</msg>
    <msg name="%ContextSwitchCpu">CPU</msg>
    <msg name="%ContextSwitchReason">Reason</msg>
    <msg name="%ContextSwitchThreadState">Thread State</msg>
    <msg name="%QuantumSync">Wait Rate</msg>
    <msg name="%QuantumSyncIssueText">The average Wait time is too low. This could be caused by small timeouts, high contention between threads, or excessive calls to system synchronization functions. Explore the call stack, the timeline, and the source code to identify what is causing low wait time per synchronization context switch.</msg>
    <msg name="%QuantumSyncDescription">Average Wait time (in milliseconds) per synchronization context switch. Low metric value may signal an increased contention between threads and inefficient use of system API.</msg>
    <msg name="%ContextSwitchCount">Context Switch Count</msg>
    <msg name="%GPUTime">GPU EU Array Usage</msg>
    <msg name="%GPUBusy">Render/GPGPU Command Streamer Loaded</msg>
    <msg name="%GPUGpuTime">GPU Time</msg>
    <msg name="%GPUGpuTimeDescription">GPU time is part of elapsed time when GPU was busy.</msg>
    <msg name="%GPUBusyDescription">The normalized sum of all cycles where commands exist on the GPU Render/GPGPU ring.</msg>
    <msg name="%GPUEUState">EU State</msg>
    <msg name="%GPUEUArrays">EU Arrays</msg>
    <msg name="%GPUEUActive">EU Array Active</msg>
    <msg name="%GPUEUActiveDescription">The normalized sum of all cycles on all EUs that were spent actively executing instructions.</msg>
    <msg name="%GPUEUArray">EU Array</msg>
    <msg name="%GPUEUActiveShort">Active</msg>
    <msg name="%GPUEUStalled">EU Array Stalled</msg>
    <msg name="%GPUEUStalledDescription">The normalized sum of all cycles on all EUs during which the EUs were stalled. At least one thread is loaded, but the EU is stalled.</msg>
    <msg name="%GPUEUStalledIssueText">A significant portion of GPU time is lost due to stalls. For compute-bound code, this could indicate that performance is limited by memory or sampler acesses.</msg>
    <msg name="%GPUEUStalledShort">Stalled</msg>
    <msg name="%GPUTimeDescription">The normalized sum of all cycles on all cores with at least one thread loaded.</msg>
    <msg name="%GPUEUIdle">EU Array Idle</msg>
    <msg name="%GPUEUIdleDescription">The normalized sum of all cycles on all cores when no threads were scheduled on a core.</msg>
    <msg name="%GPUEUIdleIssueText">A significant portion of GPU time is spent idle. This is usually caused by imbalance or thread scheduling problems.</msg>
    <msg name="%GPUEUArrayMetricsDescription">Breakdown of GPU cores array cycles.</msg>
    <msg name="%GPUEUNotActiveWhenBusy">EU Array Stalled/Idle</msg>
    <msg name="%GPUEUNotActiveWhenBusyDescription">The average portion of the time the EUs were stalled or idle.</msg>
    <msg name="%GPUEUNotActiveWhenBusyDescriptionInKernel">The average portion of the time the EUs were stalled or idle (% of GPU time).</msg>
    <msg name="%GPUEUNotActiveWhenBusyIssueText">The percentage of time when the EUs were stalled or idle is high, which has a negative impact on compute-bound applications.</msg>
    <msg name="%GPUThreadOccupancyWhenBusy">Occupancy</msg>
    <msg name="%GPUThreadOccupancyWhenBusyDescription">GPU Occupancy is the normalized sum of all cycles from across all cores and thread slots during which a slot had a thread scheduled.</msg>
    <msg name="%GPUThreadOccupancyWhenBusyDescriptionInKernel">The normalized sum of all cycles on all core and thread slots when a slot has a thread scheduled (% of peak value).</msg>
    <msg name="%GPUThreadOccupancyWhenBusyIssueText">Low value of the occupancy metric may be caused by inefficient work scheduling. Make sure work items are neither too small nor too large.</msg>
    <msg name="%GPUSamplerBusyWhenBusy">Sampler Busy</msg>
    <msg name="%GPUSamplerBusyWhenBusyDescription">The normalized sum of all cycles on all cores when the Sampler was busy while EUs were stalled or idle.</msg>
    <msg name="%GPUSamplerBusyWhenBusyDescriptionInKernel">The normalized sum of all cycles on all cores when the Sampler was busy while EUs were stalled or idle (% of peak value).</msg>
    <msg name="%GPUSamplerBusyWhenBusyIssueText">Sampler was overutilized when EUs were stalled or idle. Consider reducing the image-related operations.</msg>
    <msg name="%GPUCoreFrequency">Core Frequency</msg>
    <msg name="%GPUCoreFrequencyDescription">Frequency of a GPU core.</msg>
    <msg name="%GPUEUIdleShort">Idle</msg>
    <msg name="%GPUCSThreadIssued">Computing Threads Started, Threads/sec</msg>
    <msg name="%GPUCSThreadIssuedDescription">Number of threads started across all EUs for compute work per second.</msg>
    <msg name="%GPUCSThreadIssuedCount">Computing Threads Started</msg>
    <msg name="%GPUCSThreadIssuedCountDescription">Number of threads started across all EUs for compute work.</msg>
    <msg name="%GPUSamplerBusy">Sampler Busy</msg>
    <msg name="%GPUCSThreadIssuedIssueText">High thread issue rate lowers GPU utilization efficiency due to thread creation overhead even for lightweight GPU threads.&#10;To improve performance, change the kernel code to increase the load in a working item, adjust global working size, and so decrease the number of GPU threads.</msg>
    <msg name="%GPUSamplerBusyDescription">The normalized sum of all cycles on all cores when the sampler was busy.</msg>
    <msg name="%GPUSamplerBusyDescriptionGSim">The normalized sum of all cycles on all cores when the sampler was busy, including HDC requests processing.</msg>
    <msg name="%GPUSamplerBusyIssueText">Significant amount of sampler accesses might cause stalls. Consider decreasing the use of the sampler or access it with a better locality.</msg>
    <msg name="%GPUSamplerBottleneck">Sampler Is Bottleneck</msg>
    <msg name="%GPUSamplerBottleneckDescription">Sampler stalls EUs due to the full input fifo queue, and starves the output fifo, so EUs need to wait to submit requests to sampler.</msg>
    <msg name="%GPUSamplerBottleneckIssueText">Significant amount of sampler accesses might cause stalls. Consider decreasing the use of the sampler or access it with a better locality.</msg>
    <msg name="%GPUSampler">Sampler</msg>
    <msg name="%GPUSamplerBusyShort">Busy</msg>
    <msg name="%GPUSamplerBottleneckShort">Bottleneck</msg>
    <msg name="%GPUL3SamplerThroughput">L3 Sampler Bandwidth, GB/sec</msg>
    <msg name="%GPUL3SamplerThroughputShort">L3 Sampler</msg>
    <msg name="%GPUL3SamplerThroughputDescription">Total number of bytes transferred between Samplers and L3 cache.</msg>
    <msg name="%GPUL3SamplerThroughputDescriptionGSim">Total number of bytes transferred between Samplers and L3 cache, including HDC requests.</msg>
    <msg name="%GPUL3ShaderThroughput">L3 Bandwidth, GB/sec</msg>
    <msg name="%GPUL3ShaderThroughputShort">L3</msg>
    <msg name="%GPUL3ShaderThroughputDescription">Total number of bytes transferred directly between EU array and L3 cache.</msg>
    <msg name="%GPUEuThreadOccupancy">EU Threads Occupancy</msg>
    <msg name="%GPUEuThreadOccupancyDescription">The normalized sum of all cycles on all cores and thread slots when a slot has a thread scheduled.</msg>
    <msg name="%GPUUtilizationIssueText">GPU utilization is low. Consider offloading more work to the GPU to increase overall application performance.</msg>
    <msg name="%GPUUtilizationMaxDescription">Maximum GPU utilization across engines that had at least one packet on them.</msg>
    <msg name="%GPUUsage">GPU Utilization</msg>
    <msg name="%GPUUsageDescription">GPU Utilization indicates the percentage of the total Elapsed Time when the GPU was utilized.</msg>
    <msg name="%GPUEuAvgIpcRate">EU IPC Rate</msg>
    <msg name="%GPUEuAvgIpcRateShort">IPC Rate</msg>
    <msg name="%GPUEuAvgIpcRateDescription">The average rate of instructions per cycle (IPC) calculated for 2 FPU pipelines</msg>
    <msg name="%GPUEuFpuBothActive">FPU Utilization</msg>
    <msg name="%GPUEuFpuBothActiveShort">2 FPUs active</msg>
    <msg name="%GPUEuFpuBothActiveDescription">The average portion of the time both FPUs were utilized.</msg>
    <msg name="%GPUEuFpuBothActiveDescriptionInKernel">The average portion of the time both FPUs were utilized (% of GPU time).</msg>
    <msg name="%FPUIssueText">The performance is limited by high utilization of FPUs. Consider reducing computations.</msg>
    <msg name="%GPUEuSendActive">EU Send pipeline active</msg>
    <msg name="%GPUEuSendActiveShort">Send active</msg>
    <msg name="%GPUEuSendActiveDescription">The normalized sum of all cycles on all cores when EU send pipeline was actively processing.</msg>
    <msg name="%GPUEuInstructionMetricsDescription">Statistics on GPU cores instructions.</msg>
    <msg name="%GPUMemoryAccessCoalescenceRatio">Memory Transactions Coalescence</msg>
    <msg name="%GPUMemoryAccessCoalescenceRatioDescription">The ratio of the used memory byte count to the transferred memory byte count.</msg>
    <msg name="%GPUTypedReadCoalRatio">Typed Reads Coalescence</msg>
    <msg name="%GPUTypedWriteCoalRatio">Typed Writes Coalescence</msg>
    <msg name="%GPUUntypedReadCoalRatio">Untyped Reads Coalescence</msg>
    <msg name="%GPUUntypedWriteCoalRatio">Untyped Writes Coalescence</msg>
    <msg name="%GPUCoalRatioUniversalDescription">Transaction Coalescence is a ratio of the used bytes to all bytes requested by the transaction. The lower the coalescence, the bigger part of the bandwidth is wasted.&#10;&#10;It originates from the GPU Data Port function that dynamically merges scattered memory operations into fewer operations over non-duplicated 64-byte cacheline requests.&#10;For example, if a 16-wide SIMD operation consecutively reads integer array elements with a stride of 2, the coalescence of such a transaction is 50%, because half of the bytes in the requested cacheline is not used.</msg>
    <msg name="%GPUShaderBarriers">GPU Barriers</msg>
    <msg name="%GPUShaderAtomics">GPU Atomics</msg>
    <msg name="%GPUPsEuActive">PS EU Active</msg>
    <msg name="%GPUPsEuStall">PS EU Stall</msg>
    <msg name="%GPUVsEuStall">VS EU Stall</msg>
    <msg name="%GPUVsEuActive">VS EU Active</msg>
    <msg name="%GPUSamplesKilledInPs">Samples Killed in PS</msg>
    <msg name="%GPUSamplesWritten">Samples Written</msg>
    <msg name="%GPUSamplesBlended">Samples Blended</msg>
    <msg name="%GPUStringUntypedReads">Untyped Reads</msg>
    <msg name="%GPUStringUntypedWrites">Untyped Writes</msg>
    <msg name="%GPUStringTypedReads">Typed Reads</msg>
    <msg name="%GPUStringTypedWrites">Typed Writes</msg>
    <msg name="%GPUUntypedMemoryRead">Untyped Memory Read Bandwidth, GB/sec</msg>
    <msg name="%GPUUntypedMemoryReadDescription">Bandwidth of memory read from untyped buffers (for example created with clCreateBuffer).</msg>
    <msg name="%GPUUntypedMemoryWrite">Untyped Memory Write Bandwidth, GB/sec</msg>
    <msg name="%GPUUntypedMemoryWriteDescription">Bandwidth of memory written to untyped buffers (for example created with clCreateBuffer).</msg>
    <msg name="%GPUUntypedMemoryReadBDWMaxGB">Untyped Memory Read Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUUntypedMemoryReadBDWMaxGBDescription">The maximum bandwidth of the memory read from untyped buffers (for example, created with clCreateBuffer).</msg>
    <msg name="%GPUUntypedMemoryWriteBDWMaxGB">Untyped Memory Write Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUUntypedMemoryWriteBDWMaxGBDescription">The maximum bandwidth of the memory written to untyped buffers (for example, created with clCreateBuffer).</msg>
    <msg name="%GPUUntypedMemoryAccessBandwidthDescription">Bandwidth of memory access from/to untyped buffers</msg>
    <msg name="%GPUSharedLocalMemoryRead">Shared Local Memory Read Bandwidth, GB/sec</msg>
    <msg name="%GPUSharedLocalMemoryReadDescription">Untyped memory reads from Shared Local Memory.</msg>
    <msg name="%GPUSharedLocalMemoryWrite">Shared Local Memory Write Bandwidth, GB/sec</msg>
    <msg name="%GPUSharedLocalMemoryWriteDescription">Untyped memory writes to Shared Local Memory.</msg>
    <msg name="%GPUSLMReadBDWMax">Shared Local Memory Read Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUSLMReadBDWMaxDescription">The maximum number of untyped memory reads from Shared Local Memory.</msg>
    <msg name="%GPUSLMWriteBDWMax">Shared Local Memory Write Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUSLMWriteBDWMaxDescription">The maximum number of untyped memory writes to Shared Local Memory.</msg>
    <msg name="%GPUSLMMemoryAccessBandwidthDescription">Bandwidth of memory access from/to Shared Local memory</msg>
    <msg name="%GPUTypedMemoryRead">Typed Memory Read Bandwidth, GB/sec</msg>
    <msg name="%GPUTypedMemoryReadDescription">Bandwidth of memory read from typed buffers. Note that reads from images (for example created with clCreateImage) are counted by sampler accesses and Texture Read metrics.</msg>
    <msg name="%GPUTypedMemoryWrite">Typed Memory Write Bandwidth, GB/sec</msg>
    <msg name="%GPUTypedMemoryWriteDescription">Bandwidth of memory written to typed buffers (for example created with clCreateImage).</msg>
    <msg name="%GPUTypedMemoryRead">Typed Memory Read Bandwidth, GB/sec</msg>
    <msg name="%GPUTypedMemoryReadBDWMax">Typed Memory Read Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUTypedMemoryReadBDWMaxDescription">The maximum bandwidth of the memory read from typed buffers. Note that reads from images (for example, created with clCreateImage) are counted by Sampler Accesses and Texture Read metrics.</msg>
    <msg name="%GPUTypedMemoryWriteBDWMax">Typed Memory Write Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUTypedMemoryWriteBDWMaxDescription">The maximum bandwidth of the memory written to typed buffers (for example, created with clCreateImage).</msg>
    <msg name="%GPUTypedMemoryAccessBandwidthDescription">Bandwidth of memory access from/to typed buffers</msg>
    <msg name="%GPUMemoryRead">GPU Memory Read Bandwidth, GB/sec</msg>
    <msg name="%GPUMemoryReadDescription">GPU memory read bandwidth between the GPU, chip uncore (LLC) and main memory.&#10;This metric counts all memory accesses that miss the internal GPU L3 cache or bypass it and are serviced either from uncore or main memory.</msg>
    <msg name="%GPUMemoryWrite">GPU Memory Write Bandwidth, GB/sec</msg>
    <msg name="%GPUMemoryWriteDescription">GPU write bandwidth between the GPU, chip uncore (LLC) and main memory.&#10;This metric counts all memory accesses that miss the internal GPU L3 cache or bypass it and are serviced either from uncore or main memory.</msg>
    <msg name="%GPUMemoryReadBDWMax">GPU Memory Read Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUMemoryReadBDWMaxDescription">The maximum value of GPU memory read bandwidth between the GPU, chip uncore (LLC) and main memory.&#10;This metric counts all memory accesses that miss the internal GPU L3 cache or bypass it and are serviced either from uncore or main memory.</msg>
    <msg name="%GPUMemoryWriteBDWMax">GPU Memory Write Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUMemoryWriteBDWMaxDescription">The maximum value of GPU write bandwidth between the GPU, chip uncore (LLC) and main memory.&#10;This metric counts all memory accesses that miss the internal GPU L3 cache or bypass it and are serviced either from uncore or main memory.</msg>
    <msg name="%GPUUntypedMemoryBandwidth">Untyped Memory Bandwidth, GB/sec</msg>
    <msg name="%GPUMemoryReadShort">Read</msg>
    <msg name="%GPUMemoryWriteShort">Write</msg>
    <msg name="%GPUSharedLocalMemoryBandwidth">Shared Local Memory Bandwidth, GB/sec</msg>
    <msg name="%GPUTypedMemoryBandwidth">Typed Memory Bandwidth, GB/sec</msg>
    <msg name="%GPUMemoryBandwidth">GPU Memory Bandwidth, GB/sec</msg>
    <msg name="%GPUL3Miss">GPU L3 Misses, Misses/sec</msg>
    <msg name="%GPUL3MissRate">GPU L3 Miss Rate</msg>
    <msg name="%GPUL3MissRatio">GPU L3 Miss Ratio</msg>
    <msg name="%GPUL3MissDescription">Read and write misses in GPU L3 cache. This doesn't count code lookups.</msg>
    <msg name="%GPUL3MissRateDescription">Read and write miss rate in GPU L3 cache. This doesn't count code lookups.</msg>
    <msg name="%GPUL3MissRatioDescription">Read and write miss ratio in GPU L3 cache. This doesn't count code lookups.</msg>
    <msg name="%GPUL3MissCount">GPU L3 Misses</msg>
    <msg name="%GPUL3MissCountDescription">Read and write misses in GPU L3 cache.</msg>
    <msg name="%GPULlcMiss">LLC Misses due GPU Lookups, Misses/s</msg>
    <msg name="%GPULlcMissRate">LLC Miss Rate due GPU Lookups, Misses/s</msg>
    <msg name="%GPULlcMissRatio">LLC Miss Ratio due GPU Lookups</msg>
    <msg name="%GPULlcMissCountDescription">The Last Level Uncore cache (LLC) miss count across all lookups done from the GPU.</msg>
    <msg name="%GPULlcMissRateDescription">The Last Level Uncore cache (LLC) miss rate across all lookups done from the GPU.</msg>
    <msg name="%GPULlcMissRatioDescription">The Last Level Uncore cache (LLC) miss ratio across all lookups done from the GPU.</msg>
    <msg name="%GPUMemoryTextureRead">GPU Memory Texture Read Bandwidth, GB/sec</msg>
    <msg name="%GPUMemoryTextureReadDescription">Sampler unit misses in sampler cache.</msg>
    <msg name="%GPUTexelQuadCount">GPU Texel Quads Count, Count/sec</msg>
    <msg name="%GPUTexelQuadCountDescription">Number of texels returned from the sampler.</msg>
    <msg name="%GPUBDWRatioUniversalName">Ratio to Max Bandwidth, %</msg>
    <msg name="%GPUBDWRatioUniversalDescription">Ratio of the bandwidth on this link to its theoretical peak.</msg>
    <msg name="%GPUReadBDWRatioUniversalDescription">Ratio of the read bandwidth on this link to its read theoretical peak.</msg>
    <msg name="%GPUWriteBDWRatioUniversalDescription">Ratio of the write bandwidth on this link to its write theoretical peak.</msg>
    <msg name="%GPUSharedLocalMemoryBandwidth">Shared Local Memory Read and Write Bandwidth, GB/sec</msg>
    <msg name="%GPUMemoryBandwidth">GPU Memory Read and Write Bandwidth, GB/sec</msg>
    <msg name="%GPUL3SamplerThroughputMaxGB">L3 Sampler Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUL3SamplerThroughputMaxGBDescription">The maximum number of GPU memory bytes transferred between samplers and L3 cache.</msg>
    <msg name="%GPUL3ShaderThroughputMetricMaxGB">L3 Bandwidth Maximum Value, GB/sec</msg>
    <msg name="%GPUL3ShaderThroughputMetricMaxGBDescription">The maximum number of GPU memory bytes transferred between EU array and L3 cache without URB.</msg>
    <msg name="%GPUNode">GPU Engine</msg>
    <msg name="%GPUDXTime">GPU Engines Usage</msg>
    <msg name="%GPUDXTimeShort">GPU Time</msg>
    <msg name="%GPUSoftQueue">GPU Software Queue</msg>
    <msg name="%GPUUtilization">GPU Utilization</msg>
    <msg name="%GPUGap">Not Utilized</msg>
    <msg name="%GPUSoftQueueTime">Queue Time</msg>
    <msg name="%GPUDMAPacketSubmissionId">Packet Submission ID</msg>
    <msg name="%GPUPacketPresent">Present</msg>
    <msg name="%GPUDMAPacketPerfTag">Detailed</msg>
    <msg name="%GPUDMAPacketPerfTagType">Packet Type</msg>
    <msg name="%GPUDMAPacketDuration">Packet Duration, sec</msg>
    <msg name="%GPUDMAPacketDurationType">Packet Duration Type</msg>
    <msg name="%GPUDMAPacketInstanceCount">Packet Count</msg>
    <msg name="%GTIL3TotalBandwidth">L3 &lt;-&gt; GTI Total Bandwidth</msg>
    <msg name="%GTIL3TotalBandwidthGBsec">L3 &lt;-&gt; GTI Total Bandwidth, GB/sec</msg>
    <msg name="%L3GTIBDWMax">L3 &lt;-&gt; GTI Bandwidth Maximum Value, GB/sec</msg>
    <msg name='%DMAQueueDepth'>Packet Queue Depth</msg>
    <msg name='%DMAQueueDuration'>Duration</msg>
    <msg name="%VSync">VSync</msg>
    <msg name="%CPUGPUUsageElapsedTime">Elapsed Time</msg>
    <msg name="%CPUGPUUsageUtilization">Device Utilization</msg>
    <msg name="%CPUOnly">CPU Only</msg>
    <msg name="%GPUOnly">GPU Only</msg>
    <msg name="%CPUandGPU">CPU and GPU</msg>
    <msg name="%CPUGPUBothActive">CPU and GPU Active</msg>
    <msg name="%CPUOnlyActive">CPU Only Active</msg>
    <msg name="%GPUOnlyActive">GPU Only Active</msg>
    <msg name="%CPUGPUBothIdle">CPU and GPU Idle</msg>
    <msg name="%CPUActiveTime">CPU Active</msg>
    <msg name="%GPUActiveTime">GPU Active</msg>
    <msg name="%CPUGPUConcurrency">CPU and GPU Concurrency</msg>
    <msg name="%GPUHPCActiveTime">GPU Active Time</msg>
    <msg name="%GPUHPCActiveTimeDescription">Time elapsed with GPU active</msg>
    <msg name="%GPUHPCEuAvgCpiRate">Average GPU Execution Unit CPI Rate</msg>
    <msg name="%GPUHPCEuAvgCpiRateDescription">The average rate of cycles per instruction (CPI) calculated for 2 FPU pipelines</msg>
    <msg name="%GPUHPCEuAltAvgCpiRate">Alternative Average GPU Execution Unit CPI Rate</msg>
    <msg name="%GPUHPCEuAltAvgCpiRateDescription">The average rate of cycles per instruction (CPI) calculated for 2 FPU pipelines, for all GPU cycles including stalled and idle states of the execution units</msg>
    <msg name="%GPUHPCEUAvgUtilization">GPU Utilization when Busy</msg>
    <msg name="%ConcatenateWithFamily"><arg name='%1'/> <arg name='%2'/></msg>
    <msg name='%DStateTime'>D-States Time</msg>
    <msg name='%DStateTimeDescription'>Time spent in Device states D0i0-D0ix.</msg>
    <msg name='%DStateComplex'>Complex</msg>
    <msg name='%DStateDevice'>North Complex Devices</msg>
    <msg name='%SCDStateDevice'>South Complex Devices</msg>
    <msg name='%DStateDeviceID'>Device ID</msg>
    <msg name='%DStates'>Device States</msg>
    <msg name='%SampleCounts'>Sample Counts</msg>
    <msg name='%TotalTimeInD0'>Active Time</msg>
    <msg name='%TotalTimeInD0Description'>Total time spent in state D0i0.</msg>
    <msg name='%TotalTimeNotInS0'>Total Time in Non-S0 States</msg>
    <msg name='%TotalTimeNotInS0Description'>Total time in sleep states S0i1-S0ix.</msg>
    <msg name='%TotalTimeInS0'>Total Time in S0 State</msg>
    <msg name='%TotalTimeInS0Description'>Total time spent in the active S0i0 state.</msg>
    <msg name='%WakelockCount'>Wakelock Count</msg>
    <msg name='%WakelockCountDescription'>Total number Wakelocks locked during the collection.</msg>
    <msg name='%WakelockLockCount'>Wakelock Lock Count</msg>
    <msg name='%WakelockUnlockCount'>Wakelock Unlock Count</msg>
    <msg name='%WakelockTime'>Total Lock Duration</msg>
    <msg name='%WakelockTimeDescription'>Sum of durations of all locks held by the process.</msg>
    <msg name='%WakelockObject'>Wakelock</msg>
    <msg name='%WakelockLockReason'>Lock Reason</msg>
    <msg name='%WakelockLockProcess'>Locking Process</msg>
    <msg name='%WakelockLockProcessID'>Locking PID</msg>
    <msg name='%WakelockUnlockReason'>Unlock Reason</msg>
    <msg name='%WakelockUnlockProcess'>Unlocking Process</msg>
    <msg name='%WakelockUnlockProcessID'>Unlocking PID</msg>
    <msg name='%SStateTime'>S-States Time</msg>
    <msg name='%SStateTimeDescription'>Time spent in System states S0i0-S0ix.</msg>
    <msg name='%Machine'>System</msg>
    <msg name='%AvailableCoreTime'>Available Core Time</msg>
    <msg name='%AvailableCoreTimeDescription'>Total execution time over all cores.</msg>
    <msg name='%KernelWakelockCount'>Kernel Wakelock Count</msg>
    <msg name='%UserWakelockTag'>User Wakelock Tag</msg>
    <msg name='%UserWakelockFlag'>User Wakelock Flag</msg>
    <msg name='%UserWakelockUID'>User UID</msg>
    <msg name='%UserWakelockUIDName'>Application Name</msg>
    <msg name='%WakelockType'>Wakelock Type</msg>
    <msg name='%KernelWakelock'>Kernel Wakelock</msg>
    <msg name='%UserWakelock'>User Wakelock</msg>
    <msg name='%UserWakelockLockProcess'>User Locking Process</msg>
    <msg name='%UserWakelockLockProcessID'>User Locking PID</msg>
    <msg name='%UserWakelockUnlockProcess'>User Unlocking Process</msg>
    <msg name='%UserWakelockUnlockProcessID'>User Unlocking PID</msg>
    <msg name='%WakelockLockThread'>Locking Thread</msg>
    <msg name='%WakelockLockThreadID'>Locking TID</msg>
    <msg name='%WakelockUnlockThread'>Unlocking Thread</msg>
    <msg name='%WakelockUnlockThreadID'>Unlocking TID</msg>
    <msg name='%UserWakelockCount'>User Wakelock Count</msg>
    <msg name='%PackageCState'>Package C-States</msg>
    <msg name='%ModuleCState'>Module C-States</msg>
    <msg name='%PackageCStateTimeDescription'>Time spent in package sleep states C0-Cx.</msg>
    <msg name='%Name'>Name</msg>
    <msg name='%GPUVendor'>Vendor</msg>
    <msg name='%GPUDriver'>Driver</msg>
    <msg name='%GPUStepping'>Stepping</msg>
    <msg name='%GPUEUCount'>EU Count</msg>
    <msg name='%GPUMaxEUThreadCount'>Max EU Thread Count</msg>
    <msg name='%GPUMaxCoreFreq'>Max Core Frequency</msg>
    <msg name='%GPUGPASupport'>Graphics Performance Analysis</msg>
    <msg name='%GPUEdramSize'>eDRAM size</msg>
    <msg name='%GPUOpenCLVersion'>Version</msg>
    <msg name='%GPUOpenCLMaxComputeUnits'>Max Compute Units</msg>
    <msg name='%GPUOpenCLMaxWorkgroupSize'>Max Work Group Size</msg>
    <msg name='%GPUOpenCLLocalMemSize'>Local Memory</msg>
    <msg name='%GPUOpenClSVMCapabilities'>SVM Capabilities</msg>
    <msg name='%DeviceName'>Device Name</msg>
    <msg name='%DeviceTemperatureLegend'>Device Temperature</msg>
    <msg name='%DeviceBandwidthLegend'>Device Bandwidth</msg>
    <msg name='%DevicePowerLegend'>Device Power</msg>
    <msg name='%DeviceNCLegend'>North Complex Devices</msg>
    <msg name='%DeviceSCLegend'>South Complex Devices</msg>
    <msg name='%GfxPStates'>Graphics P-States</msg>
    <msg name='%GfxPStatesTime'>Graphics P-States Time</msg>
    <msg name='%GfxCStates'>Graphics C-States</msg>
    <msg name='%Temperature'>Temperature (Â°C)</msg>
    <msg name='%TemperatureText'>Temperature (Celsius)</msg>
    <msg name='%EventType'>Event Type</msg>
    <msg name='%EventCount'>Event Count</msg>
    <msg name='%ComponentEventType'>Component</msg>
    <msg name='%BandwidthBytes'>Bandwidth (Bytes)</msg>
    <msg name='%BandwidthMBytes'>Bandwidth (Mbytes)</msg>
    <msg name='%BandwidthBytesPerSecond'>Bandwidth (Bytes/sec)</msg>
    <msg name='%BandwidthMBytesPerSecond'>Total (MB/sec)</msg>
    <msg name='%TransferredBandwidthMBytes'>Transferred Data (MB)</msg>
    <msg name='%TotalBW'>Total (MB/sec)</msg>
    <msg name='%AvgBandwidthMBytesPerSecond'>Average Bandwidth (MB/sec)</msg>
    <msg name='%ReadWrite32'>ReadWrite32 (MB/sec)</msg>
    <msg name='%ReadWrite64'>ReadWrite64 (MB/sec)</msg>
    <msg name='%ReadWritePartial'>ReadWritePartial (MB/sec)</msg>
    <msg name='%Read32'>Read32 (MB/sec)</msg>
    <msg name='%READ32'>READ32 (MB/sec)</msg>
    <msg name='%Read64'>Read64 (MB/sec)</msg>
    <msg name='%READ64'>READ64 (MB/sec)</msg>
    <msg name='%Module0Read32'>Module0 Read32 (MB/sec)</msg>
    <msg name='%Module0Read64'>Module0 Read64 (MB/sec)</msg>
    <msg name='%Module0ReadPartial'>Module0 ReadPartial (MB/sec)</msg>
    <msg name='%Module1Read32'>Module1 Read32 (MB/sec)</msg>
    <msg name='%Module1Read64'>Module1 Read64 (MB/sec)</msg>
    <msg name='%Module1ReadPartial'>Module1 ReadPartial (MB/sec)</msg>
    <msg name='%DDR0Read32'>DDR0 Read32 (MB/sec)</msg>
    <msg name='%DDR0Rank0Read32'>DDR0 Rank0 Read32 (MB/sec)</msg>
    <msg name='%DDR0Rank1Read32'>DDR0 Rank1 Read32 (MB/sec)</msg>
    <msg name='%DDR0Read64'>DDR0 Read64 (MB/sec)</msg>
    <msg name='%DDR0Rank0Read64'>DDR0 Rank0 Read64 (MB/sec)</msg>
    <msg name='%DDR0Rank1Read64'>DDR0 Rank1 Read64 (MB/sec)</msg>
    <msg name='%DDR1Read32'>DDR1 Read32 (MB/sec)</msg>
    <msg name='%DDR1Rank0Read32'>DDR1 Rank0 Read32 (MB/sec)</msg>
    <msg name='%DDR1Rank1Read32'>DDR1 Rank1 Read32 (MB/sec)</msg>
    <msg name='%DDR1Read64'>DDR1 Read64 (MB/sec)</msg>
    <msg name='%DDR1Rank0Read64'>DDR1 Rank0 Read64 (MB/sec)</msg>
    <msg name='%DDR1Rank1Read64'>DDR1 Rank1 Read64 (MB/sec)</msg>
    <msg name='%Write32'>Write32 (MB/sec)</msg>
    <msg name='%WRITE32'>WRITE32 (MB/sec)</msg>
    <msg name='%Write64'>Write64 (MB/sec)</msg>
    <msg name='%WRITE64'>WRITE64 (MB/sec)</msg>
    <msg name='%Module0Write32'>Module0 Write32 (MB/sec)</msg>
    <msg name='%Module0Write64'>Module0 Write64 (MB/sec)</msg>
    <msg name='%Module0WritePartial'>Module0 WritePartial (MB/sec)</msg>
    <msg name='%Module1Write32'>Module1 Write32 (MB/sec)</msg>
    <msg name='%Module1Write64'>Module1 Write64 (MB/sec)</msg>
    <msg name='%Module1WritePartial'>Module1 WritePartial (MB/sec)</msg>
    <msg name='%DDR0Write32'>DDR0 Write32 (MB/sec)</msg>
    <msg name='%DDR0Rank0Write32'>DDR0 Rank0 Write32 (MB/sec)</msg>
    <msg name='%DDR0Rank1Write32'>DDR0 Rank1 Write32 (MB/sec)</msg>
    <msg name='%DDR0Write64'>DDR0 Write64 (MB/sec)</msg>
    <msg name='%DDR0Rank0Write64'>DDR0 Rank0 Write64 (MB/sec)</msg>
    <msg name='%DDR0Rank1Write64'>DDR0 Rank1 Write64 (MB/sec)</msg>
    <msg name='%DDR1Write32'>DDR1 Write32 (MB/sec)</msg>
    <msg name='%DDR1Rank0Write32'>DDR1 Rank0 Write32 (MB/sec)</msg>
    <msg name='%DDR1Rank1Write32'>DDR1 Rank1 Write32 (MB/sec)</msg>
    <msg name='%DDR1Write64'>DDR1 Write64 (MB/sec)</msg>
    <msg name='%DDR1Rank0Write64'>DDR1 Rank0 Write64 (MB/sec)</msg>
    <msg name='%DDR1Rank1Write64'>DDR1 Rank1 Write64 (MB/sec)</msg>
    <msg name='%ReadPartial'>ReadPartial (MB/sec)</msg>
    <msg name='%WritePartial'>WritePartial (MB/sec)</msg>
    <msg name='%Module0BW'>Module 0 (MB/sec)</msg>
    <msg name='%Module1BW'>Module 1 (MB/sec)</msg>
    <msg name='%GfxBW'>GFX (MB/sec)</msg>
    <msg name='%DisplayBW'>Display (MB/sec)</msg>
    <msg name='%IspBW'>ISP (MB/sec)</msg>
    <msg name='%IoBW'>IO (MB/sec)</msg>
    <msg name='%IoCh0BW'>IO Channel 0 (MB/sec)</msg>
    <msg name='%IoCh1BW'>IO Channel 1 (MB/sec)</msg>
    <msg name='%IoCh2BW'>IO Channel 2 (MB/sec)</msg>
    <msg name='%VedBW'>VED (MB/sec)</msg>
    <msg name='%IaBW'>IA (MB/sec)</msg>
    <msg name='%PUnitBW'>P-Unit (MB/sec)</msg>
    <msg name='%Thermal'>Temperature</msg>
    <msg name='%Time'>Time</msg>
    <msg name='%CPULoadObject'>CPU Load</msg>
    <msg name='%VCPULoadObject'>VCPU Load</msg>
    <msg name='%CPULoad5ms'>5ms Window</msg>
    <msg name='%CPULoad10ms'>10ms Window</msg>
    <msg name='%CPULoad20ms'>20ms Window</msg>
    <msg name='%CPULoad50ms'>50ms Window</msg>
    <msg name='%CPUUtilization'>CPU Utilization (%)</msg>
    <msg name='%CPUUtilizationDescription'>Percentage of time spent in the active C0 state over all cores.</msg>
    <msg name='%ByText'><arg name='%1'/> by <arg name='%2'/></msg>
    <msg name='%PMUTsxAbortReason'>Abort Reason</msg>
    <msg name='%PMUTsxAbortCycles'>Abort Cycles</msg>
    <msg name='%ConsumedEnergyMilliJule'>Energy Consumed (mJ)</msg>
    <msg name='%PowerMilliWatt'>Power (mW)</msg>
    <msg name='%PowerEventType'>Component</msg>
    <msg name='%SystemPower'>System (mW)</msg>
    <msg name='%PackagePower'>Package (mW)</msg>
    <msg name='%CpuPower'>CPU (mW)</msg>
    <msg name='%GpuPower'>GPU (mW)</msg>
    <msg name='%DramPower'>Dram (mW)</msg>
    <msg name='%SoCPower'>SoC (mW)</msg>
    <msg name='%IstpContextInstance'>Context Instance</msg>
    <msg name='%IstpCounters'>Counters</msg>
    <msg name='%IstpLoadTime'>Load Time</msg>
    <msg name='%State'>State</msg>
    <msg name='%Context'>Context</msg>
    <msg name='%VCore'>VCore</msg>
    <msg name='%SyncObjId'>Sync Object Id</msg>
    <msg name='%Interrupt'>Interrupt</msg>
    <msg name='%StateCount'>State Count</msg>
    <msg name='%TaskStateTime'>Task State Time</msg>
    <msg name='%InterruptCount'>Interrupt Count</msg>
    <msg name='%InterruptCountDescription'>Total number of times an interrupt handler is run.</msg>
    <msg name='%InterruptTime'>Interrupt Time</msg>
    <msg name='%InterruptTimeDescription'>Total amount of time spent within an interrupt handler.</msg>
    <msg name='%InterruptIRQ'>Interrupt Id</msg>
    <msg name='%InterruptType'>Interrupt Type</msg>
    <msg name='%InterruptDuration'>Interrupt Duration</msg>
    <msg name='%InterruptDurationType'>Interrupt Duration Type</msg>
    <msg name='%HW'>Hardware</msg>
    <msg name='%SW'>Software</msg>
    <msg name='%Transitions'>Transitions</msg>
    <msg name='%SyncObject'>Sync Object</msg>
    <msg name='%HWInterrupt'>HW Interrupt</msg>
    <msg name='%Location'>Location</msg>
    <msg name='%InterruptId'>Interrupt Id</msg>
    <msg name='%InterruptName'>Interrupt Name</msg>
    <msg name='%IstpTaskDuration'>Duration, sec</msg>
    <msg name='%IstpTaskDurationType'>Task Duration Type</msg>
    <msg name='%SlowTaskMarker'>Slow Tasks</msg>
    <msg name='%SlowInterruptMarker'>Slow Interrupts</msg>
    <msg name='%IstpSlowFunctionMarker'>Slow Functions</msg>
    <msg name='%IstpMutexAquire'>Mutex Aquire</msg>
    <msg name='%IstpMutexRelease'>Mutex Release</msg>
    <msg name='%IstpSemaphoreIncrement'>Semaphore Increment</msg>
    <msg name='%IstpSemaphoreDecrement'>Semaphore Decrement</msg>
    <msg name='%IstpQueueSend'>Queue Send</msg>
    <msg name='%IstpQueueReceive'>Queue Receive</msg>
    <msg name='%IstpEventFlagsSet'>Event Flags Set</msg>
    <msg name='%IstpEventFlagsGet'>Event Flags Get</msg>
    <msg name='%IstpSyncStat'>Sync API statistic</msg>
    <msg name='%IstpFunctionCount'>Function Count</msg>
    <msg name='%IstpBandwidthSummary'>Average Bandwidth (MB/s)</msg>
    <msg name='%IstpFunctionTime'>Function Time</msg>
    <msg name='%IstpTaskInterruptType'>Task and Interrupt</msg>
    <msg name='%CriticalTiming'>Critical Timing</msg>
    <msg name='%HighlyCriticalTiming'>Highly Critical Timing</msg>
    <msg name='%IstpWaitTime'>Wait Time</msg>
    <msg name='%IstpHoldTime'>Hold Time</msg>
    <msg name='%IstpSyncRequestCount'>Request Count</msg>
    <msg name='%IstpSyncAquireCount'>Aquire Count</msg>
    <msg name='%BandwidthGBperSec'>Total, GB/sec</msg>
    <msg name='%ReadBandwidthGBperSec'>Average Bandwidth Read, GB/sec</msg>
    <msg name='%WriteBandwidthGBperSec'>Average Bandwidth Write, GB/sec</msg>
    <msg name='%ReadBandwidthMaxRatio'>Average Bandwidth Read, GB/sec</msg>
    <msg name='%WriteBandwidthMaxRatio'>Average Bandwidth Write, GB/sec</msg>
    <msg name='%WriteBandwidthMBperSec'>Average Bandwidth Write, MB/sec</msg>
    <msg name='%QPIBandwidthGBperSec'>Total, GB/sec</msg>
    <msg name='%QPIDataReadBandwidthGBperSec'>Incoming Data, GB/sec</msg>
    <msg name='%QPINonDataReadBandwidthGBperSec'>Incoming Non-Data, GB/sec</msg>
    <msg name='%QPIDataWriteBandwidthGBperSec'>Outgoing Data, GB/sec</msg>
    <msg name='%QPINonDataWriteBandwidthGBperSec'>Outgoing Non-Data, GB/sec</msg>
    <msg name='%QPIReadBandwidthGBperSec'>Incoming, GB/sec</msg>
    <msg name='%QPIWriteBandwidthGBperSec'>Outgoing, GB/sec</msg>
    <msg name='%DataTransferredGBDescription'>Data read from or written into a dynamic random-access memory by a processor.</msg>
    <msg name='%PCIeOvertimeBandwidthDescription'>Data read or written over the PCI Express bus by a processor.</msg>
    <msg name='%PCIeTotalBandwidthDescription'>Data read or written over the PCIe bus by processor and devices.</msg>
    <msg name='%InboundPCIeBandwidthDescription'>Data read or written over the PCIe bus by the device to the system memory.</msg>
    <msg name='%OutboundPCIeBandwidthDescription'>Data read or written over the PCIe bus by the processor to the device's MMIO space.</msg>
    <msg name='%InboundPCIeReadSummary'>Inbound PCIe Read, MB/sec</msg>
    <msg name='%InboundPCIeReadSummaryDescription'>PCIe devices reading from system memory</msg>
    <msg name='%InboundPCIeWriteSummary'>Inbound PCIe Write, MB/sec</msg>
    <msg name='%InboundPCIeWriteSummaryDescription'>PCIe devices writing to system memory</msg>
    <msg name='%OutboundPCIeReadSummary'>Outbound PCIe Read, MB/sec</msg>
    <msg name='%OutboundPCIeReadSummaryDescription'>CPU reading from PCIe devices</msg>
    <msg name='%OutboundPCIeWriteSummary'>Outbound PCIe Write, MB/sec</msg>
    <msg name='%OutboundPCIeWriteSummaryDescription'>CPU writing to PCIe devices</msg>
    <msg name='%InboundPCIeRequestL3Miss'>L3 Miss, %</msg>
    <msg name='%InboundPCIeRequestL3Hit'>L3 Hit, %</msg>
    <msg name='%InboundPCIeReadL3MissRatioDescription'>Ratio of inbound PCIe read requests missing the L3</msg>
    <msg name='%InboundPCIeWriteL3MissRatioDescription'>Ratio of inbound PCIe write requests missing the L3</msg>
    <msg name='%InboundPCIeReadL3HitRatioDescription'>Ratio of inbound PCIe read requests hitting the L3</msg>
    <msg name='%InboundPCIeWriteL3HitRatioDescription'>Ratio of inbound PCIe write requests hitting the L3</msg>
    <msg name='%M2PCIe'>M2PCIe</msg>
    <msg name='%InboundPCIeReadL3MissRatioIssueText'>A significant portion of inbound PCIe read requests misses the L3 cache. To reduce inbound read latency and to avoid induced DRAM and UPI traffic, make sure both the device and the memory it accesses reside on the same socket and/or consider optimizations that localize I/O data in the L3.</msg>
    <msg name='%InboundPCIeWriteL3MissRatioIssueText'>A significant portion of inbound PCIe write requests misses the L3 cache. To reduce inbound write latency and to avoid induced DRAM and UPI traffic, make sure both the device and the memory it accesses reside on the same socket and/or consider optimizations that localize I/O data in the L3.</msg>
    <msg name='%OutboundPCIeReadBandwidthIssueText'>Non-zero MMIO read traffic on the hot path may significantly limit system throughput. Explore functions that use uncacheable reads to locate the code reading device's registers through MMIO space.</msg>
    <msg name='%UncacheableReadsPerSecond'>Uncacheable reads per second</msg>
    <msg name='%UncacheableReadsPerSecondDescription'>Number of reads from uncacheable memory per second</msg>
    <msg name='%PCIEReadBandwidth'>PCIe Read Bandwidth, MB/sec</msg>
    <msg name='%PCIEWriteBandwidth'>PCIe Write Bandwidth, MB/sec</msg>
    <msg name='%PCIeTotalBandwidth'>Total, MB/sec</msg>
    <msg name='%BandwidthRead'>Read</msg>
    <msg name='%BandwidthWrite'>Write</msg>
    <msg name='%AvgLatency'>Average Latency (cycles)</msg>
    <msg name='%AvgLatencyDescription'>This metric shows average load latency in cycles</msg>
    <msg name='%LatencyLoads'>Loads</msg>
    <msg name='%LoadLatency'>Latency</msg>
    <msg name='%UncoreEventUnit'>Unit</msg>
    <msg name='%PciDevice'>PCI device</msg>
    <msg name='%TotalLatency'>Total Latency</msg>
    <msg name='%Cacheline'>Cacheline</msg>
    <msg name='%DataAddress'>Data Address</msg>
    <msg name='%QPILink'>Link</msg>
    <msg name='%DRAMChannel'>Channel</msg>
    <msg name='%PreciseClockticks'>Precise Clockticks</msg>
    <msg name='%PreciseClockticksDescription'>Precise Clockticks count emulated using the INST_RETIRED.PREC_DIST:sa=1000000:INV=yes:CMASK=10 event.</msg>
    <msg name='%RegionProcessStartTsc'>Process Start Tsc</msg>
    <msg name='%RegionProcessEndTsc'>Process End Tsc</msg>
    <msg name='%MemoryObject'>Memory Object</msg>
    <msg name='%MemoryObjectType'>Memory Object Type</msg>
    <msg name='%MemoryObjectAddress'>Memory Object Address</msg>
    <msg name='%MemoryObjectSize'>Memory Object Size</msg>
    <msg name='%MemoryObjectAllocSourceFileLine'>Memory Object</msg>
    <msg name='%MemoryObjectSourceLocationSourceFile'>Object Source Location</msg>
    <msg name='%MemoryObjectAllocCallstack'>Object Allocation Call Stack</msg>
    <msg name='%MemoryObjectAllocBytes'>Bytes Allocated</msg>
    <msg name='%Loads'>Loads</msg>
    <msg name='%Stores'>Stores</msg>
    <msg name='%LoadsAndStores'>Accesses</msg>
    <msg name='%PMULoadsAndStores'>Loads and Stores</msg>
    <msg name='%KNLBandwidthEstimate'>KNL Bandwidth Estimate (GB/s)</msg>
    <msg name='%KNLBandwidthEstimateDescription'>Estimate maximum possible bandwidth per core that would be used if the code ran on the Intel coprocessor code named Knights Landing (KNL). This metric helps identify functions with significant bandwidth.</msg>
    <msg name='%MemoryObjectStack'>Allocation Stack</msg>
    <msg name='%MemoryObjectAllocationSource'>Memory Object Allocation Source</msg>
    <msg name='%LLCMissCount'>LLC Miss Count</msg>
    <msg name='%LLCMissCountMADescription'> The LLC (last-level cache) is the last, and longest-latency, level in the memory hierarchy before main memory (DRAM). Any memory requests missing here must be serviced by local or remote DRAM, with significant latency. The LLC Miss Count metric shows total number of demand loads which missed LLC. Misses due to HW prefetcher are not included.</msg>
    <msg name='%LLCMissCountKNL'>L2 Miss Count</msg>
    <msg name='%LLCMissCountMAKNLDescription'> The L2 is the last and longest-latency level in the memory hierarchy before the main memory (DRAM) or MCDRAM. Any memory requests missing here must be serviced by local or remote DRAM or MCDRAM, with significant latency. The L2 Miss Count metric shows the total number of demand loads that missed the L2. Misses due to the HW prefetcher are not included.</msg>
    <msg name='%LocalDRAMCount'>Local DRAM Access Count</msg>
    <msg name='%LocalDRAMCountDescription'>This metric shows the total number of LLC misses serviced by the local memory. Misses due to HW prefetcher are not included.</msg>
    <msg name='%RemoteDRAMCount'>Remote DRAM Access Count</msg>
    <msg name='%RemoteDRAMCountDescription'>This metric shows the total number of LLC misses serviced by the remote memory. Misses due to HW prefetcher are not included.</msg>
    <msg name='%Local3DXPointAccessCount'>Local Persistent Memory Access Count</msg>
    <msg name='%Local3DXPointAccessCountDescription'>This metric shows the total number of LLC misses serviced by the local Intel Optane DC Persistent Memory. Misses due to HW prefetcher are not included.</msg>
    <msg name='%Remote3DXPointAccessCount'>Remote Persistent Memory Access Count</msg>
    <msg name='%Remote3DXPointAccessCountDescription'>This metric shows the total number of LLC misses serviced by the remote Intel Optane DC Persistent Memory. Misses due to HW prefetcher are not included.</msg>
    <msg name='%RemoteCacheCount'>Remote Cache Access Count</msg>
    <msg name='%RemoteCacheCountDescription'>This metric shows the total number of LLC misses serviced by the remote cache in other sockets. Misses due to HW prefetcher are not included.</msg>
    <msg name='%2LMCacheHitRatio'>DRAM Cache Hit Ratio</msg>
    <msg name='%2LMCacheHitRatioDescription'>This metric shows DRAM cache hit ratio when Intel Optane DC Persistent Memory is configured in volatile memory mode.</msg>
    <msg name='%2LMCacheHits'>DRAM Cache Hits</msg>
    <msg name='%2LMCacheHitsDescription'>This metric shows the total number of DRAM cache hits when Intel Optane DC Persistent Memory is configured in volatile memory mode.</msg>
    <msg name='%2LMCacheMisses'>DRAM Cache Misses</msg>
    <msg name='%2LMCacheMissesDescription'>This metric shows the total number of DRAM cache misses when Intel Optane DC Persistent Memory is configured in volatile memory mode.</msg>
    <msg name='%BandwidthDomain'>Bandwidth Domain</msg>
    <msg name='%DRAM'>DRAM, GB/sec</msg>
    <msg name='%DRAMSingle'>DRAM Single-Package, GB/sec</msg>
    <msg name='%DRAMRead'>DRAM Read, GB/sec</msg>
    <msg name='%DRAMWrite'>DRAM Write, GB/sec</msg>
    <msg name='%QPI'>QPI, GB/sec</msg>
    <msg name='%QPISingle'>QPI Outgoing Single-Link, GB/sec</msg>
    <msg name='%UPISingle'>UPI Outgoing Single-Link, GB/sec</msg>
    <msg name='%QPIIncoming'>QPI Incoming Data, GB/sec</msg>
    <msg name='%QPIIncomingNonData'>QPI Incoming Non-Data, GB/sec</msg>
    <msg name='%QPIOutgoing'>QPI Outgoing Data, GB/sec</msg>
    <msg name='%QPIOutgoingTotal'>QPI Outgoing, GB/sec</msg>
    <msg name='%UPIOutgoingTotal'>UPI Outgoing, GB/sec</msg>
    <msg name='%QPIOutgoingNonData'>QPI Outgoing Non-Data, GB/sec</msg>
    <msg name='%BandwidthUtilizationType'>Bandwidth Utilization Type</msg>
    <msg name='%BandwidthUtilizationValue'>Bandwidth Utilization</msg>
    <msg name='%BandwidthUtilizationInfo'>Bandwidth Utilization Histogram</msg>
    <msg name='%BandwidthUtilizationInfoDescription'>Explore bandwidth utilization over time using the histogram and identify memory objects or functions with maximum contribution to the high bandwidth utilization.</msg>
    <msg name='%BandwidthUtilizationChart'>Bandwidth Utilization Histogram</msg>
    <msg name='%BandwidthUtilizationChartDescription'>This histogram displays the wall time the bandwidth was utilized by certain value. Use sliders at the bottom of the histogram to define thresholds for Low, Medium and High utilization levels. You can use these bandwidth utilization types in the Bottom-up view to group data and see all functions executed during a particular utilization type. To learn bandwidth capabilities, refer to your system specifications or run appropriate benchmarks to measure them; for example, Intel Memory Latency Checker can provide maximum achievable DRAM and Interconnect bandwidth.</msg>
    <msg name='%HighBandwidthUtilizationObjects'>Top Memory Objects with High Bandwidth Utilization</msg>
    <msg name='%HighBandwidthUtilizationObjectsDescription'>This section shows top memory objects, sorted by LLC Misses, that were accessed when bandwidth utilization was high for the domain selected in the histogram area.</msg>
    <msg name='%HighBandwidthUtilizationFunctions'>Top Functions with High Bandwidth Utilization</msg>
    <msg name='%HighBandwidthUtilizationFunctionsDescription'>This section shows top functions, sorted by LLC Misses that were executing when bandwidth utilization was high for the domain selected in the histogram area.</msg>
    <msg name='%HighBandwidthUtilizationFunctionsKNLDescription'>This section shows top functions, sorted by L2 Input requests that were executing when bandwidth utilization was high for the domain selected in the histogram area.</msg>
    <msg name='%BandwidthDomainMax'>Platform Maximum</msg>
    <msg name='%BandwidthDomainMaxDescription'>Expected maximum bandwidth for the system. This value can be automatically estimated using micro-benchmark at the start of analysis or hard-coded based on theoretical bandwidth limits.</msg>
    <msg name='%BandwidthMax'>Observed Maximum</msg>
    <msg name='%BandwidthMaxDescription'>Maximum bandwidth observed during the analysis. If the value is close to the Platform Max your workload is probably bandwidth-limited.</msg>
    <msg name='%HighBandwidthTimePercents'>% of Elapsed Time with High BW Utilization</msg>
    <msg name='%HighBandwidthTimePercentsDescription'>Percentage of Elapsed time spent heavily utilizing system bandwidth.</msg>
    <msg name='%AverageBandwidthDescription'>Average bandwidth utilization during the analysis.</msg>
    <msg name='%HighBandwidthTimePercentsIssue'>The system spent much time heavily utilizing bandwidth.</msg>
    <msg name='%LowBandwidth'>Low</msg>
    <msg name='%MediumBandwidth'>Medium</msg>
    <msg name='%HighBandwidth'>High</msg>
    <msg name='%SlowLatency'>Slow</msg>
    <msg name='%GoodLatency'>Good</msg>
    <msg name='%FastLatency'>Fast</msg>
    <msg name='%MCDRAMFlat'>MCDRAM Flat, GB/sec</msg>
    <msg name='%MCDRAMCache'>MCDRAM Cache, GB/sec</msg>
    <msg name='%3DXPDomain'>Persistent Memory, GB/sec</msg>
    <msg name='%3DXPDomainSingle'>Persistent Memory Single-Package, GB/sec</msg>
    <msg name='%Average3DXPointBandwidth'>Average Persistent Memory Bandwidth, GB/sec</msg>
    <msg name='%RemoteToLocalRatio'>Remote / Local memory Ratio</msg>
    <msg name='%RemoteToLocalRatioDescription'>In NUMA (non-uniform memory architecture) machines, memory requests missing LLC may be serviced either by local or remote DRAM. Memory requests to remote DRAM incur much greater latencies than those to local DRAM. It is recommended to keep as much frequently accessed data local as possible. This metric is defined by the ratio of remote DRAM loads to local DRAM loads.</msg>
    <msg name='%RemoteToLocalRatioIssueTextAll'>A significant amount of DRAM loads were serviced from remote DRAM. Wherever possible, try to consistently use data on the same core, or at least the same package, as it was allocated on.</msg>
    <msg name='%RemoteToLocalPercent'>NUMA: % of Remote Accesses</msg>
    <msg name='%RemoteToLocalPercentDescription'>In NUMA (non-uniform memory architecture) machines, memory requests missing LLC may be serviced either by local or remote DRAM. Memory requests to remote DRAM incur much greater latencies than those to local DRAM. It is recommended to keep as much frequently accessed data local as possible. This metric shows percent of remote accesses, the lower the better.</msg>
    <msg name='%MCDRAMBandwidthGBperSec'>MCDRAM Bandwidth, GB/sec</msg>
    <msg name='%MCDRAMReadBandwidthGBperSec'>MCDRAM Read Bandwidth, GB/sec</msg>
    <msg name='%MCDRAMWriteBandwidthGBperSec'>MCDRAM Write Bandwidth, GB/sec</msg>
    <msg name='%MCDRAMBandwidthFlat'>MCDRAM Flat Mode Bandwidth</msg>
    <msg name='%MCDRAMBandwidthCache'>MCDRAM Cache Mode Bandwidth</msg>
    <msg name='%MCDRAMHitRate'>MCDRAM Hit Rate</msg>
    <msg name='%MCDRAMHitMRate'>MCDRAM HitM Rate</msg>
    <msg name='%MCDRAMBandwidth'>MCDRAM Bandwidth</msg>
    <msg name='%MCDRAM'>MCDRAM, GB/sec</msg>
    <msg name='%AverageCPUUtilizationOpenMP'>Effective CPU Utilization</msg>
    <msg name='%AverageCPUUtilizationOpenMPDescription'>This metric represents how efficiently the application utilized the CPUs available and helps evaluate the parallel efficiency of the application. It shows the percent of average CPU utilization by all logical CPUs on the system. CPU utilization metric is based only on the Effective time and does not include Spin and Overhead time. A CPU utilization of 100% means that all of the logical CPUs were loaded by computations of the application.</msg>
    <msg name='%AverageCPUUtilizationOpenMPIssue'>The metric value is low, which may signal a poor logical CPU cores utilization caused by load imbalance, threading runtime overhead, contended synchronization, or thread/process underutilization. Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism or run the Locks and Waits analysis to identify parallel bottlenecks for other parallel runtimes.</msg>
    <msg name='%AveragePhysicalCPUUtilizationOpenMPIssue'><![CDATA[<p>The metric value is low, which may signal a poor physical CPU cores utilization caused by:<ul><li>load imbalance</li><li>threading runtime overhead</li> <li>contended synchronization</li><li>thread/process underutilization</li><li>incorrect affinity that utilizes logical cores instead of physical cores</li></ul>Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism or run the Locks and Waits analysis to identify parallel bottlenecks for other parallel runtimes.</p>]]></msg>
    <msg name='%AverageCPUUtilizationOpenMPLogicalOnlyIssue'>The metric value is low, which may signal a poor utilization of logical CPU cores while the utilization of physical cores is acceptable. Consider using logical cores, which in some cases can improve processor throughput and overall performance of multi-threaded applications.</msg>
    <msg name='%AverageCPUUtilizationOpenMPLogicalAndPhysicalIssue'>The metric value is low, which may signal a poor logical CPU cores utilization. Consider improving physical core utilization as the first step and then look at opportunities to utilize logical cores, which in some cases can improve processor throughput and overall performance of multi-threaded applications.</msg>
    <msg name='%AverageCPUUtilizationAPSDescription'>How many of the logical CPU cores are used by your application?  This metric helps evaluate the parallel efficiency of your application.  It estimates the percentage of all the logical CPU cores in the system that is spent in your application -- without including the overhead introduced by the parallel runtime system. 100% utilization means that your application keeps all the logical CPU cores busy for the entire time that it runs.</msg>
    <msg name='%AverageCPUUtilizationAPSIssueLong'>A low metric value can indicate poor CPU utilization because of load imbalance, synchronization contention, insufficient work, too much I/O, or overhead in a threading runtime systems.  Perform function or source line-level profiling to define particular reasons of CPU underutilization. Learn more at https://software.intel.com/en-us/concurrency-analysis-win</msg>
    <msg name='%AverageCPUUtilizationAPSIssueShort'>A low value indicates that your application is not using all of the logical CPU cores. In addition to time spent in your application, a significant amount of time is spent inside parallel runtime libraries.</msg>
    <msg name='%AverageCPUUtilizationAPSIssueMPI'>%PMUMpiTimePercents% of available CPU time is spent in the MPI runtime. This negatively impacts the scalability of your application. Possible causes include load imbalance between ranks, excessive communications, or sub-optimal of MPI library parameter. You can use MPI profiling tools to explore how efficiently you are using MPI. Learn more at https://software.intel.com/en-us/get-started-with-itac</msg>
    <msg name='%AverageCPUUtilizationAPSIssueOpenMP'>%PMUOpenMPTimePercents% of available CPU time is spent in the OpenMP runtime. Possible causes include load imbalance, scheduling overhead, lock contention, or serial regions. You can use OpenMP profiling tools to explore how efficiently you are using OpenMP. Learn more at https://software.intel.com/en-us/openmp-analysis-win</msg>
    <msg name='%AverageCPUUtilizationAPSIssueCilk'>%PMUCilkTimePercents% of available CPU time is spent in the Cilk runtime. Possible causes include load imbalance, parallel work arrangement overhead, or scheduling overhead. You can use thread profiling tools to explore how efficiently you are using Cilk. Learn more at https://software.intel.com/en-us/concurrency-analysis-win</msg>
    <msg name='%AverageCPUUtilizationAPSIssueTBB'>%PMUTbbTimePercents% of available CPU time was spent in the TBB runtime. Possible causes include user task load imbalance or scheduling overhead due to a low amount of parallelism. Learn more at https://software.intel.com/en-us/concurrency-analysis-win</msg>
    <msg name='%AverageCPUUtilizationAPSIssuePthreads'>%PMUPthreadsTimePercents% of available CPU time was spent in the thread runtime. Possible causes include load imbalance, parallel work overhead, or lock contention. You can use thread profiling tools to explore how efficiently you are using threads. Learn more at https://software.intel.com/en-us/concurrency-analysis-win</msg>
    <msg name='%IceCbbBand'>CBB</msg>
    <msg name='%IceCbbDescription'>Compute Building Block</msg>
    <msg name='%IceCbbStateBand'>Computing Node</msg>
    <msg name='%IceCbbStateIndex'>CBB State</msg>
    <msg name='%IceBidBand'>BID</msg>
    <msg name='%IceCbbExecStateRunningData'>Running Time</msg>
    <msg name='%IceCbbExecStateSuspendedData'>Suspended Time</msg>
    <msg name='%IceCbbExecStateIdleData'>Idle Time</msg>
    <msg name='%IceCbbExecStateSuspendedDataWaitReason'>Wait Reason</msg>
    <msg name='%CbbRunning'>Running</msg>
    <msg name='%CbbWaitingForInputCredits'>Input BIDs</msg>
    <msg name='%CbbWaitingForOutputCredits'>Output BIDs</msg>
    <msg name='%CbbCreditsAvailableButNotRunning'>Other</msg>
    <msg name='%CbbInterruptedByMFW'>Interrupted by MFW</msg>
    <msg name='%CbbIdle'>Idle</msg>
    <msg name='%CbbUnknown'>Unknown</msg>
    <msg name='%IceBidCreditsStateData'>BID assignment</msg>
    <msg name='%IceBidType'>Type</msg>
    <msg name='%IceBidCreditsNumber'>Credits</msg>
    <msg name='%IceBidTile'>Tile</msg>
    <msg name='%IceCbCommandDesc'>CMD</msg>
    <msg name='%IceCbCommandId'>CMD ID</msg>
    <msg name='%IceBidCreditsExpandedByBid'>Credits by BID</msg>
    <msg name='%IceCbbExecStateElapsedTime'>Elapsed Time</msg>
    <msg name='%IceCbbExecStateElapsedTimeDescription'>Elapsed Time is time during which there is a kernel that is executing on the CBB or it is not running for some reason.</msg>
    <msg name='%IceBidExecStateTime'>Execution Time</msg>
    <msg name='%IceBidExecState'>BID State</msg>
    <msg name='%IceCbbExecStateSuspendedTimeExpanded'>Suspended Time by Wait Reason</msg>
    <msg name='%IceCbbExecStateRunningTimeDescription'>Running Time is execution time during which there is a kernel that is executing on the CBB.</msg>
    <msg name='%TotalIceCbbExecutionTimeRunningDescription'>Total execution time during which there is a kernel that is executing on the CBB.</msg>
    <msg name='%IceCbbExecStateSuspendedTimeDescription'>Suspended Time is time during which the CBB is waiting for input/output buffers or has everything data to execute, but not running for some reason.</msg>
    <msg name='%IceCbbExecStateSuspendedTimeDerivedDescription'>Suspended Time is time during which the CBB is waiting for input/output buffers or has everything data to execute, but not running for some reason. Suspended time expansion by wait reason is based on BID granularity, not on tracking actually required credits number to execute over involved BIDs.</msg>
    <msg name='%TotalIceCbbExecutionTimeRunning'>Total CBB Running Time</msg>
    <msg name='%IceCbbExecStateTimeWaitingForInputCreditsDescription'>Suspended Time during which the CBB is waiting for input buffers. This means there is at least one input buffer for the CBB with no credits granted. Actually granted credits number on other input buffers for the CBB may be less than required to execute CBB.</msg>
    <msg name='%IceCbbExecStateTimeWaitingForOutputCreditsDescription'>Suspended Time during which the CBB is waiting for output buffers while all input buffers are assigned. This means there is at least one output buffer with no credits granted while there are non-zero credits granted on each input buffer for the CBB. Actually granted credits number on input/output buffers for the CBB may be less than required to execute CBB.</msg>
    <msg name='%IceCbbExecStateTimeCreditsAvailableButNotRunningDescription'>Suspended Time during which the CBB has all input/output buffers assigned, but not running for some reason. Actually granted credits number on input/output buffers for the CBB may be less than required to execute CBB.</msg>
    <msg name='%IceCbbExecStateTimeInterruptedByMFWDescription'>Suspended Time during which the CBB is interrupted by Master Firmware.</msg>
    <msg name='%BidProcessing'>Processing</msg>
    <msg name='%BidAcquired'>Acquired</msg>
    <msg name='%BidReleased'>Released</msg>
    <msg name='%IceBidExecTimeProcessing'>Processing Time</msg>
    <msg name='%IceBidExecTimeReleased'>Released Time</msg>
    <msg name='%IceBidExecTimeAcquired'>Acquired Time</msg>
    <msg name='%IceCbTimeframeData'>Command Buffer</msg>
    <msg name='%IceNetworkData'>Network</msg>
    <msg name='%IceCbTimeframeData'>Command Buffer</msg>
    <msg name='%IceCbTimeframeTime'>CB Elapsed Time</msg>
    <msg name='%IceCbTimeframeNetworkName'>Network</msg>
    <msg name='%IceCbTimeframeNetworkId'>Network UID</msg>
    <msg name='%IceCbTimeframeCbId'>CB ID</msg>
    <msg name='%IceCbTimeframeCbName'>Command Buffer</msg>
    <msg name='%IceNetworkName'>Name</msg>
    <msg name='%IceNetworkId'>UID</msg>
    <msg name="%IceLayerBand">Layer</msg>
    <msg name="%IceLayerId">Layer ID</msg>
    <msg name='%True'>True</msg>
    <msg name='%False'>False</msg>
    <msg name='%IceAliasUnknownNetworkName'>[Outside any network]</msg>
    <msg name='%IceAliasUnknownLayerName'>[Outside any layer]</msg>
    <msg name='%IceAliasUnknownCbName'>[Outside any command buffer]</msg>
    <msg name="%IceTotalNetworkCount">Total Network Count</msg>
    <msg name="%IceTotalCommandBufferCount">Total Command Buffer Count</msg>
    <msg name="%IceTransitions">Transitions</msg>
    <msg name="%IceDeviceBand">ICE</msg>
    <msg name='%IceDeviceBandDescription'>Inference Compute Engine</msg>
    <msg name="%IceDeviceIndex">ICE ID</msg>
    <msg name="%IceTotalDeviceCount">Total ICE Count</msg>
    <msg name="%IceDebugMessageData">Debug Message</msg>
    <msg name="%IceDebugMessageMsg">Message</msg>
    <msg name="%IceDebugMessageSourceFile">Source File</msg>
    <msg name="%IceDebugMessageLine">Line</msg>
    <msg name="%IceTotalRunningCbbCount">CBB Concurrency</msg>
    <msg name="%IceBidExecStateCount">Total Buffer Count</msg>
    <msg name="%IceRawCnCMessageData">Raw CnC Message</msg>
    <msg name="%IceRawCnCMessagePrefix">Type</msg>
    <msg name="%IceRawCnCMessageHeader">Header</msg>
    <msg name="%IceRawCnCMessageBody">Body</msg>
    <msg name="%IceRawCnCMessageCount">Raw CnC Message Count</msg>
    <msg name="%IceTlcBarrierData">TLC Barrier</msg>
    <msg name="%IceBidCreditsStateCount">BID Assignment Count</msg>
    <msg name="%IceFrequency">ICE Frequency</msg>
    <msg name="%IceAverageFrequency">Average Frequency</msg>
    <msg name="%IceATUMissRate">ATU Miss Rate</msg>
    <msg name="%IceAverageATUMissRate">Average ATU Miss Rate</msg>
    <msg name="%IceRequestedMemoryUsage">Requested Memory Usage</msg>
    <msg name="%IceAverageRequestedMemoryUsage">Average Requested Memory Usage</msg>
    <msg name='%IODevice'>Storage Device</msg>
    <msg name='%IOPartition'>Partition</msg>
    <msg name='%IOBinDurationType'>I/O Operation Duration Type</msg>
    <msg name='%IoWaitTime'>I/O Wait Time</msg>
    <msg name='%IoWaitTimeDescription'>This metric represents a portion of time when threads reside in I/O wait state while there are idle cores on the system</msg>
    <msg name='%PageFaultInfo'>Page Faults</msg>
    <msg name='%PageFaultCount'>Page Fault Count</msg>
    <msg name='%PageFaultStat'>Page Faults</msg>
    <msg name='%DRAMPeakSysBW'>Max DRAM System Bandwidth</msg>
    <msg name='%DRAMPeakSysBWDescription'>Maximum DRAM bandwidth measured for the whole system (across all packages) by running a micro-benchmark before the collection starts. If the system has already been actively loaded at the moment of collection start (for example, with the attach mode), the value may be less accurate.</msg>
    <msg name='%DRAMPeakLocalBW'>Max DRAM Single-Package Bandwidth</msg>
    <msg name='%DRAMPeakLocalBWDescription'>Maximum DRAM bandwidth for single package measured by running a micro-benchmark before the collection starts. If the system has already been actively loaded at the moment of collection start (for example, with the attach mode), the value may be less accurate.</msg>
    <msg name='%IstpCStateWakeUpObject'>C-State Wake-up Object</msg>
    <msg name='%IstpSStateWakeUpObject'>S-State Wake-up Object</msg>
    <msg name='%IstpMStateWakeUpObject'>Module C-State Wake-up Object</msg>
    <msg name='%SleepBlocker'>Sleep Blocker</msg>
    <msg name='%SleepBlockerInstanceCount'>Sleep Blocker Count</msg>
    <msg name='%InterruptRaiseCount'>Interrupt Raise Count</msg>
    <msg name='%InterruptRaise'>Interrupt Raise</msg>
    <msg name='%PMUCoreFrequencyDescription'>Frequency calculated using CPU unhalted clockcycles. It is a software frequency showing the ratio of clockticks per time. When the CPU is fully loaded, the metric indicates the CPU frequency. If the CPU is not fully loaded, it may indicate an average CPU load over time. The smaller the sampling interval is, the closer the metric is to the real HW frequency.</msg>
    <msg name='%CPUFrequencyFromAPerfMPerfDescription'>Frequency calculated as an average P-state frequency between samples.</msg>
    <msg name='%DRAMBandwidthBoundUncore'>DRAM Bandwidth Bound</msg>
    <msg name='%QPIBandwidthBoundUncore'>QPI Bandwidth Bound</msg>
    <msg name='%UPIBandwidthBoundUncore'>UPI Bandwidth Bound</msg>
    <msg name='%UPIUtilization'>UPI Utilization</msg>
    <msg name='%UPIUtilizationBound'>UPI Utilization Bound</msg>
    <msg name='%UPIUtilizationPercent'>UPI Utilization Outgoing, (%)</msg>
    <msg name='%UPIUtilizationSingle'>UPI Utilization Single-link, (%)</msg>
    <msg name='%UPIUtilizationDescription'>This metric represents percentage of elapsed time the system spent with high UPI utilization. Explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required.</msg>
    <msg name='%UPIUtilizationIssueText'>The system spent much time heavily utilizing UPI bandwidth. Improve data accesses using NUMA optimizations on a multi-socket system.</msg>
    <msg name='%DRAMBandwidthBoundUncoreDescription'>This metric represents percentage of elapsed time the system spent with high DRAM bandwidth utilization. Since this metric relies on the accurate peak system DRAM bandwidth measurement, explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required.</msg>
    <msg name='%QPIBandwidthBoundUncoreDescription'>This metric represents percentage of elapsed time the system spent with high QPI/UPI bandwidth utilization. Since this metric relies on the accurate peak system QPI/UPI bandwidth measurement, explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required.</msg>
    <msg name='%QPIBandwidthBoundUncoreIssueText'>The system spent much time heavily utilizing QPI/UPI bandwidth. Improve data accesses using NUMA optimizations on a multi-socket system.</msg>
    <msg name='%DRAMBandwidthBoundUncoreIssueText'>The system spent much time heavily utilizing DRAM bandwidth. Improve data accesses to reduce cacheline transfers from/to memory using these possible techniques: 1) consume all bytes of each cacheline before it is evicted (for example, reorder structure elements and split non-hot ones); 2) merge compute-limited and bandwidth-limited loops; 3) use NUMA optimizations on a multi-socket system. Note: software prefetches do not help a bandwidth-limited application. Run Memory Access analysis to identify data structures to be allocated in High Bandwidth Memory (HBM), if available.</msg>
    <msg name='%DRAMBandwidthBoundUncoreMADescription'>This metric represents percentage of elapsed time the system spent with high DRAM bandwidth utilization. Since this metric relies on the accurate peak system DRAM bandwidth measurement, explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required.</msg>
    <msg name='%DRAMBandwidthBoundUncoreMAIssueText'>The system spent much time heavily utilizing DRAM bandwidth. Improve data accesses to reduce cacheline transfers from/to memory using these possible techniques: 1) consume all bytes of each cacheline before it is evicted (for example, reorder structure elements and split non-hot ones); 2) merge compute-limited and bandwidth-limited loops; 3) use NUMA optimizations on a multi-socket system. Note: software prefetches do not help a bandwidth-limited application. Consider moving the data structures, access to which is bandwidth bound, to High Bandwidth Memory (HBM), if available.</msg>
    <msg name='%MCDRAMFlatBandwidthBoundUncore'>MCDRAM Flat Bandwidth Bound</msg>
    <msg name='%MCDRAMFlatBandwidthBoundUncoreDescription'>This metric represents percentage of elapsed time the system spent with high MCDRAM Flat bandwidth utilization. Review the Bandwidth Utilization Histogram to estimate the scale of the issue.</msg>
    <msg name='%MCDRAMFlatBandwidthBoundUncoreIssueText'>The system spent a significant percentage of elapsed time with high MCDRAM Flat bandwidth utilization. Review the Bandwidth Utilization Histogram to estimate the scale of the issue. Consider improving data locality and/or merging compute-intensive code with bandiwdth-intensive code.</msg>
    <msg name='%MCDRAMCacheBandwidthBoundUncore'>MCDRAM Cache Bandwidth Bound</msg>
    <msg name='%MCDRAMCacheBandwidthBoundUncoreDescription'>This metric represents percentage of elapsed time the system spent with high MCDRAM Cache bandwidth utilization. Review the Bandwidth Utilization Histogram to estimate the scale of the issue.</msg>
    <msg name='%MCDRAMCacheBandwidthBoundUncoreIssueText'>The system spent a significant percentage of elapsed time with high MCDRAM Cache bandwidth utilization. Review the Bandwidth Utilization Histogram to estimate the scale of the issue. Consider improving data locality and L1/L2 cache reuse.</msg>
    <msg name='%MemBoundGroup'>Memory Bound Group</msg>
    <msg name='%MCDRAMBandwidthBoundUncore'>MCDRAM Bandwidth Bound</msg>
    <msg name='%MCDRAMBandwidthBoundUncoreDescription'>This metric represents percentage of elapsed time the system spent with high MCDRAM bandwidth utilization. Review the Bandwidth Utilization Histogram to estimate the scale of the issue.</msg>
    <msg name='%MCDRAMBandwidthBoundUncoreIssueText'>The system spent a significant percentage of elapsed time with high MCDRAM bandwidth utilization. Review the Bandwidth Utilization Histogram to estimate the scale of the issue. Consider improving data locality and L1/L2 cache reuse.</msg>
    <msg name='%3DXPointBandwidthBoundUncore'>Persistent Memory Bandwidth Bound</msg>
    <msg name='%3DXPointBandwidthBoundUncoreMADescription'>This metric represents percentage of elapsed time the system spent with high Intel Optane DC Persistent Memory bandwidth utilization. Since this metric relies on the accurate peak system Intel Optane DC Persistent Memory bandwidth measurement, explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required.</msg>
    <msg name='%3DXPointBandwidthBoundUncoreMAIssueText'>The system spent a significant percentage of elapsed time with high Intel Optane DC Persistent Memory bandwidth utilization.</msg>
    <msg name='%PCIeBandwidthBoundUncore'>PCIe Bandwidth Bound</msg>
    <msg name='%PCIeBandwidthBoundUncoreDescription'>This metric represents percentage of elapsed time the system spent with high PCIe utilization. Explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required.</msg>
    <msg name='%PCIeBandwidthBoundUncoreIssueText'>The system spent much time heavily utilizing PCIe bandwidth.</msg>
    <msg name='%BranchSource'>FromAddress</msg>
    <msg name='%BranchType'>TypeOfBranch</msg>
    <msg name='%BranchHitCount'>HitCount</msg>
    <msg name='%BranchDestinationAddress'>DestinationAddress</msg>
    <msg name='%BranchDestinationBbEndAddress'>DestinationEndOfBlockAddress</msg>
    <msg name='%BranchSourceSourceLineRelative'>FromSourceOffsetLine</msg>
    <msg name='%BranchDestinationBBEndSourceLineRelative'>DestinationSourceOffsetLine</msg>
    <msg name='%BranchSourceFunctionInstanceType'>FromFunctionInlineFlag</msg>
    <msg name='%BranchDestinationBBEndFunctionInstanceType'>DestinationFunctionInlineFlag</msg>
    <msg name='%BranchSourceSourceFile'>FromSourceFile</msg>
    <msg name='%BranchSourceSourceLine'>FromSourceLine</msg>
    <msg name='%BranchSourceSourceColumn'>FromSourceColumn</msg>
    <msg name='%BranchSourceFunctionMangled'>FromFunctionMangled</msg>
    <msg name='%BranchDestinationBBEndSourceFile'>DestinationSourceFile</msg>
    <msg name='%BranchDestinationBBEndSourceLine'>DestinationSourceLine</msg>
    <msg name='%BranchDestinationBBEndSourceColumn'>DestinationSourceColumn</msg>
    <msg name='%BranchDestinationBBEndFunctionMangled'>DestinationFunctionMangled</msg>
    <msg name='%SourceDiscriminator'>Discriminator</msg>
    <msg name='%BranchSourceSourceDiscriminator'>FromSourceDiscriminator</msg>
    <msg name='%BranchDestinationSourceDiscriminator'>DestinationSourceDiscriminator</msg>
    <msg name='%FunctionType'>Function Type</msg>
    <msg name='%FunctionInstanceType'>Function Instance Type</msg>
    <msg name='%WaitSyncObjCreation'>Object Creation</msg>
    <msg name='%SystemOverhead'>System Overhead</msg>
    <msg name='%Executor'>Executor</msg>
    <msg name='%FunctionSubtype'>Function Subtype</msg>
    <msg name='%BusyWaitOnBarrier'>Busy Wait On Barrier</msg>
    <msg name='%BusyWaitOnLock'>Busy Wait On Lock</msg>
    <msg name='%MPISpinning'>MPI Spinning</msg>
    <msg name='%WorkForking'>Work Forking</msg>
    <msg name='%WorkScheduling'>Work Scheduling</msg>
    <msg name='%Reduction'>Reduction</msg>
    <msg name='%Atomics'>Atomics</msg>
    <msg name='%L1DRequestsMiss'>L1 Cache Misses</msg>
    <msg name='%L1DRequestsAny'>L1 Cache Requests</msg>
    <msg name='%L2RequestsMiss'>L2 Cache Misses</msg>
    <msg name='%L2RequestsAny'>L2 Cache Requests</msg>
    <msg name='%L3RequestsMiss'>L3 Cache Misses</msg>
    <msg name='%L3RequestsAny'>L3 Cache Requests</msg>
    <msg name='%DramCacheRequestsMiss'>MCDRAM Cache Misses</msg>
    <msg name='%DramCacheRequestsAny'>MCDRAM Cache Requests</msg>
    <msg name='%eDRAM'>eDRAM, GB/sec</msg>
    <msg name='%eDRAMReadGB'>eDRAM Read, GB</msg>
    <msg name='%eDRAMReadBandwidth'>eDRAM Read, GB/sec</msg>
    <msg name='%eDRAMWriteGB'>eDRAM Write, GB</msg>
    <msg name='%eDRAMWriteBandwidth'>eDRAM Write, GB/sec</msg>
    <msg name='%eDRAMTotal'>eDRAM Total, GB</msg>
    <msg name='%eDRAMTotalBandwidth'>eDRAM Total, GB/sec</msg>
    <msg name='%GpuL3Bound'>GPU L3 Bandwidth Bound</msg>
    <msg name='%GpuL3BoundDescription'>This metric shows how often the GPU was idle or stalled on the L3 cache.</msg>
    <msg name='%GpuL3BoundDescriptionInKernel'>This metric shows how often the GPU was idle or stalled on the L3 cache (% of peak value).</msg>
    <msg name='%GpuL3BoundIssueText'>L3 bandwidth was high when EUs were stalled or idle. Consider improving cache reuse.</msg>
    <msg name='%PacketStage'>GPU Render and EU Engine State</msg>
    <msg name='%MPIImbalance'>MPI Imbalance</msg>
    <msg name='%MPIImbalanceDescription'>MPI Imbalance shows the CPU time spent by ranks spinning in waits on communication operations, normalized by the number of ranks. High metric value can be caused by application workload imbalance between ranks, nonoptimal communication schema or settings of MPI library. Explore details on communication inefficiencies with Intel Trace Analyzer and Collector.</msg>
    <msg name='%MPIImbalanceLowBusyWaitIssueText'>MPI Imbalance is significant and can negatively impact the application performance and scalability. This can be caused by application workload imbalance between ranks since minimum MPI Busy Wait time by ranks on the node is small. Explore performance of the rank on the critical path to look for opportunities to speed up the whole application.</msg>
    <msg name='%MPIImbalanceHighBusyWaitIssueText'>MPI Imbalance is significant and can negatively impact the application performance and scalability. High metric value can be caused by application workload imbalance between ranks, nonoptimal communication schema or settings of MPI library. Explore details on communication inefficiencies with Intel Trace Analyzer and Collector.</msg>
    <msg name='%CriticalMPIRank'>MPI Rank on the Critical Path</msg>
    <msg name='%CriticalMPIRankDescription'>The section contains metrics for the rank with minimum MPI Busy Wait time that is on the critical path of application execution on this node. Consider exploring CPU utilization efficiency for this rank.</msg>
    <msg name='%GPUGTPinCycles'>Estimated GPU Cycles</msg>
    <msg name='%GPUGTPinCyclesDesc'>The number of cycles spent by GPU executing the profiled instructions.</msg>
    <msg name='%GPUGTPinInstructionCount'>GPU Instructions Executed</msg>
    <msg name='%GPUGTPinInstLatency'>Average Latency, Cycles</msg>
    <msg name='%GPUGTPinInstLatencyDesc'>The average latency of memory read and synchronization instructions, in cycles.</msg>
    <msg name='%GPUGTPinMemReadLatency'>Memory Reads</msg>
    <msg name='%GPUGTPinMemReadLatencyDesc'>The average latency of memory read instructions, in cycles.</msg>
    <msg name='%GPUGTPinSyncLatency'>Synchronization</msg>
    <msg name='%GPUGTPinSyncLatencyDesc'>The average latency of synchronization instructions, in cycles.</msg>
    <msg name='%GPUGTPinInstructionMixDescription'>GPU Instruction Mix is a breakdown of instructions executed by the kernel shown in the following categories:</msg>
    <msg name='%GPUGTPinInstructionType'>Instruction Type</msg>
    <msg name='%GPUGTPinInstructionSimdUtilization'>SIMD Utilization</msg>
    <msg name='%GPUGTPinInstructionSimdUtilizationDesc'>A ratio of active SIMD lanes to the width of the SIMD instructions.</msg>
    <msg name='%gSim16bitGOPsPerClk'>16-bit OP/clk</msg>
    <msg name='%gSim16bitGOPsPerClkDescription'>Number of 16-bit operations per clocktick</msg>
    <msg name='%gSim16bitIOPsPerClk'>Int16 IOP/clk</msg>
    <msg name='%gSim16bitIOPsPerClkDescription'>Number of 16-bit integer operations per clocktick</msg>
    <msg name='%gSimHpFLOPsPerClk'>HP FLOP/clk</msg>
    <msg name='%gSimHpFLOPsPerClkDescription'>Number of half-precision floating point operations per clocktick</msg>
    <msg name='%gSim32bitGOPsPerClk'>32-bit OP/clk</msg>
    <msg name='%gSim32bitGOPsPerClkDescription'>Number of 32-bit operations per clocktick</msg>
    <msg name='%gSim32bitIOPsPerClk'>Int32 IOP/clk</msg>
    <msg name='%gSim32bitIOPsPerClkDescription'>Number of 32-bit integer operations per clocktick</msg>
    <msg name='%gSimSpFLOPsPerClk'>SP FLOP/clk</msg>
    <msg name='%gSimSpFLOPsPerClkDescription'>Number of single precision floating point operations per clocktick</msg>
    <msg name='%gSim64bitGOPsPerClk'>64-bit OP/clk</msg>
    <msg name='%gSim64bitGOPsPerClkDescription'>Number of 64-bit operations per clocktick</msg>
    <msg name='%gSim64bitIOPsPerClk'>Int64 IOP/clk</msg>
    <msg name='%gSim64bitIOPsPerClkDescription'>Number of 64-bit integer operations per clocktick</msg>
    <msg name='%gSimDpFLOPsPerClk'>DP FLOP/clk</msg>
    <msg name='%gSimDpFLOPsPerClkDescription'>Number of double precision floating point operations per clocktick</msg>
    <msg name='%LLCDemandFractionKNL'>Demand Misses</msg>
    <msg name='%LLCDemandFractionKNLDescription'>A percentage of total L2 cache input requests caused by demand loads.</msg>
    <msg name='%LLCPrefetcherFractionKNL'>HW Prefetcher</msg>
    <msg name='%LLCPrefetcherFractionKNLDescription'>A percentage of total L2 cache input requests caused by hardware prefetcher.</msg>
    <msg name='%LLCPrefetcherFractionKNLIssue'>A significant fraction of L2 input requests is caused by hardware prefetcher while the system spent much time heavily utilizing DRAM or MCDRAM bandwidth.</msg>
    <msg name='%LLCInputRequestsKNL'>L2 Input Requests</msg>
    <msg name='%LLCInputRequestsKNLDescription'>A total number of L2 allocations. This metric accounts for both demand loads and HW prefetcher requests.</msg>
    <msg name='%LLCInputRequestsKNLForUnits'>of L2 Input Requests</msg>
    <msg name='%LLCInputRequestsPrefetcherKNL'>L2 HW Prefetcher Allocations</msg>
    <msg name='%LLCInputRequestsPrefetcherKNLDescription'>The number of L2 allocations caused by HW Prefetcher.</msg>
    <msg name='%DpdkRxBurstNumFetchedPackets'>Number of packets fetched on a single rte_eth_rx_burst() call</msg>
    <msg name='%DpdkRxBurstCallTypeCount'>Calls</msg>
    <msg name='%DpdkRxBurstCallDomain'>Statistics Domain</msg>
    <msg name='%DpdkRxSpinTime'>DPDK Rx Spin Time</msg>
    <msg name='%DpdkRxSpinTimeDescription'>The portion of rte_eth_rx_burst() calls that return zero packets</msg>
    <msg name='%DpdkDequeueNumPackets'>Number of events dequeued on a single rte_event_dequeue_burst() call</msg>
    <msg name='%DpdkDequeueCallCount'>Calls</msg>
    <msg name='%DpdkDequeueDomain'>Dequeue Statistics Domain</msg>
    <msg name='%DpdkDequeueSpinTime'>DPDK Event Dequeue Spin Time</msg>
    <msg name='%DpdkDequeueSpinTimeDescription'>The portion of rte_event_dequeue_burst() calls that dequeue zero events</msg>
    <msg name='%SpdkIoTime'>SPDK Time</msg>
    <msg name='%spdkIoType'>SPDK Operation Type</msg>
    <msg name='%spdkIoBinValue'>SPDK Throughput, MB/sec</msg>
    <msg name='%spdkIoLatencyBinValue'>SPDK Operations Latency, sec</msg>
    <msg name="%spdkIoOperation">SPDK Throughput Utilization</msg>
    <msg name="%spdkIoOperationLatency">SPDK Operations Latency</msg>
    <msg name='%spdkIOBinType'>SPDK Device</msg>
    <msg name='%spdkIOBinValueType'>SPDK Throughput Utilization</msg>
    <msg name='%spdkIOLatencyBinValueType'>SPDK Operations Latency</msg>
    <msg name='%spdkIoDevice'>SPDK Device</msg>
    <msg name='%SpdkIoRead'>Read</msg>
    <msg name='%SpdkIoWrite'>Write</msg>
    <msg name='%SpdkIoReads'>Reads</msg>
    <msg name='%SpdkIoWrites'>Writes</msg>
    <msg name='%SpdkIoTotal'>Total</msg>
    <msg name='%SpdkIoReadsBytes'>Read Bytes</msg>
    <msg name='%SpdkIoWritesBytes'>Written Bytes</msg>
    <msg name='%SpdkIoTotalBytes'>Total Bytes</msg>
    <msg name='%SpdkIoEffectiveTime'>SPDK Effective Time</msg>
    <msg name='%SpdkIoEffectiveTimeDescription'>The amount of time the SPDK is effectively interacting with devices.</msg>
    <msg name='%GPUElapsedTimeForUnit'>of Elapsed time</msg>
    <msg name='%GPUPeakForUnit'>of peak value</msg>
    <msg name='%GPUElapsedTimeWhenBusyForUnit'>of Elapsed time with GPU busy</msg>
    <msg name='%GPUDRAMBoundWhenBusy'>DRAM Bandwidth Bound</msg>
    <msg name='%GPUDRAMBoundWhenBusyDescription'>This metric represents percentage of elapsed time the system spent heavily utilizing DRAM bandwidth while Render GPGPU engine was active. Since this metric relies on the accurate peak system DRAM bandwidth measurement, explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required.</msg>
    <msg name='%GPUDRAMBoundWhenBusyIssueText'>The system spent much time heavily utilizing DRAM bandwidth. Improve data accesses to reduce cacheline transfers from/to memory using these possible techniques: 1) consume all bytes of each cacheline before it is evicted; 2) merge compute-limited and bandwidth-limited loops.</msg>
    <msg name='%GPUMetricConfidenceText'>This metric cannot be reliably calculated due to a low number of collected samples.</msg>
    <msg name="%SerialCPUTime">Serial CPU Time</msg>
    <msg name="%SerialCPUTimeDescription">Serial CPU Time is the time spent by the application outside any OpenMP region in the master thread during collection. It directly impacts application Collection Time and scaling. High values signal a performance problem to be solved via code parallelization or algorithm tuning.</msg>
    <msg name="%OtherSerialCPUTimeDescription">This metric shows unclassified Serial CPU Time.</msg>
    <msg name="%GPULowOccupancyIssue">The performance is limited by low occupancy. Consider increasing the number of working items.</msg>
    <msg name="%GPUHighOccupancyIssue">The performance is limited by low occupancy. Consider adding more work to working items.</msg>
    <msg name="%GPUMiscIssue">The performance is limited by low occupancy. Consider reducing the usage of SLM or barriers.</msg>
    <msg name="%AverageDRAMBandwidth">Average DRAM Bandwidth, GB/s</msg>
    <msg name="%AverageMCDRAMFlatBandwidth">Average MCDRAM Flat Bandwidth, GB/s</msg>
    <msg name="%AverageMCDRAMCacheBandwidth">Average MCDRAM Cache Bandwidth, GB/s</msg>
    <msg name="%AverageMCDRAMBandwidth">Average MCDRAM Bandwidth, GB/s</msg>
    <msg name="%GPUOccupancyNotEnoughSamples">Occupancy metric cannot be reliably calculated due to a low number of collected samples for this kernel.</msg>
    <msg name="%GPUSamplerBusyNotEnoughSamples">Sampler Busy metric cannot be reliably calculated due to a low number of collected samples for this kernel.</msg>
    <msg name="%GPUL3BandwidthNotEnoughSamples">L3 bandwidth cannot be reliably calculated due to a low number of collected samples for this kernel.</msg>
    <msg name="%IptDomain">Code Region Of Interest</msg>
    <msg name="%IptRegion">Code Region Of Interest (Instance)</msg>
    <msg name="%IptRegionTime">Elapsed Time</msg>
    <msg name='%AliasUnknownIptRegionDomain'>[Outside any ipt region]</msg>
    <msg name='%AliasUnknownIptRegion'>[Outside any ipt region instance]</msg>
    <msg name='%AveragePhysicalCPUUtilizationOpenMP'>Effective Physical Core Utilization</msg>
    <msg name='%AveragePhysicalCPUUtilizationOpenMPDescription'>This metric represents how efficiently the application utilized the physical CPU cores available and helps evaluate the parallel efficiency of the application. It shows the percent of average utilization by all physical CPU cores on the system. Effective Physical Core Utilization contains only effective time and does not contain spin and overhead. An utilization of 100% means that all of the physical CPU cores were loaded by computations of the application.</msg>
    <msg name='%PhysicalCPUUsageAverageOpenMP'>Average Effective Physical Core Utilization</msg>
    <msg name='%PhysicalCPUUsageAverageDescription'>The metrics shows average CPU physical cores utilization by computations of the application. Spin and Overhead time are not counted. Ideal usage is equal to the number of physical CPU cores.</msg>
    <msg name='%AverageCPUUtilizationOpenMPBigCores'>Effective Logical Core Utilization</msg>
    <msg name='%AverageCPUUsageOpenMPWithThreadConcurrencyBigCores'>Average Effective Logical Core Utilization</msg>
    <msg name="%PhysicalCPUUsageAverage">Average Physical Core Utilization</msg>
    <msg name='%PhysicalCPUUsageAverageDescription'>The metrics shows average physical cores utilization by computations of the application. Spin and Overhead time are not counted. Ideal average CPU utilization is equal to the number of physical CPU cores.</msg>
    <msg name="%LogicalCPUUsageAverage">Average Logical Core Utilization</msg>
    <msg name='%LogicalCPUUsageAverageDescription'>The metrics shows average logical cores utilization by computations of the application. Spin and Overhead time are not counted. Ideal average CPU utilization is equal to the number of logical CPU cores.</msg>
    <msg name='%ContainerId'>Container ID</msg>
    <msg name='%ContainerName'>Container Name</msg>
    <msg name='%AliasUnknownContainer'>Host</msg>
    <msg name='%EffectiveCPUUtilization'>Effective CPU Utilization</msg>
    <msg name='%UArchEfficiencyIssueText'>Ineffective hardware usage detected.</msg>
    <msg name='%ParallelEfficiency'>Parallelism</msg>
    <msg name='%HWUsageEfficiency'>Hardware Usage</msg>
    <msg name='%AverageCPUUtilizationCountingMode'>CPU Utilization</msg>
    <msg name='%AverageCPUUtilizationCountingModeDescription'>This metric represents how efficiently the application utilized the CPUs available and helps evaluate the parallel efficiency of the application. It shows the percent of average CPU utilization by all logical CPUs on the system.</msg>
    <msg name='%AveragePhysicalCPUUtilizationCountingMode'>Physical Core Utilization</msg>
    <msg name='%AveragePhysicalCPUUtilizationCountingModeDescription'>This metric represents how efficiently the application utilized the physical CPU cores available and helps evaluate the parallel efficiency of the application. It shows the percent of average utilization by all physical CPU cores on the system.</msg>
    <msg name='%AverageLogicalCPUUtilizationCountingMode'>Logical Core Utilization</msg>
    <msg name='%AveragePhysicalCPUUtilizationCountingModeIssue'><![CDATA[<p>The metric value is low, which may signal a poor physical CPU cores utilization caused by:<ul><li>load imbalance</li><li>threading runtime overhead</li> <li>contended synchronization</li><li>thread/process underutilization</li><li>incorrect affinity that utilizes logical cores instead of physical cores</li></ul>Run the HPC Performance Characterization analysis to estimate the efficiency of MPI and OpenMP parallelism or run the Locks and Waits analysis to identify parallel bottlenecks for other parallel runtimes.</p>]]></msg>
    <msg name='%OmniPathOutgoingBandwidthDomain'>Interconnect Outgoing Bandwidth, GB/sec</msg>
    <msg name='%OmniPathIncomingBandwidthDomain'>Interconnect Incoming Bandwidth, GB/sec</msg>
    <msg name='%OmniPathOutgoingPacketRateDomain'>Interconnect Outgoing Packet Rate, Million Packets/sec</msg>
    <msg name='%OmniPathIncomingPacketRateDomain'>Interconnect Incoming Packet Rate, Million Packets/sec</msg>
    <msg name='%OmniPathIncomingBandwidth'>Incoming Bandwidth</msg>
    <msg name='%OmniPathOutgoingBandwidth'>Outgoing Bandwidth</msg>
    <msg name='%OmniPathOutgoingPacketRate'>Outgoing Packet Rate</msg>
    <msg name='%OmniPathIncomingPacketRate'>Incoming Packet Rate</msg>
    <msg name='%OmniPathBandwidthSection'>Interconnect Bandwidth</msg>
    <msg name='%OmniPathPacketRateSection'>Interconnect Packet Rate</msg>
    <msg name='%OmniPathOutgoingBandwidthBoundUncore'>Outgoing Bandwidth Bound</msg>
    <msg name='%OmniPathOutgoingBandwidthBoundUncoreDescription'>This metric represents a percentage of elapsed time the system spent with a high outgoing bandwidth utilization of the Intel Omni-Path Fabric. Note that the metric is calculated towards theoretical maximum networking bandwidth and does not take into account dynamic network conditions such as link oversubscription that can reduce the theoretical maximum.</msg>
    <msg name='%OmniPathOutgoingBandwidthBoundUncoreIssueText'>High outgoing network bandwidth utilization was detected. This may lead to increased communication time. You may use Intel Trace Analyzer and Collector for communication pattern analysis.</msg>
    <msg name='%OmniPathIncomingBandwidthBoundUncore'>Incoming Bandwidth Bound</msg>
    <msg name='%OmniPathIncomingBandwidthBoundUncoreDescription'>This metric represents a percentage of elapsed time the system spent with a high incoming bandwidth utilization of the Intel Omni-Path Fabric. Note that the metric is calculated towards theoretical maximum networking bandwidth and does not take into account dynamic network conditions such as link oversubscription that can reduce the theoretical maximum.</msg>
    <msg name='%OmniPathIncomingBandwidthBoundUncoreIssueText'>High incoming network bandwidth utilization was detected. This may lead to increased communication time. You may use Intel Trace Analyzer and Collector for communication pattern analysis.</msg>
    <msg name='%OmniPathOutgoingPacketRateBoundUncore'>Outgoing Packet Rate Bound</msg>
    <msg name='%OmniPathOutgoingPacketRateBoundUncoreDescription'>This metric represents a percentage of elapsed time the system spent with high Intel Omni-Path Fabric outgoing packet rate. Explore the Packet Rate Histogram to scale the issue.</msg>
    <msg name='%OmniPathOutgoingPacketRateBoundUncoreIssueText'>High outgoing network packet rate was detected. This may lead to increased communication time. You may use Intel Trace Analyzer and Collector for communication pattern analysis.</msg>
    <msg name='%OmniPathIncomingPacketRateBoundUncore'>Incoming Packet Rate Bound</msg>
    <msg name='%OmniPathIncomingPacketRateBoundUncoreDescription'>This metric represents a percentage of elapsed time the system spent with a high incoming packet rate of the Intel Omni-Path Fabric. Explore the Packet Rate Histogram to scale the issue.</msg>
    <msg name='%OmniPathIncomingPacketRateBoundUncoreIssueText'>High incoming network packet rate was detected. This may lead to increased communication time. You may use Intel Trace Analyzer and Collector for communication pattern analysis.</msg>
    <msg name='%PacketRateInfo'>Packet Rate Histogram</msg>
    <msg name='%PacketRateInfoDescription'>Explore packet rate over time using the histogram and identify functions with maximum contribution to the high rate of packets.</msg>
    <msg name='%PacketRateChart'>Packet Rate Histogram</msg>
    <msg name='%PacketRateGrid'>Packet Rate</msg>
    <msg name='%PacketRateChartDescription'>This histogram displays the wall time that the packet rate was at a certain value. Use the sliders at the bottom of the histogram to define thresholds for Low, Medium, and High rate levels. You can use these bandwidth utilization types in the Bottom-up view to group data and see all functions executed during a particular rate type.</msg>
    <msg name='%BandwidthUtilizationTypePacketRate'>Packet Rate Type</msg>
    <msg name='%Domain'>Domain</msg>
    <msg name='%HighPacketRateFunctions'>Top Functions with High Packet Rate</msg>
    <msg name='%HighPacketRateFunctionsDescription'>This section shows top functions, sorted by CPU Time, that were executing when packet rate was high for the domain selected in the histogram area.</msg>
    <msg name='%HighOmniPathBandwidthUtilizationFunctionsDescription'>This section shows top functions, sorted by CPU Time, that were executing when bandwidth utilization was high for the domain selected in the histogram area.</msg>
    <msg name='%OmniPathBandwidth'>Interconnect Bandwidth</msg>
    <msg name='%OmniPathPacketRate'>Interconnect Packet Rate</msg>
    <msg name='%BandwidthUtilizationBinValuePacketRate'>Packet Rate</msg>
    <msg name='%OmniPathBandwidthChartDescription'>This histogram displays how long the network bandwidth has been utilized by a certain value. Use sliders at the bottom of the histogram to define thresholds for Low, Medium and High utilization levels. Default thresholds are based on the theoretical maximum and ignore dynamic network conditions such as link oversubscription that can reduce the theoretical maximum. You can use these bandwidth utilization types in the Bottom-up view to group data and see all functions executed with a particular utilization type.</msg>
    <msg name='%WaitTimeWithPoorCPUUtilization'>Wait Time with poor CPU Utilization</msg>
    <msg name='%WaitTimeWithPoorCPUUtilizationPercents'>(% from Object Wait Time)</msg>
    <msg name='%WaitAndInactiveTimeWithPoorCPUUtilization'>Inactive Wait Time with poor CPU Utilization</msg>
    <msg name='%WaitAndInactiveTimeWithPoorCPUUtilizationGrid'>Inactive Wait Time</msg>
    <msg name='%WaitAndInactiveTimeWithPoorCPUUtilizationPercents'>(% from Inactive Wait Time)</msg>
    <msg name='%InactiveSyncWaitTime'>Inactive Sync Wait Time</msg>
    <msg name='%InactiveSyncWaitTimeIssueText'>Avarage wait time per synchronization context switch is low that can signal high contended synchronization between threads or inefficient use of system API</msg>
    <msg name='%InactiveSyncWaitCount'>Inactive Sync Wait Count</msg>
    <msg name='%PreemptionWaitTime'>Preemption Wait Time</msg>
    <msg name='%PreemptionWaitTimeIssueText'>The wait time because of thread preemption is significant and can signal about thread oversubscription or conflicts with other processes or kernel activities by CPU usage. It can also be a result of improper thread affinity. Keeping the thread number correspondent to the number of logical cores on the system can help to avoid thread oversubscription and as a result wait time on thread preemption.</msg>
    <msg name='%PreemptionWaitCount'>Preemption Wait Count</msg>
    <msg name='%InactiveWaitTime'>Inactive Wait Time</msg>
    <msg name='%InactiveWaitCount'>Inactive Wait Count</msg>
    <msg name='%InactiveWaitTimeDescription'>Inactive Wait Time is the time when a thread remains inactive and excluded from execution by the OS scheduler due to either synchronization or preemption. Significant Inactive Wait Time on the critical path of an application execution, combined with a poor CPU Utilization, negatively impacts application parallelism. Explore wait stacks to identify contended synchronization objects and apply optimization techniques to reduce the contention.</msg>
    <msg name='%InactiveWaitCountDescription'>Inactive Wait Count is the number of context switches a thread experiences when it is excluded from execution by the OS scheduler due to either synchronization or preemption. Excessive number of thread context switches may negatively impact application performance. Reduce synchronization contention to minimize synchronization context switches, or eliminate thread oversubscription to minimize thread preemption.</msg>
    <msg name='%InactiveSyncWaitTimeDescription'>Inactive Sync Wait Time is the time when a thread remains inactive and excluded from execution by the OS scheduler due to synchronization. Significant Inactive Sync Wait Time on the critical path of an application execution, combined with a poor CPU Utilization, negatively impacts application parallelism. Explore wait stacks to identify contended synchronization objects and apply optimization techniques to reduce the contention.</msg>
    <msg name='%InactiveSyncWaitCountDescription'>Inactive Sync Wait Count is the number of context switches a thread experiences when it is excluded from execution by the OS scheduler due to synchronization. Excessive number of thread context switches may negatively impact application performance. Apply optimization techniques to reduce synchronization contention and eliminate the problem.</msg>
    <msg name='%PreemptionWaitTimeDescription'>Preemption Wait Time is the time when a thread remains inactive and excluded from execution by the OS scheduler due to thread preemption. Significant Preemption Wait Time can signal a thread oversubscription problem either because of oversubscription of application threads or a conflict with OS threads or other applications on the system. Explore Total Thread count and eliminate thread oversubscription changing the number of threads in the application.</msg>
    <msg name='%PreemptionWaitCountDescription'>Preemption Wait Count is the number of context switches a thread experiences when it is excluded from execution by the OS scheduler due to thread preemption. Excessive number of thread context switches may negatively impact application performance. Apply optimization techniques to reduce synchronization contention and eliminate the problem. Explore Total Thread count and eliminate thread oversubscription changing the number of threads in the application.</msg>
    <msg name='%AverageOmniPathOutgoingBandwidth'>Average Outgoing Interconnect Bandwidth, GB/sec</msg>
    <msg name='%AverageOmniPathIncomingBandwidth'>Average Incoming Interconnect Bandwidth, GB/sec</msg>
    <msg name='%AverageOmniPathOutgoingPacketRate'>Average Outgoing Interconnect Packet Rate, Million Packets/sec</msg>
    <msg name='%AverageOmniPathIncomingPacketRate'>Average Incoming Interconnect Packet Rate, Million Packets/sec</msg>
    <msg name='%AveragePacketRate'>Average Packet Rate</msg>
    <msg name='%HighPacketRateTimePercents'>% of Elapsed Time with High Packet Rate</msg>
    <msg name='%WeightedAverage'>Weighted Average</msg>
    <msg name='%ParallelFsType'>Parallel File System Type</msg>
    <msg name='%ParallelFsObject'>Parallel File System</msg>
    <msg name='%ParallelFsMetricType'>Metric Type</msg>
    <msg name='%ParallelFsCount'>Metrics Count</msg>
    <msg name='%ParallelFsDuration'>Samples Duration</msg>
    <msg name='%ParallelFsReadBytes'>Read Bytes</msg>
    <msg name='%ParallelFsReadSamplesCount'>Read Requests</msg>
    <msg name='%ParallelFsReadWaittime'>Read Wait Time</msg>
    <msg name='%ParallelFsWriteBytes'>Write Bytes</msg>
    <msg name='%ParallelFsWriteSamplesCount'>Write Requests</msg>
    <msg name='%ParallelFsWriteWaittime'>Write Wait Time</msg>
    <msg name='%ParallelFsReqSamplesCount'>All Requests</msg>
    <msg name='%ParallelFsReqWaittime'>Request Wait Time</msg>
    <msg name='%ParallelFsOtherWaittime'>Other Wait Time</msg>
    <msg name='%ParallelFsSumWaittime'>Wait Time</msg>
    <msg name='%ParallelFsWaittimeExpand'>Wait Time Expansion</msg>
    <msg name='%ParallelFsLustreWaittime'>Lustre File System IO Wait Time</msg>
    <msg name='%ParallelFsReadWriteSamplesCount'>Sum of Read and Write Samples Count</msg>
    <msg name='%ParallelFsOtherSamplesCount'>Other Requests</msg>
    <msg name='%ParallelFsSumSamplesCount'>Request Count</msg>
    <msg name='%ParallelFsSamplesCountExpand'>Samples Count Expansion</msg>
    <msg name='%ParallelFsReadWriteBytes'>Traffic, B</msg>
    <msg name='%ParallelFsReadWriteExpand'>Traffic Expansion</msg>
    <msg name='%ParallelFsTraffic'>Traffic, MB</msg>
    <msg name='%ParallelFsReadRequestsDuration'>Read Duration</msg>
    <msg name='%ParallelFsWriteRequestsDuration'>Write Duration</msg>
    <msg name='%ParallelFsReadWriteRequestsDuration'>Sum of Read and Write Duration</msg>
    <msg name='%ParallelFsReadWriteBandwidth'>Bandwidth, B/s</msg>
    <msg name='%ParallelFsPackageRate'>Package Rate, pack/s</msg>
    <msg name='%ParallelFsAverageReadWritePackageSize'>Average Package Size, KB</msg>
    <msg name='%ParallelFsGrid'>Parallel File System detailed info:</msg>
    <msg name='%RawIptRegionCount'>Count</msg>
    <msg name='%RawIptRegionCountForSummary'>Code Region Of Interest Count</msg>
    <msg name='%RawIptRegionDomain'>Code Region Of Interest</msg>
    <msg name='%RawIptRegionDuration'>Duration</msg>
    <msg name='%RawIptRegionDurationName'>Duration Type</msg>
    <msg name='%CbrRatio'>Average CPU Frequency</msg>
    <msg name='%CbrFrequency'>CPU Frequency</msg>
    <msg name='%Offset'>Offset</msg>
    <msg name='%GpuSourceComputeTaskNew'>Source Computing Task</msg>
    <msg name='%PacketStageNew'>Render and GPGPU Engine State </msg>
    <msg name='%FPGADeviceMetricsGroup'>Device Metrics</msg>
    <msg name='%StallsPercentage'>Stalls (%)</msg>
    <msg name='%OccupancyPercentage'>Occupancy (%)</msg>
    <msg name='%Type'>Type</msg>
    <msg name='%DataTransferredGlobal'>Data Transferred, Global</msg>
    <msg name='%Size'>Size</msg>
    <msg name='%TransferSize'>Data Transfer Size</msg>
    <msg name='%AvgBandwidthGBperSec'>Average Bandwidth, GB/s</msg>
    <msg name='%FPGABandwidthGlobalGB'>Global Bandwidth, GB/s</msg>
    <msg name='%FPGADeviceMetrics'>FPGA Metrics</msg>
    <msg name='%Channel'>Channel</msg>
    <msg name='%GpuUsageRecommendationTmpl'><![CDATA[There is significant usage of GPU. Use <a class="icon gear10" action-link="?type=collection&analysisId=]]><arg name='%1'/><![CDATA[" cli="]]><arg name='%1'/><![CDATA[">]]><arg name='%1'/><![CDATA[</a> to identify GPU computing tasks that consume the most time. ]]></msg>
    <msg name='%IOWaitTimeRecommendationTmpl'><![CDATA[There is significant usage of I/O devices. Use <a class="icon gear10" action-link="?type=collection&analysisId=]]><arg name='%1'/><![CDATA[" cli="]]><arg name='%1'/><![CDATA[">]]><arg name='%1'/><![CDATA[</a> to analyze utilization of IO subsystem, CPU, and processor buses. ]]></msg>
    <msg name='%NetworkDevicesPCIETrafficRecommendationTmpl'><![CDATA[There is significant network bandwidth. Use <a class="icon gear10" action-link="?type=collection&analysisId=]]><arg name='%1'/><![CDATA[" cli="]]><arg name='%1'/><![CDATA[">]]><arg name='%1'/><![CDATA[</a>  to analyze utilization of IO subsystem, CPU, and processor buses. ]]></msg>
    <msg name='%AcceleratorsPCIETrafficRecommendationTmpl'><![CDATA[There is significant FPGA PCIe traffic. Use <a class="icon gear10" action-link="?type=collection&analysisId=]]><arg name='%1'/><![CDATA[" cli="]]><arg name='%1'/><![CDATA[">]]><arg name='%1'/><![CDATA[</a> to analyze CPU/FPGA interaction issues. To do this, explore OpenCL kernels running on the FPGA and identify the most time consuming kernels. ]]></msg>
    <msg name='%ShortCollectionMuxRecommendation'>Increase execution time</msg>
    <msg name='%ShortCollectionMuxRecommendationIssue'>Application execution time is too short. Metrics data may be unreliable. Consider reducing the sampling interval or increasing your application execution time.</msg>
    <msg name='%GPUDataTransferType'>GPU Data Transfer Type</msg>
    <msg name='%SurveyMessageHs'>Start with Hotspots analysis to understand the efficiency of your algorithm.</msg>
    <msg name='%SurveyRecommendationHs'>Use Hotspots analysis to identify the most time consuming functions. Drill down to see the time spent on every line of code.</msg>
    <msg name='%SurveyMessageUEPart1'>There is low microarchitecture usage </msg>
    <msg name='%SurveyMessageUEPart2'> of available hardware resources. </msg>
    <msg name='%SurveyRecommendationUE'>Run Microarchitecture Exploration analysis to analyze CPU microarchitecture bottlenecks that can affect application performance. </msg>
    <msg name='%SurveyMessageMAPart1'>The Memory Bound metric is high </msg>
    <msg name='%SurveyMessageMAPart2'> A significant fraction of execution pipeline slots could be stalled due to demand memory load and stores. </msg>
    <msg name='%SurveyRecommendationMA'>Use Memory Access analysis to measure metrics that can identify memory access issues. </msg>
    <msg name='%SurveyMessageHPCPart1'>Vectorization </msg>
    <msg name='%SurveyMessageHPCPart2'> is low. </msg>
    <msg name='%SurveyMessageHPCMPI'>Your application uses MPI. Start with the HPC Performance Characterization analysis to estimate the efficiency of MPI and OpenMP parallelism. </msg>
    <msg name='%SurveyRecommendationHPC'>Use HPC Performance Characterization analysis to analyze important aspects of your application performance. This includes CPU utilization with details on OpenMP efficiency analysis, memory usage, and FPU utilization with vectorization information. </msg>
    <msg name='%SurveyMessageThrPart1'>There is poor utilization of logical CPU cores </msg>
    <msg name='%SurveyMessageThrPart2'> in your application. </msg>
    <msg name='%SurveyRecommendationThr'>If you are not already taking advantage of all available CPU cores, use Threading analysis to examine parallelism in your application. </msg>
    <msg name='%SurveyMessageInOutPart1'>The system spent significant time </msg>
    <msg name='%SurveyMessageInOutPart2'> on heavy utilization of PCIe bandwidth. </msg>
    <msg name='%SurveyRecommendationInOut'>Use Input and Output (preview) analysis to analyze the  utilization of I/O subsystems, CPU, and processor buses. </msg>
    <msg name='%SurveyMessageSO'>Start with the System Overview analysis across the platform to monitor a general behavior of your target system and identify platform-level factors that limit performance. </msg>
    <msg name='%SurveyRecommendationSO'>Use System Overview analysis. </msg>
    <msg name='%SurveyMessageGPUHSPart1'>EUs were stalled or idle for a significant portion of time </msg>
    <msg name='%SurveyMessageGPUHSPart2'> This has a negative impact on compute-bound applications. </msg>
    <msg name='%SurveyRecommendationGPUHS'>Use GPU Compute/Media Hotspots (preview) analysis to analyze the most time-consuming GPU kernels.  Characterize GPU usage based on GPU hardware metrics and identify performance issues caused by memory latency or inefficient kernel algorithms. You can also analyze GPU instruction frequency for certain instruction types. </msg>
    <msg name='%SurveyMessageGPUOffloadPart1'>GPU Active Time </msg>
    <msg name='%SurveyMessageGPUOffloadPart2'> is low. Consider offloading more work to the GPU to increase overall application performance. </msg>
    <msg name='%SurveyRecommendationGPUOffload'>Use GPU Offload (preview) analysis to explore code execution on various CPU and GPU cores on your platform. Estimate how your code benefits from offloading to the GPU. Identify whether your application is CPU or GPU bound. </msg>
    <msg name='%SurveyMessagePP'> Use Platform Profiler analysis to collect system data over an extended period of time. </msg>
    <msg name='%SurveyRecommendationPP'>Use Platform Profiler analysis. </msg>
    <msg name='%SurveyMessageFPGA'>FPGA description. </msg>
    <msg name='%SurveyMessageMemCons'>Memory Consumption description. </msg>
    <msg name='%FPGADeviceStallsPercentageDescriptionText'>Percentage of cycles where memory and channel access is causing pipleine stalls. It helps identify whether the memory or channel can fulfill an access request.</msg>
    <msg name='%FPGADeviceOccupancyPercentageDescriptionText'>Percentage of cycles when a valid work-item executes the memory or channel instruction. The profiler reports a high occupancy percentage if the offline compiler generates a highly efficient pipeline from your kernel, where work-items or iterations are moving through the pipeline stages without stalling.</msg>
    <msg name='%L3CATAvailability'>L3 Cache Availability</msg>
    <msg name='%L2CATAvailability'>L2 Cache Availability</msg>
    <msg name='%AverageL3CATAvailability'>Average L3 Cache Availability</msg>
    <msg name='%AverageL2CATAvailability'>Average L2 Cache Availability</msg>
  </catalog>
</xmc>
