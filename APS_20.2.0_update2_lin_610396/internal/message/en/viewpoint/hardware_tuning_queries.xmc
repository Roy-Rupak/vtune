<?xml version="1.0" encoding="utf-8" ?>
<!--

 Copyright Â© 2009-2020 Intel Corporation. All rights reserved.

 The information contained herein is the exclusive property of
 Intel Corporation and may not be disclosed, examined, or reproduced in
 whole or in part without explicit written authorization from the Company.

-->
<xmc version="1.0">
    <catalog name="viewpoint" lang="en">
        <msg name="%4KAliasing">4K Aliasing</msg>
        <msg name="%4KAliasingDescriptionAll">This metric estimates how often memory load accesses were aliased by preceding stores (in the program order) with a 4K address offset. Possible false match may incur a few cycles to re-issue a load. However, a short re-issue duration is often hidden by the out-of-order core and HW optimizations. Hence, you may safely ignore a high value of this metric unless it propagates up into parent nodes of the hierarchy (for example, to L1 Bound).</msg>
        <msg name="%4KAliasingIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant proportion of cycles is spent dealing with false 4k aliasing between loads and stores.</p><p><strong>Tips:</strong> Use the source/assembly view to identify the aliasing loads and stores, and then adjust your data layout so that the loads and stores no longer alias.</p> See the Intel 64 and IA-32 Architectures Optimization Reference Manual for more details. ]]></msg>
        <msg name="%Assists">Assists</msg>
        <msg name="%AssistsDescriptionAll"> This metric estimates cycles fraction the CPU retired uops delivered by the Microcode_Sequencer as a result of Assists. Assists are long sequences of uops that are required in certain corner-cases for operations that cannot be handled natively by the execution pipeline. For example, when working with very small floating point values (so-called Denormals), the FP units are not set up to perform these operations natively. Instead, a sequence of instructions to perform the computation on the Denormals is injected into the pipeline. Since these microcode sequences might be hundreds of uops long, Assists can be extremely deleterious to performance and they can be avoided in many cases.</msg>
        <msg name="%AssistsIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of execution time is spent in microcode assists.</p><p><strong>Tips:</strong></p><p>1. Examine the FP_ASSIST and OTHER_ASSISTS events to determine the specific cause.</p><p>2. Add options eliminating x87 code and set the compiler options to enable DAZ (denormals-are-zero) and FTZ (flush-to-zero).</p> ]]></msg>
        <msg name="%BackendBoundPipelineSlots">Back-End Bound</msg>
        <msg name="%BackendBoundPipelineSlotsDescriptionAll">Back-End Bound metric represents a Pipeline Slots fraction where no uOps are being delivered due to a lack of required resources for accepting new uOps in the Back-End. Back-End is a portion of the processor core where an out-of-order scheduler dispatches ready uOps into their respective execution units, and, once completed, these uOps get retired according to program order. For example, stalls due to data-cache misses or stalls due to the divider unit being overloaded are both categorized as Back-End Bound. Back-End Bound is further divided into two main categories: Memory Bound and Core Bound.</msg>
        <msg name="%BackendBoundPipelineSlotsIssueTextAll"> A significant proportion of pipeline slots are remaining empty. When operations take too long in the back-end, they introduce bubbles in the pipeline that ultimately cause fewer pipeline slots containing useful work to be retired per cycle than the machine is capable of supporting.  This opportunity cost results in slower execution. Long-latency operations like divides and memory operations can cause this, as can too many operations being directed to a single execution port (for example, more multiply operations arriving in the back-end per cycle than the execution unit can support).</msg>
        <msg name="%BranchMispredict">Branch Mispredict</msg>
        <msg name="%BranchMispredictDescriptionAll"> When a branch mispredicts, some instructions from the mispredicted path still move through the pipeline. All work performed on these instructions is wasted since they would not have been executed had the branch been correctly predicted. This metric represents slots fraction the CPU has wasted due to Branch Misprediction. These slots are either wasted by uOps fetched from an incorrectly speculated program path, or stalls when the out-of-order part of the machine needs to recover its state from a speculative path.</msg>
        <msg name="%BranchMispredictIssueTextAll"><![CDATA[ <p><strong>Issue:</strong>: A significant proportion of branches are mispredicted, leading to excessive wasted work or Backend stalls due to the machine need to recover its state from a speculative path.</p><p><strong>Tips:</strong></p><p>1. Identify heavily mispredicted branches and consider making your algorithm more predictable or reducing the number of branches. You can add more work to 'if' statements and move them higher in the code flow for earlier execution. If using 'switch' or 'case' statements, put the most commonly executed cases first. Avoid using virtual function pointers for heavily executed calls.</p><p>2. Use profile-guided optimization in the compiler.</p><p>See the <a web-link="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html" cli="Intel 64 and IA-32 Architectures Optimization Reference Manual">Intel 64 and IA-32 Architectures Optimization Reference Manual</a> for general strategies to address branch misprediction issues.</p> ]]></msg>
        <msg name="%BusLock">Bus Lock</msg>
        <msg name="%BusLockDescriptionAll"> Intel processors provide a LOCK# signal that is asserted automatically during certain critical memory operations to lock the system bus or equivalent link.  While this output signal is asserted, requests from other processors or bus agents for control of the bus are blocked. This metric measures the ratio of bus cycles, during which a LOCK# signal is asserted on the bus. The LOCK# signal is asserted when there is a locked memory access due to uncacheable memory, locked operation that spans two cache lines, and page-walk from an uncacheable page table.</msg>
        <msg name="%BusLockIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> Bus locks have a very high performance penalty. It is highly recommended to avoid locked memory accesses to improve memory concurrency.</p><p><strong>Tips:</strong> Examine the BUS_LOCK_CLOCKS.SELF event in the source/assembly view to determine where the LOCK# signals are asserted from. If they come from themselves, look at Back-end issues, such as memory latency or reissues. Account for skid.</p> ]]></msg>
        <msg name="%CancelledPipelineSlots">Bad Speculation</msg>
        <msg name="%CancelledPipelineSlotsDescriptionAll">Bad Speculation represents a Pipeline Slots fraction wasted due to incorrect speculations. This includes slots used to issue uOps that do not eventually get retired and slots for which the issue-pipeline was blocked due to recovery from an earlier incorrect speculation. For example, wasted work due to mispredicted branches is categorized as a Bad Speculation category. Incorrect data speculation followed by Memory Ordering Nukes is another example.</msg>
        <msg name="%CancelledPipelineSlotsIssueTextAll"> A significant proportion of pipeline slots containing useful work are being cancelled.  This can be caused by mispredicting branches or by machine clears. Note that this metric value may be highlighted due to Branch Resteers issue.</msg>
        <msg name="%ContestedAccesses">Contested Accesses</msg>
        <msg name="%ContestedAccessesDescriptionAll"> Contested accesses occur when data written by one thread is read by another thread on a different core.  Examples of contested accesses include synchronizations such as locks, true data sharing such as modified locked variables, and false sharing. This metric is a ratio of cycles generated while the caching system was handling contested accesses to all cycles.</msg>
        <msg name="%ContestedAccessesIssueTextAll"><![CDATA[ <p><strong>Issues:</strong> There is a high number of contested accesses to cachelines modified by another core.</p><p><strong>Tips:</strong> Consider either using techniques suggested for other long latency load events (for example, LLC Miss) or reducing the contested accesses. To reduce contested accesses, first identify the cause. If it is a synchronization, try increasing synchronization granularity. If it is true data sharing, consider data privatization and reduction. If it is false data sharing, restructure the data to place contested variables into distinct cachelines. This may increase the working set due to padding, but false sharing can always be avoided.</p> ]]></msg>
        <msg name="%DIVActive">Divider</msg>
        <msg name="%DIVActiveDescriptionAll"> Not all arithmetic operations take the same amount of time.  Divides and square roots, both performed by the DIV unit, take considerably longer than integer or floating point addition, subtraction, or multiplication. This metric represents cycles fraction where the Divider unit was active.</msg>
        <msg name="%DIVActiveIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> The DIV unit is active for a significant portion of execution time.</p><p><strong>Tips:</strong> Locate the hot long-latency operation(s) and try to eliminate them.  For example, if dividing by a constant, consider replacing the divide by a product of the inverse of the constant. If dividing an integer, consider using a right-shift instead.</p> ]]></msg>
        <msg name="%DSBtoMITESwitchCost">DSB Switches</msg>
        <msg name="%DSBtoMITESwitchCostDescriptionAll"> Intel microarchitecture code name Sandy Bridge introduces a new decoded ICache. This cache, called the DSB (Decoded Stream Buffer), stores uOps that have already been decoded, avoiding many of the penalties of the legacy decode pipeline, called the MITE (Micro-instruction Translation Engine). However, when control flows out of the region cached in the DSB, the front-end incurs a penalty as uOp issue switches from the DSB to the MITE. This metric measures this penalty.</msg>
        <msg name="%DSBtoMITESwitchCostIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of cycles is spent switching from the DSB to the MITE.  This may happen if a hot code region is too large to fit into the DSB.</p><p><strong>Tips:</strong> Consider changing code layout (for example, via profile-guided optimization) to help your hot regions fit into the DSB.</p><p>See the "Optimization for Decoded ICache" section in the Intel 64 and IA-32 Architectures Optimization Reference Manual.</p> ]]></msg>
        <msg name="%DTLBOverhead">DTLB Overhead</msg>
        <msg name="%DTLBOverheadDescriptionAll"> In x86 architectures, mappings between virtual and physical memory are facilitated by a page table, which is kept in memory. To minimize references to this table, recently-used portions of the page table are cached in a hierarchy of 'translation look-aside buffers', or TLBs, which are consulted on every virtual address translation. As with data caches, the farther a request has to go to be satisfied, the worse the performance impact. This metric estimates the performance penalty paid for missing the first-level data TLB (DTLB) that includes hitting in the second-level data TLB (STLB) as well as performing a hardware page walk on an STLB miss.</msg>
        <msg name="%DTLBOverheadIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of cycles is being spent handling first-level data TLB misses.</p><p><strong>Tips:</strong></p><p>1.  As with ordinary data caching, focus on improving data locality and reducing the working-set size to minimize the DTLB overhead.</p><p>2. Consider using profile-guided optimization (PGO) to collocate frequently-used data on the same page.</p><p>3. Try using larger page sizes for large amounts of frequently-used data.</p> ]]></msg>
        <msg name="%DataSharing">Data Sharing</msg>
        <msg name="%DataSharingDescriptionAll"> Data shared by multiple threads (even just read shared) may cause increased access latency due to cache coherency. This metric measures the impact of that coherency.  Excessive data sharing can drastically harm multithreaded performance. This metric is defined by the ratio of cycles while the caching system is handling shared data to all cycles. It does not measure waits due to contention on a variable, which is measured by the Locks and Waits analysis.</msg>
        <msg name="%DataSharingIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> Significant data sharing by different cores is detected.</p><p><strong>Tips:</strong></p><p>1. Examine the Contested Accesses metric to determine whether the major component of data sharing is due to contested accesses or simple read sharing. Read sharing is a lower priority than Contested Accesses or issues such as LLC Misses and Remote Accesses.</p><p>2. If simple read sharing is a performance bottleneck, consider changing data layout across threads or rearranging computation. However, this type of tuning may not be straightforward and could bring more serious performance issues back.</p> ]]></msg>
        <msg name="%ExecutionStalls">Execution Stalls</msg>
        <msg name="%ExecutionStallsDescriptionAll"> Execution stalls may signify that a machine is running at full capacity, with no computation resources wasted. Sometimes, however, long-latency operations can serialize while waiting for critical computation resources. This metric is the ratio of cycles with no micro-operations executed to all cycles.</msg>
        <msg name="%ExecutionStallsIssueTextAll"> The percentage of cycles with no micro-operations executed is high. Look for long-latency operations at code regions with high execution stalls and try to use alternative methods or lower latency operations. For example, consider replacing 'div' operations with right-shifts, or try to reduce the latency of memory accesses.</msg>
        <msg name="%FPAssists">FP Assists</msg>
        <msg name="%FPAssistsDescriptionAll"> Certain floating point operations cannot be handled natively by the execution pipeline and must be performed by microcode (small programs injected into the execution stream).  For example, when working with very small floating point values (so-called denormals), the floating-point units are not set up to perform these operations natively. Instead, a sequence of instructions to perform the computation on the denormal is injected into the pipeline.  Since these microcode sequences might be hundreds of instructions long, these microcode assists are extremely deleterious to performance.</msg>
        <msg name="%FPAssistsIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of execution time is spent in floating point assists.</p><p><strong>Tips:</strong> Consider enabling the DAZ (Denormals Are Zero) and/or FTZ (Flush To Zero) options in your compiler to flush denormals to zero. This option may improve performance if the denormal values are not critical in your application. Also note that the DAZ and FTZ modes are not compatible with the IEEE Standard 754.</p> ]]></msg>
        <msg name="%FlagsMergeStalls">Flags Merge Stalls</msg>
        <msg name="%FlagsMergeStallsDescriptionAll"> Some instructions have increased latency on Intel microarchitecture code name Sandy Bridge.  Shift cl operations require a potentially expensive flag merge. This metric estimates the performance penalty of that merge.</msg>
        <msg name="%FlagsMergeStallsIssueTextAll"> A significant proportion of cycles were spent handling flags merge operations.  Use the source view to discover the responsible instructions and try to avoid their use.</msg>
        <msg name="%FrontendBoundPipelineSlots">Front-End Bound</msg>
        <msg name="%FrontendBoundPipelineSlotsDescriptionAll">Front-End Bound metric represents a slots fraction where the processor's Front-End undersupplies its Back-End. Front-End denotes the first part of the processor core responsible for fetching operations that are executed later on by the Back-End part. Within the Front-End, a branch predictor predicts the next address to fetch, cache-lines are fetched from the memory subsystem, parsed into instructions, and lastly decoded into micro-ops (uOps). Front-End Bound metric denotes unutilized issue-slots when there is no Back-End stall (bubbles where Front-End delivered no uOps while Back-End could have accepted them). For example, stalls due to instruction-cache misses would be categorized as Front-End Bound.</msg>
        <msg name="%FrontendBoundPipelineSlotsIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of Pipeline Slots is remaining empty due to issues in the Front-End.</p><p><strong>Tips:</strong>  Make sure the code working size is not too large, the code layout does not require too many memory accesses per cycle to get enough instructions for filling four pipeline slots, or check for microcode assists.</p> ]]></msg>
        <msg name="%ICacheMisses">ICache Misses</msg>
        <msg name="%ICacheMissesDescriptionAll"> To introduce new uOps into the pipeline, the core must either fetch them from a decoded instruction cache, or fetch the instructions themselves from memory and then decode them. In the latter path, the requests to memory first go through the L1I (level 1 instruction) cache that caches the recent code working set. Front-end stalls can accrue when fetched instructions are not present in the L1I. Possible reasons are a large code working set or fragmentation between hot and cold code. In the latter case, when a hot instruction is fetched into the L1I, any cold code on its cache line is brought along with it. This may result in the eviction of other, hotter code.</msg>
        <msg name="%ICacheMissesIssueTextAll"><![CDATA[  <p><strong>Issue:</strong> A significant portion of instruction fetches is missing in the instruction cache.</p><p><strong>Tips:</strong></p><p>1. Use profile-guided optimization to reduce the size of hot code regions.</p><p>2. Consider compiler options to reorder functions so that hot functions are located together.</p><p>3. If your application makes significant use of macros, try to reduce this by either converting the relevant macros to functions or using linker options to eliminate repeated code.</p><p>4. Consider the Os/O1 optimization level or the following subset of optimizations to decrease your code footprint: <ul><li>use inlining only when it decreases the footprint</li><li>disable loop unrolling</li><li>disable intrinsic inlining</li></ul></p><p><strong>Optimization examples:</strong><p><a web-link="https://software.intel.com/en-us/vtune-amplifier-cookbook-instruction-cache-misses" cli="Instruction Cache Misses recipe from PRODUCT_LEGAL_NAME Performance Analysis Cookbook">Instruction Cache Misses recipe</a></p></p> ]]></msg>
        <msg name="%ITLBOverhead">ITLB Overhead</msg>
        <msg name="%ITLBOverheadDescriptionAll"> In x86 architectures, mappings between virtual and physical memory are facilitated by a page table, which is kept in memory. To minimize references to this table, recently-used portions of the page table are cached in a hierarchy of 'translation look-aside buffers', or TLBs, which are consulted on every virtual address translation. As with data caches, the farther a request has to go to be satisfied, the worse the performance impact. This metric estimates the performance penalty of page walks induced on ITLB (instruction TLB) misses.</msg>
        <msg name="%ITLBOverheadIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of cycles is spent handling instruction TLB misses.</p><p><strong>Tips:</strong></p><p>1. Use profile-guided optimization and IPO to reduce the size of hot code regions.</p><p>2. Consider compiler options to reorder functions so that hot functions are located together.</p><p>3. If your application makes significant use of macros, try to reduce this by either converting the relevant macros to functions or using linker options to eliminate repeated code.</p><p>4. For Windows targets, add function splitting.</p><p>5. Consider using large code pages.</p> ]]></msg>
        <msg name="%InstructionStarvation">Instruction Starvation</msg>
        <msg name="%InstructionStarvationDescriptionAll"> A large code working set size or a high degree of branch misprediction can induce instruction delivery stalls at the front-end, such as misses in the L1I. Such stalls are called Instruction Starvation. This metric is the ratio of cycles generated when no instruction was issued by the front-end to all cycles.</msg>
        <msg name="%InstructionStarvationIssueTextAll"> A significant number of CPU cycles is spent waiting for code to be delivered due to L1I misses or other problems.  Look for ways to reduce the code working set, branch misprediction, and the use of virtual functions.</msg>
        <msg name="%L1DReplacementPercentage">L1D Replacement Percentage</msg>
        <msg name="%L1DReplacementPercentageDescriptionAll"> When a cache line is brought into the L1 cache, another line must be evicted to make room for it. When lines in active use are evicted, a performance problem may arise from continually rotating data back into the cache.  This metric measures the percentage of all replacements due to each row.  For example, if the grouping is set to 'Function', this metric shows the percentage of all replacements due to each function, summing up to 100%.</msg>
        <msg name="%L1DReplacementPercentageIssueTextAll"> This row is responsible for a majority of all L1 cache replacements.  Some replacements are unavoidable, and a high level of replacements may not indicate a problem. Consider this metric only when looking for the source of a significant number of L1 cache misses for a particular grouping.  If these replacements are marked as a problem, try rearranging data structures (for example, moving infrequently-used data away from more-frequently-used data so that unused data is not taking up cache space) or re-ordering operations (to get as much use as possible out of data before it is evicted).</msg>
        <msg name="%L1DReplacements">L1D Replacements</msg>
        <msg name="%L1DReplacementsDescriptionAll"> Replacements into the L1D</msg>
        <msg name="%L1DReplacementsIssueTextAll"></msg>
        <msg name="%L1IStallCycles">L1I Stall Cycles</msg>
        <msg name="%L1IStallCyclesDescriptionAll"> In a shared-memory machine, instructions and data are stored in the same memory address space. However, for performance, they are cached separately. Large code working set, branch misprediction, including one caused by excessive use of virtual functions, can induce misses into L1I and so cause instruction starvation that badly influence application performance.</msg>
        <msg name="%L1IStallCyclesIssueTextAll"> A significant number of CPU cycles is spent waiting for code to arrive into L1I. Review application code for the patterns causing instruction starvation and rearrange the code.</msg>
        <msg name="%L2ReplacementPercentage">L2 Replacement Percentage</msg>
        <msg name="%L2ReplacementPercentageDescriptionAll"> When a cache line is brought into the L2 cache, another line must be evicted to make room for it. When lines in active use are evicted, a performance problem may arise from continually rotating data back into the cache.  This metric measures the percentage of all replacements due to each row.  For example, if the grouping is set to 'Function', this metric shows the percentage of all replacements due to each function, summing up to 100%.</msg>
        <msg name="%L2ReplacementPercentageIssueTextAll"> This row is responsible for a majority of all L2 cache replacements.  Some replacements are unavoidable, and a high level of replacements may not indicate a problem. Consider this metric only when looking for the source of a significant number of L2 cache misses for a particular grouping.  If these replacements are marked as a problem, try rearranging data structures (for example, moving infrequently-used data away from more-frequently-used data so that unused data is not taking up cache space) or re-ordering operations (to get as much use as possible out of data before it is evicted).</msg>
        <msg name="%L2Replacements">L2 Replacements</msg>
        <msg name="%L2ReplacementsDescriptionAll"> Replacements into the L2</msg>
        <msg name="%L2ReplacementsIssueTextAll"></msg>
        <msg name="%LLCHit">LLC Hit</msg>
        <msg name="%LLCHitDescriptionAll"> The LLC (last-level cache) is the last, and longest-latency, level in the memory hierarchy before main memory (DRAM).  While LLC hits are serviced much more quickly than hits in DRAM, they can still incur a significant performance penalty.  This metric also includes coherence penalties for shared data.</msg>
        <msg name="%LLCHitIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of cycles is being spent on data fetches that hit in the LLC. This metric includes coherence penalties for shared data.</p><p><strong>Tips: </strong></p><p>1. If contested accesses or data sharing are indicated as likely issues, address them first.  Otherwise, consider the performance tuning applicable to an LLC-missing workload: reduce the data working set size, improve data access locality, consider blocking or partitioning your working set so that it fits into the low-level cache, or better exploit hardware prefetchers.</p><p>2. Consider using software prefetchers, but note that they can interfere with normal loads, potentially increasing latency, as well as increase pressure on the memory system.</p> ]]></msg>
        <msg name="%LLCHitKNL">L2 Hit Bound</msg>
        <msg name="%LLCHitKNLDescriptionAll"> The L2 is the last and longest-latency level in the memory hierarchy before the main memory (DRAM) or MCDRAM.  While L2 hits are serviced much more quickly than hits in DRAM or MCDRAM, they can still incur a significant performance penalty.  This metric also includes coherence penalties for shared data. The L2 Hit Bound metric shows a ratio of cycles spent handling L2 hits to all cycles. The cycles spent handling L2 hits are calculated as L2 CACHE HIT COST * L2 CACHE HIT COUNT where L2 CACHE HIT COST is a constant measured as typical L2 access latency in cycles.</msg>
        <msg name="%LLCHitKNLIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of cycles is being spent on data fetches that miss the L1 but hit the L2. This metric includes coherence penalties for shared data.</p><p><strong>Tips: </strong></p><p>1. If contested accesses or data sharing are indicated as likely issues, address them first. Otherwise, consider the performance tuning applicable to an L2-missing workload: reduce the data working set size, improve data access locality, consider blocking or  partitioning your working set so that it fits into the L1, or better exploit hardware prefetchers.</p><p>2. Consider using software prefetchers, but note that they can interfere with normal loads, potentially increasing latency, as well as increase pressure on the memory system.</p> ]]></msg>
        <msg name="%LLCLoadMissesServicedByRemoteDRAM">LLC Load Misses Serviced By Remote DRAM</msg>
        <msg name="%LLCLoadMissesServicedByRemoteDRAMDescriptionAll"> In NUMA (non-uniform memory architecture) machines, memory requests missing in LLC may be serviced either by local or remote DRAM. Memory requests to remote DRAM incur much greater latencies than those to local DRAM. It is recommended to keep as much frequently accessed data local as possible. This metric is defined by the ratio of cycles when LLC load misses are serviced by remote DRAM to all cycles.</msg>
        <msg name="%LLCLoadMissesServicedByRemoteDRAMIssueTextAll"> A significant amount of time is spent servicing memory requests from remote DRAM. Wherever possible, try to consistently use data on the same core, or at least the same package, as it was allocated on.</msg>
        <msg name="%LLCMiss">LLC Miss</msg>
        <msg name="%LLCMissDescriptionAll"> The LLC (last-level cache) is the last, and longest-latency, level in the memory hierarchy before main memory (DRAM).  Any memory requests missing here must be serviced by local or remote DRAM, with significant latency. The LLC Miss metric shows a ratio of cycles with outstanding LLC misses to all cycles.</msg>
        <msg name="%LLCMissIssueTextAll"> A high number of CPU cycles is being spent waiting for LLC load misses to be serviced. Possible optimizations are to reduce data working set size, improve data access locality, blocking and consuming data in chunks that fit in the LLC, or better exploit hardware prefetchers. Consider using software prefetchers but they can increase latency by interfering with normal loads, and can increase pressure on the memory system.</msg>
        <msg name="%LLCMissKNL">L2 Miss Bound</msg>
        <msg name="%LLCMissKNLDescriptionAll"> The L2 is the last and longest-latency level in the memory hierarchy before the main memory (DRAM) or MCDRAM.  Any memory requests missing here must be serviced by local or remote DRAM or MCDRAM, with significant latency. The L2 Miss Bound metric shows a ratio of cycles spent handling L2 misses to all cycles. The cycles spent handling L2 misses are calculated as L2 CACHE MISS COST * L2 CACHE MISS COUNT where L2 CACHE MISS COST is a constant measured as typical DRAM access latency in cycles.</msg>
        <msg name="%LLCMissKNLIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A high number of CPU cycles is being spent waiting for L2 load misses to be serviced.</p><p><strong>Tips:</strong></p><p>1. Reduce the data working set size, improve data access locality, blocking and consuming data in chunks that fit into the L2, or better exploit hardware prefetchers.</p><p>2. Consider using software prefetchers but note that they can increase latency by interfering with normal loads, as well as increase pressure on the memory system.</p> ]]></msg>
        <msg name="%LLCReplacementPercentage">LLC Replacement Percentage</msg>
        <msg name="%LLCReplacementPercentageDescriptionAll"> When a cache line is brought into the last-level cache, another line must be evicted to make room for it.  When lines in active use are evicted, a performance problem may arise from continually rotating data back into the cache. This metric measures the percentage of all replacements due to each row.  For example, if the grouping is set to 'Function', this metric shows the percentage of all replacements due to each function, summing up to 100%.</msg>
        <msg name="%LLCReplacementPercentageIssueTextAll"> This row is responsible for a majority of all last-level cache replacements.  Some replacements are unavoidable, and a high level of replacements may not indicate a problem. Consider this metric only when looking for the source of a significant number of last-level cache misses for a particular grouping.  If these replacements are marked as a problem, try rearranging data structures (for example, moving infrequently-used data away from more-frequently-used data so that unused data is not taking up cache space) or re-ordering operations (to get as much use as possible out of data before it is evicted).</msg>
        <msg name="%LLCReplacements">LLC Replacements</msg>
        <msg name="%LLCReplacementsDescriptionAll"> Replacements into the LLC</msg>
        <msg name="%LLCReplacementsIssueTextAll"></msg>
        <msg name="%LoadsBlockedbyStoreForwarding">Loads Blocked by Store Forwarding</msg>
        <msg name="%LoadsBlockedbyStoreForwardingDescriptionAll"> To streamline memory operations in the pipeline, a load can avoid waiting for memory if a prior store, still in flight, is writing the data that the load wants to read (a 'store forwarding' process). However, in some cases, generally when the prior store is writing a smaller region than the load is reading, the load is blocked for a signficant time pending the store forward.  This metric measures the performance penalty of such blocked loads.</msg>
        <msg name="%LoadsBlockedbyStoreForwardingIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> Loads are blocked during store forwarding for a significant proportion of cycles.</p><p><strong>Tips:</strong> Use source/assembly view to identify the blocked loads, then identify the problematically-forwarded stores, which will typically be within the ten dynamic instructions prior to the load.  If the forwarding store is smaller than the load, change the store to be the same size as the load.</p> ]]></msg>
        <msg name="%MSAssists">MS Assists</msg>
        <msg name="%MSAssistsDescriptionAll"> Certain corner-case operations cannot be handled natively by the execution pipeline and must be performed by the microcode sequencer (MS), where 1 or more uOps are issued.  The microcode sequencer performs microcode assists (small programs injected into the execution stream), inserting flows, and writing to the instruction queue (IQ). For example, when working with very small floating point values (so-called denormals), the floating-point units are not set up to perform these operations natively.  Instead, a sequence of instructions to perform the computation on the denormal is injected into the pipeline.  Since these microcode sequences might be hundreds of instructions long, these microcode assists are extremely deleterious to performance.</msg>
        <msg name="%MSAssistsIssueTextAll"> A significant portion of execution time is spent in microcode assists, inserted flows, and writing to the instruction queue (IQ). Examine the FP Assist and SIMD Assist metrics to determine the specific cause.</msg>
        <msg name="%MachineClears">Machine Clears</msg>
        <msg name="%MachineClearsDescriptionAll"> Certain events require the entire pipeline to be cleared and restarted from just after the last retired instruction.  This metric measures three such events: memory ordering violations, self-modifying code, and certain loads to illegal address ranges. Machine Clears metric represents slots fraction the CPU has wasted due to Machine Clears.  These slots are either wasted by uOps fetched prior to the clear, or stalls the out-of-order portion of the machine needs to recover its state after the clear.</msg>
        <msg name="%MachineClearsIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of execution time is spent handling machine clears.</p><p><strong>Tips:</strong> See the "Memory Disambiguation" section in the Intel 64 and IA-32 Architectures Optimization Reference Manual.</p> ]]></msg>
        <msg name="%MemoryBusTransactions">Memory Bus Transactions</msg>
        <msg name="%MemoryBusTransactionsDescriptionAll"> This metric reflects the number of memory bus transactions.</msg>
        <msg name="%MemoryBusTransactionsIssueTextAll"></msg>
        <msg name="%PageWalk">Page Walk</msg>
        <msg name="%PageWalkDescriptionAll"> In x86 architectures, mappings between virtual and physical memory are facilitated by a page table that is kept in memory. To minimize references to this table, recently-used portions of the page table are cached in a hierarchy of 'translation look-aside buffers', or TLBs, which are consulted on every virtual address translation. As with data caches, the farther a request has to go to be satisfied, the worse the performance impact is. This metric estimates the performance penalty paid for missing the first-level TLB that includes hitting in the second-level data TLB (STLB) as well as performing a hardware page walk on an STLB miss.</msg>
        <msg name="%PageWalkIssueTextAll"> Page Walks have a large performance penalty because they involve accessing the contents of multiple memory locations to calculate the physical address. Since this metric includes the cycles handling both instruction and data TLB misses, look at ITLB Overhead and DTLB Overhead and follow the instructions to improve performance. Also examine PAGE_WALKS.D_SIDE_CYCLES and PAGE_WALKS.I_SIDE_CYCLES events in the source/assembly view for further breakdown. Account for skid.</msg>
        <msg name="%RetireStalls">Retire Stalls</msg>
        <msg name="%RetireStallsDescriptionAll"> This metric is defined as a ratio of the number of cycles when no micro-operations are retired to all cycles. In the absence of performance issues, long latency operations, and dependency chains, retire stalls are insignificant.  Otherwise, retire stalls result in a performance penalty.  On processors based on the Intel microarchitecture code name Nehalem, this metric is based on precise events that do not suffer from significant skid.</msg>
        <msg name="%RetireStallsIssueTextAll"> A high number of retire stalls is detected. This may result from branch misprediction, instruction starvation, long latency operations, and other issues. Use this metric to find where you have stalled instructions. Once you have located the problem, analyze metrics such as LLC Miss, Execution Stalls, Remote Accesses, Data Sharing, and Contested Accesses, or look for long-latency instructions like divisions and string operations to understand the cause.</msg>
        <msg name="%RetiredPipelineSlots">Retiring</msg>
        <msg name="%RetiredPipelineSlotsDescriptionAll">Retiring metric represents a Pipeline Slots fraction utilized by useful work, meaning the issued uOps that eventually get retired. Ideally, all Pipeline Slots would be attributed to the Retiring category.  Retiring of 100% would indicate the maximum possible number of uOps retired per cycle has been achieved.  Maximizing Retiring typically increases the Instruction-Per-Cycle metric. Note that a high Retiring value does not necessary mean no more room for performance improvement. For example, Microcode assists are categorized under Retiring. They hurt performance and can often be avoided.</msg>
        <msg name="%RetiredPipelineSlotsIssueTextAll">A high fraction of pipeline slots was utilized by useful work. While the goal is to make this metric value as big as possible, a high Retiring value for non-vectorized code could prompt you to consider code vectorization. Vectorization enables doing more computations without significantly increasing the number of instructions, thus improving the performance. Note that this metric value may be highlighted due to Microcode Sequencer (MS) issue, so the performance can be improved by avoiding using the MS.</msg>
        <msg name="%UArchEfficiency">uArch Efficiency</msg>
        <msg name="%RetiringuOps">Retiring uOps</msg>
        <msg name="%RetiringuOpsDescriptionAll"></msg>
        <msg name="%RetiringuOpsIssueTextAll"></msg>
        <msg name="%SIMDAssists">SIMD Assists</msg>
        <msg name="%SIMDAssistsDescriptionAll"> SIMD assists are invoked when an EMMS instruction is executed after MMX technology code has changed the MMX state in the floating point stack. The EMMS instruction clears the MMX technology state at the end of all MMX technology procedures or subroutines and before calling other procedures or subroutines that may execute x87 floating-point instructions, which can incur a performance penalty when intermixing MMX and X87 instructions. The SIMD assists are required in the streaming SIMD Extensions (SSE) instructions with denormal input when the DAZ (Denormals Are Zeros) flag is off or underflow result when the FTZ (Flush To Zero) flag is off.</msg>
        <msg name="%SIMDAssistsIssueTextAll"> A significant portion of execution time is spent in SIMD assists. Consider enabling the DAZ (Denormals Are Zero) and/or FTZ (Flush To Zero) options in your compiler to flush denormals to zero. This option may improve performance if the denormal values are not critical in your application. Also note that the DAZ and FTZ modes are not compatible with the IEEE Standard 754.</msg>
        <msg name="%SMCMachineClear">SMC Machine Clear</msg>
        <msg name="%SMCMachineClearDescriptionAll"> Certain events require the entire pipeline to be cleared and restarted from just after the last retired instruction. This metric measures only self-modifying code (SMC) events. This event counts the number of times a program writes to a code section that is shared with another processor or itself as a data page, causing the entire pipeline and the trace caches to be cleared. Self-modifying code causes a severe penalty in all processors based on Intel architecture.</msg>
        <msg name="%SMCMachineClearIssueTextAll"> A significant portion of execution time is spent handling machine clears incurred by self-modifying code event. Dynamically-modified code (for example, target fix-ups) is likely to suffer from performance degradation due to SMC. To avoid this, introduce indirect branches and use data tables on data pages (not code pages) with register-indirect calls.</msg>
        <msg name="%MOMachineClear">MO Machine Clear Overhead</msg>
        <msg name="%MOMachineClearDescriptionAll"> Certain events require the entire pipeline to be cleared and restarted from just after the last retired instruction.  This metric estimates the overhead of machine clears due to Memory Ordering.  The memory ordering (MO) machine clear happens when a snoop request from another processor matches a source for a data operation in the pipeline.  In this situation the pipeline is cleared before the loads and stores in progress are retired.  Then the pipeline is restarted from the previous retired instruction, ensuring that memory ordering of loads and stores can be preserved, both within one core and across cores. Memory ordering issues cause a severe penalty in all processors based on Intel architecture.</msg>
        <msg name="%MOMachineClearIssueTextAll"> A significant portion of execution time is spent clearing the machine to handle memory ordering requirements.  To avoid this, reorder your load and store instructions, particularly loads and stores of data that is shared, or reduce sharing requirements.</msg>
        <msg name="%SlowLEAStalls">Slow LEA Stalls</msg>
        <msg name="%SlowLEAStallsDescriptionAll"> Some instructions have increased latency in Intel microarchitecture code name Sandy Bridge.  Some LEA instructions, most notably three-operand LEA instructions, have increased latency and reduced dispatch port choices compared to other LEAs. This metric estimates the performance penalty of such slow LEAs.</msg>
        <msg name="%SlowLEAStallsIssueTextAll"> A significant proportion of cycles were spent handling slow LEA operations. Use the source view to discover the responsible instructions and try to avoid their use.</msg>
        <msg name="%SplitLoads">Split Loads</msg>
        <msg name="%SplitLoadsDescriptionAll"> Throughout the memory hierarchy, data moves at cache line granularity - 64 bytes per line.  Although this is much larger than many common data types, such as integer, float, or double, unaligned values of these or other types may span two cache lines.  Recent Intel architectures have significantly improved the performance of such 'split loads' by introducing split registers to handle these cases, but split loads can still be problematic, especially if many split loads in a row consume all available split registers.</msg>
        <msg name="%SplitLoadsIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of cycles is spent handling split loads.</p><p><strong>Tips:</strong> Consider aligning your data to the 64-byte cache line granularity.</p><p>See the Intel 64 and IA-32 Architectures Optimization Reference Manual for more details.</p> ]]></msg>
        <msg name="%SplitStores">Split Stores</msg>
        <msg name="%SplitStoresDescriptionAll"> Throughout the memory hierarchy, data moves at cache line granularity - 64 bytes per line.  Although this is much larger than many common data types, such as integer, float, or double, unaligned values of these or other types may span two cache lines.  Recent Intel architectures have significantly improved the performance of such 'split stores' by introducing split registers to handle these cases.  But split stores can still be problematic, especially if they consume split registers which could be servicing other split loads.</msg>
        <msg name="%SplitStoresIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant portion of cycles is spent handling split stores.</p><p><strong>Tips:</strong> Consider aligning your data to the 64-byte cache line granularity.</p><p>Note that this metric value may be highlighted due to Port 4 issue.</p> ]]></msg>
        <msg name="%LLCMissIssueTextVariant1"> A large percentage of memory load instructions is missing in the LLC. Possible optimizations are to reduce the dataset size; improve data access locality, blocking and consuming data in chunks that fit in the LLC; or better exploit hardware prefetchers. Consider using software prefetchers but beware that they can increase latency by interfering with normal loads and also can increase pressure on the memory system.</msg>
        <msg name="%LLCMissDescriptionVariant1"> The LLC (last-level cache) is the last and longest-latency level in the memory hierarchy before the main memory (DRAM). Any memory requests missing here must be serviced by local or remote DRAM, with significant latency. The LLC Miss metric shows a ratio of memory requests missing in the LLC and going off-core to all retired load instructions. This metric includes speculative accesses (issued memory accesses that later were cancelled due to branch misprediction or other misspeculation).</msg>
        <msg name="%RetiringuOpsIssueTextVariant1"> Relatively few uOps are retiring properly.  Investigate Wasted Work, Front-end Issues, and Back-end Issues uOps more closely to determine the source of the low retirement rate.</msg>
        <msg name="%RetiringuOpsDescriptionVariant1"> Superscalar processors can be conceptually divided into the 'front-end', where instructions are fetched and decoded into the operations that constitute them; and the 'back-end', where the required computation is performed.  Each cycle, the front-end generates up to two of these operations, which it places into pipeline slots that then move through the back-end.  Thus, for a given execution duration in clock cycles, it is easy to determine the maximum number of pipeline slots containing useful work that can be retired in that duration.  The actual number of retired pipeline slots containing useful work, though, rarely equals this maximum.  This can be due to several factors: some pipeline slots cannot be filled with useful work, either because the front-end could not fetch or decode instructions in time ('Front-end bound' execution) or because the back-end was not prepared to accept more operations of a certain kind ('Back-end bound' execution). Moreover, even pipeline slots that do contain useful work may not retire due to bad speculation.  Front-end bound execution may be due to a large code working set, poor code layout, or microcode assists. Back-end bound execution may be due to long-latency operations or other contention for execution resources. Bad speculation is most frequently due to branch misprediction.</msg>
        <msg name="%FELatency">Front-End Latency</msg>
        <msg name="%FELatencyDescriptionAll">This metric represents a fraction of slots during which CPU was stalled due to front-end latency issues, such as instruction-cache misses, ITLB misses or fetch stalls after a branch misprediction. In such cases, the front-end delivers no uOps.</msg>
        <msg name="%FELatencyIssueTextAll">This metric represents a fraction of slots during which CPU was stalled due to front-end latency issues, such as instruction-cache misses, ITLB misses or fetch stalls after a branch misprediction. In such cases, the front-end delivers no uOps.</msg>
        <msg name="%FEBandwidth">Front-End Bandwidth</msg>
        <msg name="%FEBandwidthDescriptionAll">This metric represents a fraction of slots during which CPU was stalled due to front-end bandwidth issues, such as inefficiencies in the instruction decoders or code restrictions for caching in the DSB (decoded uOps cache). In such cases, the front-end typically delivers a non-optimal amount of uOps to the back-end.</msg>
        <msg name="%FEBandwidthIssueTextAll">This metric represents a fraction of slots during which CPU was stalled due to front-end bandwidth issues, such as inefficiencies in the instruction decoders or code restrictions for caching in the DSB (decoded uOps cache). In such cases, the front-end typically delivers a non-optimal amount of uOps to the back-end.</msg>
        <msg name="%FEBandwidthDSB">Front-End Bandwidth DSB</msg>
        <msg name="%FEBandwidthDSBDescriptionAll">This metric represents a fraction of cycles during which CPU was likely limited due to DSB (decoded uop cache) fetch pipeline.  For example, inefficient utilization of the DSB cache structure or bank conflict when reading from it, are categorized here.</msg>
        <msg name="%FEBandwidthDSBIssueTextAll">This metric represents a fraction of cycles during which CPU was likely limited due to DSB (decoded uop cache) fetch pipeline.  For example, inefficient utilization of the DSB cache structure or bank conflict when reading from it, are categorized here.</msg>
        <msg name="%FEBandwidthMITE">Front-End Bandwidth MITE</msg>
        <msg name="%FEBandwidthMITEDescriptionAll">This metric represents a fraction of cycles during which CPU was stalled due to the MITE fetch pipeline issues, such as inefficiencies in the instruction decoders.</msg>
        <msg name="%FEBandwidthMITEIssueTextAll">This metric represents a fraction of cycles during which CPU was stalled due to the MITE fetch pipeline issues, such as inefficiencies in the instruction decoders.</msg>
        <msg name="%BackendBound">Back-End Bound</msg>
        <msg name="%BackendBoundDescriptionAll">Back-End Bound metric represents a Pipeline Slots fraction where no uOps are being delivered due to a lack of required resources for accepting new uOps in the Back-End. Back-End is the portion of the processor core where an out-of-order scheduler dispatches ready uOps into their respective execution units, and, once completed, these uOps get retired according to the program order. For example, stalls due to data-cache misses or stalls due to the divider unit being overloaded are both categorized as Back-End Bound. Back-End Bound is further divided into two main categories: Memory Bound and Core Bound.</msg>
        <msg name="%BackendBoundIssueTextAll">A significant portion of pipeline slots are remaining empty. When operations take too long in the back-end, they introduce bubbles in the pipeline that ultimately cause fewer pipeline slots containing useful work to be retired per cycle than the machine is capable to support.  This opportunity cost results in slower execution. Long-latency operations like divides and memory operations can cause this, as can too many operations being directed to a single execution port (for example, more multiply operations arriving in the back-end per cycle than the execution unit can support).</msg>
        <msg name="%MemBound">Memory Bound</msg>
        <msg name="%DRAMLatencyBound">DRAM Latency Bound</msg>
        <msg name="%DRAMBandwidthBound">DRAM Bandwidth Bound</msg>
        <msg name="%DRAMBoundDescriptionHPCPC">This metric shows how often the CPU was stalled on the main memory (DRAM) because of demand loads or stores.</msg>
        <msg name="%DRAMBoundIssueHPCPC">The metric value is high. This indicates that a significant fraction of cycles could be stalled on the main memory (DRAM) because of demand loads or stores.</msg>
        <msg name="%DRAMLatencyBoundIssue">The code is latency bound, which means that there are a significant fraction of cycles during which the code could be stalled due to main memory latency. Consider optimizing data layout or using software prefetches through the compiler to improve cache reuse and to reduce the data fetched from the main memory.</msg>
        <msg name="%DRAMBandwidthBoundIssue">The code is memory bandwidth bound, which means that there are a significant fraction of cycles during which the bandwidth limits of the main memory are being reached and the code could stall. Review the Bandwidth Utilization Histogram to estimate the scale of the issue. Improve data accesses to reduce cacheline transfers from/to memory using these possible techniques: 1) consume all bytes of each cacheline before it is evicted (for example, reorder structure elements and split non-hot ones); 2) merge compute-limited and bandwidth-limited loops; 3) use NUMA optimizations on a multi-socket system.</msg>
        <msg name="%CacheBound">Cache Bound</msg>
        <msg name="%CacheBoundDescriptionText">This metric shows how often the machine was stalled on L1, L2, and L3 caches. While cache hits are serviced much more quickly than hits in DRAM, they can still incur a significant performance penalty. This metric also includes coherence penalties for shared data.</msg>
        <msg name="%CacheBoundIssueText">A significant proportion of cycles are being spent on data fetches from caches. Check Memory Access analysis to see if accesses to L2 or L3 caches are problematic and consider applying the same performance tuning as you would for a cache-missing workload. This may include reducing the data working set size, improving data access locality, blocking or partitioning the working set to fit in the lower cache levels, or exploiting hardware prefetchers. Consider using software prefetchers, but note that they can interfere with normal loads, increase latency, and increase pressure on the memory system. This metric includes coherence penalties for shared data. Check Microarchitecture Exploration analysis to see if contested accesses or data sharing are indicated as likely issues.</msg>
        <msg name="%MemEfficiency">Memory Efficiency</msg>
        <msg name="%MemEfficiencyDescription">This metric represents how efficiently the memory subsystem was used by the application. It shows the percent of cycles where the pipeline was not stalled due to demand load or store instructions. The metric is based on the Memory Bound measurement.</msg>
        <msg name="%MemBoundDescriptionAll">This metric shows how memory subsystem issues affect the performance. Memory Bound measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. This accounts mainly for incomplete in-flight memory demand loads that coincide with execution starvation in addition to less common cases where stores could imply back-pressure on the pipeline.</msg>
        <msg name="%MemBoundIssueTextAll">The metric value is high. This can indicate that the significant fraction of execution pipeline slots could be stalled due to demand memory load and stores. Use Memory Access analysis to have the metric breakdown by memory hierarchy, memory bandwidth information, correlation by memory objects.</msg>
        <msg name="%MemBoundIssueTextMA">The metric value is high. This may indicate that a significant fraction of execution pipeline slots could be stalled due to demand memory load and stores. Explore the metric breakdown by memory hierarchy, memory bandwidth information, and correlation by memory objects.</msg>
        <msg name="%L1Bound">L1 Bound</msg>
        <msg name="%L1BoundDescriptionAll">This metric shows how often machine was stalled without missing the L1 data cache. The L1 cache typically has the shortest latency. However, in certain cases like loads blocked on older stores, a load might suffer a high latency even though it is being satisfied by the L1.</msg>
        <msg name="%L1BoundIssueTextAll">This metric shows how often machine was stalled without missing the L1 data cache. The L1 cache typically has the shortest latency. However, in certain cases like loads blocked on older stores, a load might suffer a high latency even though it is being satisfied by the L1. Note that this metric value may be highlighted due to DTLB Overhead or Cycles of 1 Port Utilized issues.</msg>
        <msg name="%L1BoundNonGEIssueTextAll">This metric shows how often machine was stalled without missing the L1 data cache. The L1 cache typically has the shortest latency. However, in certain cases like loads blocked on older stores, a load might suffer a high latency even though it is being satisfied by the L1.</msg>
        <msg name="%L2Bound">L2 Bound</msg>
        <msg name="%L2BoundDescriptionAll">This metric shows how often machine was stalled on L2 cache. Avoiding cache misses (L1 misses/L2 hits) will improve the latency and increase performance.</msg>
        <msg name="%L2BoundIssueTextAll">This metric shows how often machine was stalled on L2 cache. Avoiding cache misses (L1 misses/L2 hits) will improve the latency and increase performance.</msg>
        <msg name="%UncoreBound">Uncore Bound</msg>
        <msg name="%UncoreBoundDescriptionAll">This metric shows how often machine was stalled on resources external to a core, for example, L3 cache and DRAM.</msg>
        <msg name="%UncoreBoundIssueTextAll">This metric shows how often machine was stalled on resources external to a core, for example, L3 cache and DRAM.</msg>
        <msg name="%CoreBound">Core Bound</msg>
        <msg name="%CoreBoundDescriptionAll">This metric represents how much Core non-memory issues were of a bottleneck.  Shortage in hardware compute resources, or dependencies software's instructions are both categorized under Core Bound. Hence it may indicate the machine ran out of an OOO resources, certain execution units are overloaded or dependencies in program's data- or instruction- flow are limiting the performance (e.g. FP-chained long-latency arithmetic operations).</msg>
        <msg name="%CoreBoundIssueTextAll">This metric represents how much Core non-memory issues were of a bottleneck.  Shortage in hardware compute resources, or dependencies software's instructions are both categorized under Core Bound. Hence it may indicate the machine ran out of an OOO resources, certain execution units are overloaded or dependencies in program's data- or instruction- flow are limiting the performance (e.g. FP-chained long-latency arithmetic operations).</msg>
        <msg name="%Cycles0PortsUtilized">Cycles of 0 Ports Utilized</msg>
        <msg name="%Cycles0PortsUtilizedDescriptionAll">This metric represents a fraction of cycles with no uOps executed by the CPU on any execution port. Long-latency instructions like divides may contribute to this metric.</msg>
        <msg name="%Cycles0PortsUtilizedIssueTextAll">CPU executed no uOps on any execution port during a significant fraction of cycles. Long-latency instructions like divides may contribute to this issue. Check the Assembly view and Appendix C in the Optimization Guide to identify instructions with 5 or more cycles latency.</msg>
        <msg name="%Cycles1PortUtilized">Cycles of 1 Port Utilized</msg>
        <msg name="%Cycles1PortUtilizedDescriptionAll">This metric represents cycles fraction where the CPU executed total of 1 uop per cycle on all execution ports. This can be due to heavy data-dependency among software instructions, or oversubscribing a particular hardware resource. In some other cases with high 1_Port_Utilized and L1 Bound, this metric can point to L1 data-cache latency bottleneck that may not necessarily manifest with complete execution starvation (due to the short L1 latency e.g. walking a linked list) - looking at the assembly can be helpful.</msg>
        <msg name="%Cycles1PortUtilizedIssueTextAll">This metric represents cycles fraction where the CPU executed total of 1 uop per cycle on all execution ports. This can be due to heavy data-dependency among software instructions, or oversubscribing a particular hardware resource. In some other cases with high 1_Port_Utilized and L1 Bound, this metric can point to L1 data-cache latency bottleneck that may not necessarily manifest with complete execution starvation (due to the short L1 latency e.g. walking a linked list) - looking at the assembly can be helpful. Note that this metric value may be highlighted due to L1 Bound issue.</msg>
        <msg name="%Cycles2PortsUtilized">Cycles of 2 Ports Utilized</msg>
        <msg name="%Cycles2PortsUtilizedDescriptionAll">This metric represents cycles fraction CPU executed total of 2 uops per cycle on all execution ports. Tip: Loop Vectorization - most compilers feature auto-Vectorization options today- reduces pressure on the execution ports as multiple elements are calculated with same uop.</msg>
        <msg name="%Cycles2PortsUtilizedIssueTextAll">This metric represents cycles fraction CPU executed total of 2 uops per cycle on all execution ports. Tip: Loop Vectorization - most compilers feature auto-Vectorization options today- reduces pressure on the execution ports as multiple elements are calculated with same uop.</msg>
        <msg name="%Cycles3mPortsUtilized">Cycles of 3+ Ports Utilized</msg>
        <msg name="%Cycles3mPortsUtilizedDescriptionAll">This metric represents Core cycles fraction CPU executed total of 3 or more uops per cycle on all execution ports.</msg>
        <msg name="%Cycles3mPortsUtilizedIssueTextAll">This metric represents Core cycles fraction CPU executed total of 3 or more uops per cycle on all execution ports.</msg>
        <msg name="%LCP">Length Changing Prefixes</msg>
        <msg name="%LCPDescriptionAll">This metric represents a fraction of cycles during which CPU was stalled due to Length Changing Prefixes (LCPs). To avoid this issue, use proper compiler flags. Intel Compiler enables these flags by default.</msg>
        <msg name="%LCPIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was stalled due to Length Changing Prefixes (LCPs).</p><p><strong>Tips:</strong> To avoid this issue, use proper compiler flags. Intel Compiler enables these flags by default.</p><p>See the "Length-Changing Prefixes (LCP)" section in the Intel 64 and IA-32 Architectures Optimization Reference Manual.</p> ]]></msg>
        <msg name="%MicroSequencer">Microcode Sequencer</msg>
        <msg name="%MicroSequencerDescriptionAll">This metric represents a fraction of slots during which CPU was retiring uOps fetched by the Microcode Sequencer (MS) ROM. The MS is used for CISC instructions not fully decoded by the default decoders (like repeat move strings), or by microcode assists used to address some modes of operation (like in Floating-Point assists).</msg>
        <msg name="%MicroSequencerIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was spent retiring uOps fetched by the Microcode Sequencer.</p><p><strong>Tips:</strong></p><p>1. Make sure the /arch compiler flags are correct.</p><p>2. Check the child Assists metric and, if it is highlighted as an issue, follow the provided recommendations.</p><p>Note that this metric value may be highlighted due to MS Switches issue.</p> ]]></msg>
        <msg name="%BASE">General Retirement</msg>
        <msg name="%BASEDescriptionAll">This metric represents a fraction of slots during which CPU was retiring uOps not originated from the Microcode Sequencer. This correlates with the total number of instructions executed by the program. A uOps-per-Instruction ratio of 1 is expected. While this is the most desirable of the top 4 categories, high values may still indicate areas for improvement. If possible focus on techniques that reduce instruction count or result in more efficient instructions generation such as vectorization.</msg>
        <msg name="%BASEIssueTextAll">This metric represents a fraction of slots during which CPU was retiring uOps not originated from the Microcode Sequencer. This correlates with the total number of instructions executed by the program. A uOps-per-Instruction ratio of 1 is expected. While this is the most desirable of the top 4 categories, high values may still indicate areas for improvement. If possible focus on techniques that reduce instruction count or result in more efficient instructions generation such as vectorization.</msg>
        <msg name="%PortUtil">Port Utilization</msg>
        <msg name="%PortUtilDescriptionAll">This metric represents a fraction of cycles during which an application was stalled due to Core non-divider-related issues. For example, heavy data-dependency between nearby instructions, or a sequence of instructions that overloads specific ports. Hint: Loop Vectorization - most compilers feature auto-Vectorization options today - reduces pressure on the execution ports as multiple elements are calculated with same uop.</msg>
        <msg name="%PortUtilIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was stalled due to Core non-divider-related issues.</p><p><strong>Tips:</strong> Use vectorization to reduce pressure on the execution ports as multiple elements are calculated with same uOp.</p> ]]></msg>
        <msg name="%L3Latency">L3 Latency</msg>
        <msg name="%L3LatencyDescriptionAll">This metric shows a fraction of cycles with demand load accesses that hit the L3 cache under unloaded scenarios (possibly L3 latency limited). Avoiding private cache misses (i.e. L2 misses/L3 hits) will improve the latency, reduce contention with sibling physical cores and increase performance. Note the value of this node may overlap with its siblings.</msg>
        <msg name="%L3LatencyIssueTextAll">This metric shows a fraction of cycles with demand load accesses that hit the L3 cache under unloaded scenarios (possibly L3 latency limited). Avoiding private cache misses (i.e. L2 misses/L3 hits) will improve the latency, reduce contention with sibling physical cores and increase performance. Note the value of this node may overlap with its siblings.</msg>
        <msg name="%L3Bound">L3 Bound</msg>
        <msg name="%L3BoundDescriptionAll">This metric shows how often CPU was stalled on L3 cache, or contended with a sibling Core. Avoiding cache misses (L2 misses/L3 hits) improves the latency and increases performance.</msg>
        <msg name="%L3BoundIssueTextAll">This metric shows how often CPU was stalled on L3 cache, or contended with a sibling Core. Avoiding cache misses (L2 misses/L3 hits) improves the latency and increases performance.</msg>
        <msg name="%DRAMBound">DRAM Bound</msg>
        <msg name="%DRAMBoundDescriptionAll">This metric shows how often CPU was stalled on the main memory (DRAM). Caching typically improves the latency and increases performance.</msg>
        <msg name="%DRAMBoundIssueTextAll">This metric shows how often CPU was stalled on the main memory (DRAM). Caching typically improves the latency and increases performance.</msg>
        <msg name="%MCDRAMBound">MCDRAM Bound</msg>
        <msg name="%MCDRAMBoundDescriptionAll">This metric shows how often CPU was stalled on the hig bandiwdth memory (MCDRAM). Caching typically improves the latency and increases performance.</msg>
        <msg name="%MCDRAMBoundIssueTextAll">This metric shows how often CPU was stalled on the hig bandiwdth memory (MCDRAM). Caching typically improves the latency and increases performance.</msg>
        <msg name="%MEMBandwidth">Memory Bandwidth</msg>
        <msg name="%MEMBandwidthDescriptionAll">This metric represents a fraction of cycles during which an application could be stalled due to approaching bandwidth limits of the main memory (DRAM). This metric does not aggregate requests from other threads/cores/sockets (see Uncore counters for that). Consider improving data locality in NUMA multi-socket systems.</msg>
        <msg name="%MEMBandwidthIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was stalled due to approaching bandwidth limits of the main memory (DRAM).</p><p><strong>Tips:</strong> Improve data accesses to reduce cacheline transfers from/to memory using these possible techniques:<ul><li>Consume all bytes of each cacheline before it is evicted (for example, reorder structure elements and split non-hot ones).</li><li>Merge compute-limited and bandwidth-limited loops.</li><li>Use NUMA optimizations on a multi-socket system.</li></ul></p><p>Note: software prefetches do not help a bandwidth-limited application.</p> ]]></msg>
        <msg name="%MEMLatency">Memory Latency</msg>
        <msg name="%MEMLatencyDescriptionAll">This metric represents a fraction of cycles during which an application could be stalled due to the latency of the main memory (DRAM). This metric does not aggregate requests from other threads/cores/sockets (see Uncore counters for that). Consider optimizing data layout or using Software Prefetches (through the compiler).</msg>
        <msg name="%MEMLatencyIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was stalled due to the latency of the main memory (DRAM).</p><p><strong>Tips:</strong> Improve data accesses or interleave them with compute using such possible techniques as data layout re-structuring or software prefetches (through the compiler).</p> ]]></msg>
        <msg name="%LocalDRAM">Local DRAM</msg>
        <msg name="%LocalDRAMDescriptionAll">This metric shows how often CPU was stalled on loads from local memory. Caching will improve the latency and increase performance.</msg>
        <msg name="%LocalDRAMIssueTextAll">The number of CPU stalls on loads from the local memory exceeds the threshold. Consider caching data to improve the latency and increase the performance.</msg>
        <msg name="%RemoteDRAM">Remote DRAM</msg>
        <msg name="%RemoteDRAMDescriptionAll">This metric shows how often CPU was stalled on loads from remote memory. This is caused often due to non-optimal NUMA allocations.</msg>
        <msg name="%RemoteDRAMIssueTextAll">The number of CPU stalls on loads from the remote memory exceeds the threshold. This is often caused by non-optimal NUMA memory allocations.</msg>
        <msg name="%Local_PMM">Local Persistent Memory</msg>
        <msg name="%Local_PMMDescriptionAll">This metric shows how often CPU was stalled on loads from the local Intel Optane DC Persistent Memory. Caching will improve the latency and increase performance.</msg>
        <msg name="%Remote_PMM">Remote Persistent Memory</msg>
        <msg name="%Remote_PMMDescriptionAll">This metric shows how often CPU was stalled on loads from the remote Intel Optane DC Persistent Memory. This is caused often due to non-optimal NUMA allocations.</msg>
        <msg name="%RemoteCache">Remote Cache</msg>
        <msg name="%RemoteCacheDescriptionAll">This metric shows how often CPU was stalled on loads from remote cache in other sockets. This is caused often due to non-optimal NUMA allocations.</msg>
        <msg name="%RemoteCacheIssueTextAll">The number of CPU stalls on loads from the remote cache exceeds the threshold. This is often caused by non-optimal NUMA memory allocations.</msg>
        <msg name="%StoresBound">Store Bound</msg>
        <msg name="%StoresBoundDescriptionAll">This metric shows how often CPU was stalled on store operations. Even though memory store accesses do not typically stall out-of-order CPUs; there are few cases where stores can lead to actual stalls.</msg>
        <msg name="%StoresBoundIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> CPU was stalled on store operations for a significant fraction of cycles.</p><p><strong>Tips:</strong> Consider False Sharing analysis as your next step.</p> ]]></msg>
        <msg name="%FalseSharing">False Sharing</msg>
        <msg name="%FalseSharingDescriptionAll">This metric shows how often CPU was stalled on store operations to a shared cache line. It can be easily avoided by padding to make threads access different lines.</msg>
        <msg name="%FalseSharingIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> For a significant fraction of cycles CPU was stalled on store operations to a shared cache line.</p><p><strong>Tips:</strong> Use padding to make threads access different lines.</p> ]]></msg>
        <msg name="%SplitStores">Split Stores</msg>
        <msg name="%SplitStoresDescriptionAll">This metric represents a rate of split store accesses. Consider aligning your data to the 64-byte cache line granularity.</msg>
        <msg name="%SplitStoresIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was stalled due to split store accesses.</p><p><strong>Tips:</strong> Consider aligning your data to the 64-byte cache line granularity.</p> ]]></msg>
        <msg name="%DTLBStoreOverhead">DTLB Store Overhead</msg>
        <msg name="%DTLBStoreOverheadDescriptionAll">This metric represents a fraction of cycles spent on handling first-level data TLB store misses. As with ordinary data caching, focus on improving data locality and reducing working-set size to reduce DTLB overhead. Additionally, consider using profile-guided optimization (PGO) to collocate frequently-used data on the same page. Try using larger page sizes for large amounts of frequently-used data.</msg>
        <msg name="%DTLBStoreOverheadIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles is spent on handling first-level data TLB store misses.</p><p><strong>Tips:</strong></p><p>1. As with ordinary data caching, focus on improving data locality and reducing working-set size to minimize the DTLB overhead.</p><p>2. Consider using profile-guided optimization (PGO) to collocate frequently-used data on the same page.</p><p>3. Try using larger page sizes for large amounts of frequently-used data.</p> ]]></msg>
        <msg name="%BranchResteers">Branch Resteers</msg>
        <msg name="%BranchResteersDescriptionAll">This metric represents cycles fraction the CPU was stalled due to Branch Resteers. Branch Resteers estimates the Frontend delay in fetching operations from corrected path, following all sorts of misspredicted branches. For example, branchy code with lots of misspredictions might get categorized under Branch Resteers. Note the value of this node may overlap with its siblings.</msg>
        <msg name="%BranchResteersIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was stalled due to Branch Resteers. Branch Resteers estimate the Front-End delay in fetching operations from corrected path, following all sorts of mispredicted branches. For example, branchy code with lots of mispredictions might get categorized as Branch Resteers. Note the value of this node may overlap its siblings.</p> ]]></msg>
        <msg name="%FarBranch">Far Branch</msg>
        <msg name="%FarBranchDescriptionAll">This metric indicates when a call/return is using a far pointer. A far call is often used to transfer from user code to privileged code.</msg>
        <msg name="%FarBranchIssueTextAll">Transferring from user to privileged code may be too frequent. Consider reducing calls to system APIs.</msg>
        <msg name="%FP_Arith">FP Arithmetic</msg>
        <msg name="%FP_ArithDescriptionAll">This metric represents an overall arithmetic floating-point (FP) uOps fraction the CPU has executed (retired).</msg>
        <msg name="%FP_ArithIssueTextAll">This metric represents an overall arithmetic floating-point (FP) uOps fraction the CPU has executed (retired).</msg>
        <msg name="%FP_x87">FP x87</msg>
        <msg name="%FP_x87DescriptionAll">This metric represents a floating-point (FP) x87 uops fraction the CPU has executed. It accounts for instructions beyond X87 FP arithmetic operations; hence may be used as a thermometer to avoid X87 high usage and preferably upgrade to modern ISA. Consider compiler flags to generate newer AVX (or SSE) instruction sets, which typically perform better and feature vectors.</msg>
        <msg name="%FP_x87IssueTextAll">This metric represents a floating-point (FP) x87 uops fraction the CPU has executed. It accounts for instructions beyond X87 FP arithmetic operations; hence may be used as a thermometer to avoid X87 high usage and preferably upgrade to modern ISA. Consider compiler flags to generate newer AVX (or SSE) instruction sets, which typically perform better and feature vectors.</msg>
        <msg name="%FP_Scalar">FP Scalar</msg>
        <msg name="%FP_ScalarDescriptionAll">This metric represents an arithmetic floating-point (FP) scalar uops fraction the CPU has executed. Analyze metric values to identify why vector code is not generated, which is typically caused by the selected algorithm or missing/wrong compiler switches.</msg>
        <msg name="%FP_ScalarIssueTextAll">This metric represents an arithmetic floating-point (FP) scalar uops fraction the CPU has executed. Analyze metric values to identify why vector code is not generated, which is typically caused by the selected algorithm or missing/wrong compiler switches.</msg>
        <msg name="%FP_Vector">FP Vector</msg>
        <msg name="%FP_VectorDescriptionAll">This metric represents an arithmetic floating-point (FP) vector uops fraction the CPU has executed. Make sure vector width is expected.</msg>
        <msg name="%FP_VectorIssueTextAll">This metric represents an arithmetic floating-point (FP) vector uops fraction the CPU has executed. Make sure vector width is expected.</msg>
        <msg name="%OTHER">Other</msg>
        <msg name="%OTHERDescriptionAll">This metric represents a non-floating-point (FP) uop fraction the CPU has executed. If your application has no FP operations, this is likely to be the biggest fraction.</msg>
        <msg name="%OTHERIssueTextAll">This metric represents a non-floating-point (FP) uop fraction the CPU has executed. If your application has no FP operations, this is likely to be the biggest fraction.</msg>
        <msg name="%MSSwitches">MS Switches</msg>
        <msg name="%MSSwitchesDescriptionAll">This metric represents a fraction of cycles when the CPU was stalled due to switches of uop delivery to the Microcode Sequencer (MS). Commonly used instructions are optimized for delivery by the DSB or MITE pipelines. Certain operations cannot be handled natively by the execution pipeline, and must be performed by microcode (small programs injected into the execution stream). Switching to the MS too often can negatively impact performance. The MS is designated to deliver long uop flows required by CISC instructions like CPUID, or uncommon conditions like Floating Point Assists when dealing with Denormals.</msg>
        <msg name="%MSSwitchesIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of cycles was stalled due to switches of uOp delivery to the Microcode Sequencer (MS). Commonly used instructions are optimized for delivery by the DSB or MITE pipelines. Certain operations cannot be handled natively by the execution pipeline, and must be performed by microcode (small programs injected into the execution stream). Switching to the MS too often can negatively impact performance. The MS is designated to deliver long uOp flows required by CISC instructions like CPUID, or uncommon conditions like Floating Point Assists when dealing with Denormals. Note that this metric value may be highlighted due to Microcode Sequencer issue.</p> ]]></msg>
        <msg name="%Serializing_Operation">Serializing Operations</msg>
        <msg name="%Serializing_OperationDescriptionAll">This metric represents a fraction of cycles the CPU issue pipeline was stalled due to serializing operations. Instructions like CPUID, WRMSR, or LFENCE serialize the out-of-order execution, which may limit performance.</msg>
        <msg name="%Serializing_OperationIssueTextAll">A significant fraction of cycles was spent handling serializing operations. Instructions like CPUID, WRMSR, or LFENCE serialize the out-of-order execution, which may limit performance.</msg>
        <msg name="%Slow_Pause">Slow Pause</msg>
        <msg name="%Slow_PauseDescriptionAll">This metric represents a fraction of cycles the CPU was stalled due to PAUSE instructions.</msg>
        <msg name="%Slow_PauseIssueTextAll">A significant fraction of cycles was spent handling PAUSE inctructions.</msg>
        <msg name="%IXP_Bound">Persistent Memory Bound</msg>
        <msg name="%IXP_BoundDescriptionAll">This metric estimates how often the CPU was stalled on accesses to external Intel Optane DC Persistent Memory by loads.</msg>
        <msg name="%IXP_BoundIssueTextAll">Significant percentage of cycles is being spent on demand loads from the Intel Optane DC Persistent Memory. Consider caching data to improve the latency and increase the performance.</msg>
        <msg name="%PMM_Bound">Persistent Memory Bound</msg>
        <msg name="%PMM_BoundDescriptionAll">This metric estimates how often the CPU was stalled on accesses to external Intel Optane DC Persistent Memory by loads.</msg>
        <msg name="%PMM_BoundIssueTextAll">Significant percentage of cycles is being spent on demand loads from the Intel Optane DC Persistent Memory. Consider caching data to improve the latency and increase the performance.</msg>
        <msg name="%Reliability">MUX Reliability</msg>
        <msg name="%ReliabilityDescriptionAll">This metric estimates reliability of HW event-related metrics. Since the number of collected HW events exceeds the number of counters, PRODUCT_LEGAL_SHORT_NAME uses event multiplexing (MUX) to share HW counters and collect different subsets of events over time. This may affect the precision of collected event data. The ideal value for this metric is 1. If the value is less than 0.7, the collected data may be not reliable.</msg>
        <msg name="%ReliabilityIssueTextAll">Precision of collected HW event data is not enough. Metrics data may be unreliable. Consider increasing your application execution time, using the multiple runs mode instead of event multiplexing, or creating a custom analysis with a limited subset of HW events. If you are using a driverless collection, consider reducing the value of /sys/bus/event_source/devices/cpu/perf_event_mux_interval_ms file.</msg>
        <msg name="%HLERegionsStarted">HLE Regions</msg>
        <msg name="%HLERegionsStartedDescription">This metric shows a number of times an HLE transactional execution was started.</msg>
        <msg name="%RTMRegionsStarted">RTM Regions</msg>
        <msg name="%RTMRegionsStartedDescription">This metric shows a number of times an RTM transactional execution was started.</msg>
        <msg name="%TSXRegionsStarted">TSX Regions</msg>
        <msg name="%TSXRegionsStartedDescription">This metric shows a number of times an HLE or RTM transactional execution was started.</msg>
        <msg name="%HLEAborted">HLE Aborts</msg>
        <msg name="%HLEAbortedDescription">This metric shows the number of aborted HLE transactional executions.</msg>
        <msg name="%RTMAborted">RTM Aborts</msg>
        <msg name="%RTMAbortedDescription">This metric shows the number of aborted RTM transactional executions.</msg>
        <msg name="%TSXAborted">TSX Aborts</msg>
        <msg name="%TSXAbortedDescription">This metric shows the number of aborted HLE or RTM transactional executions.</msg>
        <msg name="%TSXCommitted">TSX Commits</msg>
        <msg name="%TSXCommittedDescription">This metric shows the number of committed HLE or RTM transactional executions.</msg>
        <msg name="%TSXAbortedInstruction">Instruction</msg>
        <msg name="%TSXAbortedInstructionDescription">This metric shows the number of HLE or RTM transactional executions that aborted due to a specific instruction.</msg>
        <msg name="%TSXAbortedDataConflict">Data Conflict</msg>
        <msg name="%TSXAbortedDataConflictDescription">This metric shows the number of HLE or RTM transactional executions that aborted due to a data conflict.</msg>
        <msg name="%TSXAbortedCapacity">Capacity</msg>
        <msg name="%TSXAbortedCapacityDescription">This metric shows the number of HLE or RTM transactional executions that aborted due to capacity.</msg>
        <msg name="%TSXAbortedOther">Other</msg>
        <msg name="%TSXAbortedOtherDescription">This metric shows the number of HLE or RTM transactional executions that aborted due to other issues.</msg>
        <msg name="%HLEAbortRate">HLE Abort Ratio</msg>
        <msg name="%HLEAbortRateDescription">This metric shows a fraction of aborted HLE transactional executions.</msg>
        <msg name="%RTMAbortRate">RTM Abort Rate</msg>
        <msg name="%RTMAbortRateDescription">This metric shows a fraction of aborted RTM transactional executions.</msg>
        <msg name="%TSXAbortRate">TSX Abort Ratio</msg>
        <msg name="%TSXAbortRateDescription">This metric shows a fraction of aborted HLE or RTM transactional executions.</msg>
        <msg name="%TSXAbortRateDataConflict">Data Conflicts</msg>
        <msg name="%TSXAbortRateDataConflictDescription">This metric shows a fraction of HLE or RTM transactional executions that aborted due to a data conflict.</msg>
        <msg name="%TSXAbortRateResource">Resource</msg>
        <msg name="%TSXAbortRateResourceDescription">This metric shows a fraction of HLE or RTM transactional executions that aborted due to resource limitations.</msg>
        <msg name="%ElideLocksRate">Non-Elided Ratio</msg>
        <msg name="%ElideLocksRateDescription">This metric shows a fraction of locks not marked as elided. If the number of lock loads is significantly higher than the number of transactional regions started, this usually means that not all locks are marked for lock elision.</msg>
        <msg name="%HLESpecificAborts">HLE Sepcific Aborts</msg>
        <msg name="%HLEStoreToElidedLock">Store to Elided Lock</msg>
        <msg name="%HLEStoreToElidedLockDescription">This metric counts the number of transactional aborts due to a store operation without the XRELEASE prefix operating on an elided lock in the elision buffer. This often happens because the library is missing the XRELEASE prefix on the lock release instruction.</msg>
        <msg name="%HLEBufferNotEmpty">Buffer Not Empty</msg>
        <msg name="%HLEBufferNotEmptyDescription">This metric counts the number of transactional aborts that occur because an XRELEASE prefixed lock release instruction committing the transactional execution finds the elision buffer with an elided lock. This typically happens for code sequences where an XRELEASE occurs on a lock that was not elided and hence was not in the elision buffer.</msg>
        <msg name="%HLEBufferMismatch">Buffer Mismatch</msg>
        <msg name="%HLEBufferMismatchDescription">This metric counts the number of transactional aborts occurred because the XRELEASE lock does not satisfy the address and value requirements for elision in the elision buffer. This may occur if a value being written by the XRELEASE operation is different from the value that was read by the earlier XACQUIRE operation to the same lock.</msg>
        <msg name="%HLEUnsupportedAlignment">Unsupported Alignment</msg>
        <msg name="%HLEUnsupportedAlignmentDescription">This metric counts the number of transactional aborts if the lock in the elision buffer was accessed by a read in the transactional region but the read could not be serviced. This typically occurs if the access was not properly aligned, or had a partial overlap, or the read operation's linear address was different than the elided locks but the physical address was the same. These are fairly rare events.</msg>
        <msg name="%TransactionalSuccess">Transactional Success</msg>
        <msg name="%TransactionalCycles">Transactional Cycles</msg>
        <msg name="%TransactionalCyclesDescription">This metric counts the number of cycles spent inside transactional regions. If this metric is near zero then the application is either not using lock-based synchronization or
not using a synchronization library enabled for lock elision through the Intel TSX instructions.</msg>
        <msg name="%TransactionalCyclesAborted">Abort Cycles</msg>
        <msg name="%TransactionalCyclesAbortedDescription">This metric counts the number of cycles spent inside transactional regions which were eventually aborted. If this metric is small relative to Transactional Cycles, then the transactional success rate
is high and additional tuning is not required. If this metric is almost the same as Transactional Cycles (but not very small), then most transactional regions are aborting and lock elision is not going to be beneficial. The next step would be to identify the causes for transactional aborts and reduce them</msg>
        <msg name="%TSXAbortedInstructions">Abort Instructions</msg>
        <msg name="%TSXAbortedInstructionsDescription">This metric counts the number of instructions spent inside aborted transactional regions.</msg>
        <msg name="%TransactionalCyclesCommitted">Commit Cycles</msg>
        <msg name="%TransactionalCyclesCommittedDescription">This metric counts the number of cycles spent inside committed transactional regions.</msg>
        <msg name="%TSXCommittedInstructions">Commit Instructions</msg>
        <msg name="%TSXCommittedInstructionsDescription">This metric counts the number of instructions spent inside committed transactional regions.</msg>
        <msg name="%TransactionalCyclesPercentage">Transactional Cycles (%)</msg>
        <msg name="%TransactionalCyclesPercentageDescription">Percentage of cycles spent inside transactional regions.</msg>
        <msg name="%TransactionalCyclesAbortPercentage">Abort Cycles (%)</msg>
        <msg name="%TransactionalCyclesAbortPercentageDescription">Percentage of Transactional Cycles which were eventually aborted.</msg>
        <msg name="%PTTSXAbortedMetrics">Aborts</msg>
        <msg name="%PTTSXAbortedMetricsDescription">This group of metrics describes aborted transactional regions.</msg>
        <msg name="%PTTSXCommittedMetrics">Commits</msg>
        <msg name="%PTTSXCommittedMetricsDescription">This group of metrics describes committed transactional regions.</msg>
        <msg name="%UpperBoundDescription">This metric calculates floating point operations (FLOP) with the assumption that vector instructions operate with full 512-bit vectors (FLOP upper bound). A poor upper bound value usually reflects a performance problem. Use Intel Advisor for more accurate FLOP counting.</msg>
        <msg name="%UpperBoundName">&#32;Upper Bound</msg>
        <msg name="%SingleFPOperationsPerCycle">SP FLOPs Per Cycle<arg name="arg0"/></msg>
        <msg name="%SingleFPOperationsPerCycleDescription">Number of single precision floating point operations (0...<arg name="arg1"/>) executed per clocktick. This processor can execute maximum 8 single precision operations per cycle per core. This metric shows how effectively your code uses the floating point unit. <arg name="arg0"/></msg>
        <msg name="%DoubleFPOperationsPerCycle">DP FLOPs Per Cycle<arg name="arg0"/></msg>
        <msg name="%DoubleFPOperationsPerCycleDescription">Number of double precision floating point operations (0...<arg name="arg1"/>) executed per clocktick. This processor can execute maximum 4 double precision operations per cycle per core. This metric shows how effectively your code uses the floating point unit. <arg name="arg0"/></msg>
        <msg name="%x87FPOperationsPerCycle">x87 FLOPs Per Cycle<arg name="arg0"/></msg>
        <msg name="%x87FPOperationsPerCycleDescription">Number of x87 floating point operations (0...1) executed per clocktick. Only one x87 floating point operation can be executed per cycle per core. Note that these operations cannot be parallelized at the instructions level. <arg name="arg0"/></msg>
        <msg name="%FPUUsage">FPU Usage<arg name="arg0"/></msg>
        <msg name="%FPUUsageDescription">This metric represents how intensively your program uses the FPU. 1 means that FPU is 100% loaded. Note that CPU utilization for a double-precision operation is equal to two single-precision operations. <arg name="arg0"/></msg>
        <msg name="%ScalarFPOperationsPerCycle">Scalar FLOPs Per Cycle<arg name="arg0"/></msg>
        <msg name="%ScalarFPOperationsPerCycleDescription">Number of scalar floating point operations per clocktick. A high level of scalar floating point operations can indicate inefficient vectorization. <arg name="arg0"/></msg>
        <msg name="%PackedFPOperationsPerCycle">Packed FLOPs Per Cycle<arg name="arg0"/></msg>
        <msg name="%PackedFPOperationsPerCycleDescription">Number of packed floating point operations per clocktick. A low level of Packed FLOPs Per Cycle compared to the Scalar FLOPs Per Cycle can indicate inefficient vectorization. <arg name="arg0"/></msg>
        <msg name="%FLOPPerCycle">FLOPs Per Cycle<arg name="arg0"/></msg>
        <msg name="%FLOPPerCycleDescription">Number of floating point operations per clocktick. This metric shows how intensively your code uses the FPU. <arg name="arg0"/></msg>
        <msg name="%VectInstSetIssue">You are not using a modern vectorization instruction set. Consider recompiling your code using compiler options that allow using a modern vectorization instruction set. See the compiler User and Reference Guide for C++ or Fortran for more details.</msg>
        <msg name="%PackedFLOPsBandwidthBoundIssue">The code is Memory Bandwidth Bound. Further vectorization improvements might not increase the application performance. Use the Memory Access Analysis to find opportunities to reduce memory bandwidth of the code.</msg>
        <msg name="%FPUUsageVectorizationIssue">The code has potential for vectorization improvements. Please try the Intel Advisor for data and tips to improve the efficiency of vectorization in your application.</msg>
        <msg name="%ScalarFLOPsIssue">This code has floating point operations and is not vectorized. Consider using the Intel Advisor to vectorize loops.</msg>
        <msg name="%RunAdvisorForMoreInfo">Consider using vector analysis in Intel Advisor for a deeper understanding of instruction-level parallelism in your code.</msg>
        <msg name="%FPUUtilizationAPSDescription">How much of the FPU is used by your application? This metric helps to evaluate the vector efficiency of your application. It estimates the percentage of operations that are performed by the FPU. 100% means that the FPU is fully used.</msg>
        <msg name="%FPUUtilizationAPSIssue">A low value indicates that your application is not effectively vectorized and uses the FPU poorly. This can be caused by integer applications that do not use floating point arithmetic. If your application is using significant floating point arithmetic, you can use vectorization efficiency analysis tools to explore where to vectorize, causes of inefficient vectorization, or memory access pattern issues. Learn more at https://software.intel.com/en-us/get-started-with-advisor</msg>
        <msg name="%MemBoundAPSDescription">How does the memory subsystem affect the performance of your application? This metric measures the fraction of CPU pipeline slots that are stalled due to load or store instructions. Such stalls are usually caused by load instructions causing execution to stall until the load is completed, or, in less common cases, where incomplete stores imply back-pressure on the pipeline causing it to stall.</msg>
        <msg name="%MemBoundAPSIssueLong">A high value indicates significant pipeline stalls due to memory-related performance issues like poor cache reuse, excessive NUMA remote accesses, false sharing, or bandwidth limits. You can use memory access profiling tools to identify specific bottlenecks so that you can reorganize your code to reduce the stalls caused by memory operations. Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%MemBoundAPSIssueShort">A high value indicates significant pipeline stalls due to memory-related performance issues.</msg>
        <msg name="%CacheBoundAPSIssueText">%CacheBoundPercentageAPS% of cycles are being spent on data fetches from caches. You can use memory access profiling tools to see if accesses to L2 or L3 caches are problematic and consider applying the same performance tuning as you would for a cache-missing workload. Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%DRAMBoundAPSIssueLong">%DRAMBoundForHPCPCPercentageAPS% of cycles are being spent on demand loads and stores from the main memory (DRAM). You can use memory access profiling tools to identify specific bottlenecks so that you can reorganize your code to reduce the stalls caused by access to DRAM. Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%DRAMBoundAPSIssueShort">%DRAMBoundForHPCPCPercentageAPS% of cycles are being spent on demand loads and stores from the main memory (DRAM).</msg>
        <msg name="%DRAMBandwidthBoundAPSIssueText">- The application is memory bandwidth bound, which means that there are a significant fraction of cycles during which the bandwidth limits of the main memory are being reached and the application could stall. Consider improving data locality on NUMA multi-socket systems, which will reduce code memory bandwidth consumption. Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%DRAMLatencyBoundAPSIssueText">- The application is latency bound, which means that there are a significant fraction of cycles during which the application could be stalled due to main memory latency. Consider optimizing data layout or using software prefetches through the compiler to improve cache reuse and to reduce the data fetched from the main memory. Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%BackendBoundAPSDescription">Superscalar processors can be conceptually divided into the `front-end`, where instructions are fetched and decoded into the operations that constitute them and the `back-end`, where the required computation is performed. During each cycle, the front-end generates up to four of these operations, places them into pipeline slots and moves them through the back-end. The actual number of retired pipeline slots containing useful work rarely equals this maximum. This can be because the back-end was not prepared to accept more operations of a certain kind (`Back-end bound` execution). Back-end bound execution may be due to long-latency operations or other contention for execution resources.</msg>
        <msg name="%BackendBoundAPSIssueLong">A high value indicates significant pipeline stalls due to long-latency operations like divides, memory operations, or too many operations being directed to a single execution port (for example, more multiply operations arriving in the back-end per cycle than the execution unit can support). Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%BackendBoundAPSIssueShort">A high value indicates significant pipeline stalls due to long-latency operations like divides, memory operations, or too many operations being directed to a single execution port.</msg>
        <msg name="%L2HitBoundAPSIssue">%LLCHitKNLAPS% of CPU cycles are being spent on data fetches that miss the L1 but hit the L2. This metric includes coherence penalties for shared data. If contested accesses or data sharing are indicated as likely issues, address them first. Otherwise, consider the same performance tuning as you would apply for an L2-missing workload. Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%L2MissBoundAPSIssue">%LLCMissKNLAPS% of CPU cycles are being spent waiting for L2 load misses to be serviced. Possible optimizations are to reduce data working set size, improve data access locality, blocking and consuming data in chunks that fit in the L2, or better exploit hardware prefetchers. Learn more at https://software.intel.com/en-us/memory-access-analysis-win</msg>
        <msg name="%FEBandwidthLSD">Front-End Bandwidth LSD</msg>
        <msg name="%FEBandwidthLSDDescriptionAll">This metric represents a fraction of cycles during which CPU operation was limited by the LSD (Loop Stream Detector) unit. Typically, LSD provides good uOp supply. However, in some rare cases, optimal uOp delivery cannot be reached for small loops whose size (in terms of number of uOps) does not suit well the LSD structure.</msg>
        <msg name="%FEBandwidthLSDIssueTextAll">A significant number of CPU cycles is spent waiting for uOps for the LSD (Loop Stream Detector) unit. Typically, LSD provides good uOp support. However, in some rare cases, optimal uOp delivery cannot be reached for small loops whose size (in terms of number of uOps) does not suit well the LSD structure.</msg>
        <msg name="%PMUMetricConfidenceText">This metric cannot be reliably calculated due to multiplexing issues or a low number of collected PMU samples.</msg>
        <msg name="%UPI">UPI</msg>
        <msg name="%FBFull">FB Full</msg>
        <msg name="%FBFullDescriptionAll">This metric does a rough estimation of how often L1D Fill Buffer unavailability limited additional L1D miss memory access requests to proceed. The higher the metric value, the deeper the memory hierarchy level the misses are satisfied from. Often it hints on approaching bandwidth limits (to L2 cache, L3 cache or external memory).</msg>
        <msg name="%FBFullIssueTextAll">This metric does a rough estimation of how often L1D Fill Buffer unavailability limited additional L1D miss memory access requests to proceed. The higher the metric value, the deeper the memory hierarchy level the misses are satisfied from. Often it hints on approaching bandwidth limits (to L2 cache, L3 cache or external memory). Avoid adding software prefetches if indeed memory BW limited.</msg>
        <msg name="%L3Bandwidth">L3 Bandwidth</msg>
        <msg name="%L3BandwidthDescriptionAll">This metric represents cycles fraction with demand load accesses that hit the L3 cache under loaded scenarios (possibly L3 Bandwidth limited).  Avoiding private cache misses (i.e. L2 misses/L3 hits) will improve the latency, reduce contention with sibling physical cores and increase performance. Note the value of this node may overlap with its siblings.</msg>
        <msg name="%L3BandwidthIssueTextAll">This metric represents cycles fraction with demand load accesses that hit the L3 cache under loaded scenarios (possibly L3 Bandwidth limited).  Avoiding private cache misses (i.e. L2 misses/L3 hits) will improve the latency, reduce contention with sibling physical cores and increase performance. Note the value of this node may overlap with its siblings.</msg>
        <msg name="%SQFull">SQ Full</msg>
        <msg name="%SQFullDescriptionAll">This metric measures fraction of cycles where the Super Queue (SQ) was full taking into account all request-types and both hardware SMT threads. The Super Queue is used for requests to access the L2 cache or to go out to the Uncore.</msg>
        <msg name="%SQFullIssueTextAll">This metric measures fraction of cycles where the Super Queue (SQ) was full taking into account all request-types and both hardware SMT threads. The Super Queue is used for requests to access the L2 cache or to go out to the Uncore.</msg>
        <msg name="%StoreLatency">Store Latency</msg>
        <msg name="%StoreLatencyDescriptionAll">This metric represents cycles fraction the CPU spent handling long-latency store misses (missing 2nd level cache).</msg>
        <msg name="%StoreLatencyIssueTextAll">This metric represents a fraction of cycles the CPU spent handling long-latency store misses (missing the 2nd level cache). Consider avoiding/reducing unnecessary (or easily loadable/computable) memory store. Note that this metric value may be highlighted due to a Lock Latency issue.</msg>
        <msg name="%Other">Other</msg>
        <msg name="%LockLatency">Lock Latency</msg>
        <msg name="%LockLatencyDescriptionAll">This metric represents cycles fraction the CPU spent handling cache misses due to lock operations. Due to the microarchitecture handling of locks, they are classified as L1 Bound regardless of what memory source satisfied them.</msg>
        <msg name="%LockLatencyIssueTextAll">A significant fraction of CPU cycles spent handling cache misses due to lock operations. Due to the microarchitecture handling of locks, they are classified as L1 Bound regardless of what memory source satisfied them. Note that this metric value may be highlighted due to Store Latency issue.</msg>
        <msg name="%Port0">Port 0</msg>
        <msg name="%Port0DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 0 (SNB+: ALU; HSW+:ALU and 2nd branch)</msg>
        <msg name="%Port0IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 0 (SNB+: ALU; HSW+:ALU and 2nd branch)</msg>
        <msg name="%Port1">Port 1</msg>
        <msg name="%Port1DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 1 (ALU)</msg>
        <msg name="%Port1IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 1 (ALU)</msg>
        <msg name="%Port2">Port 2</msg>
        <msg name="%Port2DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 2 (Loads and Store-address)</msg>
        <msg name="%Port2IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 2 (Loads and Store-address)</msg>
        <msg name="%Port3">Port 3</msg>
        <msg name="%Port3DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 3 (Loads and Store-address)</msg>
        <msg name="%Port3IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 3 (Loads and Store-address)</msg>
        <msg name="%Port4">Port 4</msg>
        <msg name="%Port4DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 4 (Store-data)</msg>
        <msg name="%Port4IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 4 (Store-data). Note that this metric value may be highlighted due to Split Stores issue.</msg>
        <msg name="%Port5">Port 5</msg>
        <msg name="%Port5DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 5 (SNB+: Branches and ALU; HSW+: ALU)</msg>
        <msg name="%Port5IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 5 (SNB+: Branches and ALU; HSW+: ALU)</msg>
        <msg name="%Port6">Port 6</msg>
        <msg name="%Port6DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 6 (Branches and simple ALU)</msg>
        <msg name="%Port6IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 6 (Branches and simple ALU)</msg>
        <msg name="%Port7">Port 7</msg>
        <msg name="%Port7DescriptionAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 7 (simple Store-address)</msg>
        <msg name="%Port7IssueTextAll">This metric represents Core cycles fraction CPU dispatched uops on execution port 7 (simple Store-address)</msg>
        <msg name="%IntegerActive">Integer</msg>
        <msg name="%IntegerActiveDescriptionAll">This metric measures fraction of cycles spent executing integer arithmetic operations.</msg>
        <msg name="%IntegerActiveIssueTextAll">A significant fraction of CPU cycles spent executing integer arithmetic operations.</msg>
        <msg name="%FPActive">Floating Point</msg>
        <msg name="%FPActiveDescriptionAll">This metric measures fraction of cycles spent executing floating point arithmetic operations.</msg>
        <msg name="%FPActiveIssueTextAll">A significant fraction of CPU cycles spent executing floating point arithmetic operations.</msg>
        <msg name="%UTLBOverhead">UTLB Overhead</msg>
        <msg name="%UTLBOverheadDescriptionAll">This metric represents a fraction of cycles spent on handling first-level data TLB (or UTLB) misses. As with ordinary data caching, focus on improving data locality and reducing working-set size to reduce UTLB overhead. Additionally, consider using profile-guided optimization (PGO) to collocate frequently-used data on the same page. Try using larger page sizes for large amounts of frequently-used data. This metric does not include store TLB misses.</msg>
        <msg name="%UTLBOverheadIssueTextAll"> A significant proportion of cycles is being spent handling first-level data TLB misses.  As with ordinary data caching, focus on improving data locality and reducing working-set size to reduce UTLB overhead.  Additionally, consider using profile-guided optimization (PGO) to collocate frequently-used data on the same page.  Try using larger page sizes for large amounts of frequently-used data.</msg>
        <msg name="%ContestedAccessesKNL">Contested Accesses (Intra-Tile)</msg>
        <msg name="%ContestedAccessesKNLDescriptionAll">Contested accesses occur when data written by one thread is read by another thread on a different core.  Examples of contested accesses include synchronizations such as locks, true data sharing such as modified locked variables, and false sharing. Contested accesses metric is a ratio of the number of contested accesses to all demand loads and stores. This metrics only accounts for contested accesses between two cores on the same tile.</msg>
        <msg name="%SplitLoadsKNLDescriptionAll">Throughout the memory hierarchy, data moves at cache line granularity - 64 bytes per line.  Although this is much larger than many common data types, such as integer, float, or double, unaligned values of these or other types may span two cache lines. Split loads can often be result of unaligned vector loads which straddle across two cache lines. Split loads metric shows the fraction of demand loads that generated a split load.</msg>
        <msg name="%SplitLoadsKNLDescriptionAll">Throughout the memory hierarchy, data moves at cache line granularity - 64 bytes per line.  Although this is much larger than many common data types, such as integer, float, or double, unaligned values of these or other types may span two cache lines. Split stores metric shows the fraction of demand stores that generated a split store.</msg>
        <msg name="%LoadsBlockedbyStoreForwardingKNLDescriptionAll">To streamline memory operations in the pipeline, a load can avoid waiting for memory if a prior store, still in flight, is writing the data that the load wants to read (a 'store forwarding' process). However, in some cases, when the prior store is writing a smaller region than the load is reading, the load is blocked for a significant time pending the store forward.  This metric measures the fraction of demand loads that were blocked by store forwarding.</msg>
        <msg name="%LoadsBlockedbyStoreForwardingKNLIssueTextAll"> Loads are blocked during store forwarding for a significant proportion of loads. Use source/assembly view to identify the blocked loads, then identify the problematically-forwarded stores, which will typically be within the ten dynamic instructions prior to the load.  If the forwarding store is smaller than the load, change the store to be the same size as the load.</msg>
        <msg name="%VPUUtilization">VPU Utilization</msg>
        <msg name="%VPUUtilizationDescriptionAll">This metric measures the fraction of micro-ops that performed packed vector operations of any vector length and any mask. VPU utilization metric can be used in conjunction with the compiler's vectorization report to assess VPU utilization and to understand the compiler's judgement about the code. Note that this metric does not account for loads and stores and does not take into consideration vector length as well as masking. Includes integer packed simd.</msg>
        <msg name="%VPUUtilizationIssueTextAll">This metric measures the fraction of micro-ops that performed packed vector operations of any vector length and any mask. VPU utilization metric can be in conjunction with the compiler's vectorization report to assess VPU utilization and to understand the compiler's judgement about the code. Note that this metric does not account for loads and stores and does not take into consideration vector length as well as masking. This metric includes integer packed SIMD.</msg>
        <msg name="%MSAssistsKNLDescriptionAll">Certain corner-case operations cannot be handled natively by the execution pipeline and must be performed by the microcode sequencer (MS), where 1 or more uOps are issued.  The microcode sequencer performs microcode assists (small programs injected into the execution stream), inserting flows, and writing to the instruction queue (IQ). For example, when working with very small floating point values (so-called denormals), the floating-point units are not set up to perform these operations natively.  Instead, a sequence of instructions to perform the computation on the denormal is injected into the pipeline.  Since these microcode sequences might be hundreds of instructions long, these microcode assists are extremely detrimental to performance.</msg>
        <msg name="%MSAssistsKNLIssueTextAll"> A significant portion of retired uOps were issued by the micro-sequencer (MS). Examine the FP Assist metrics to determine the specific cause.</msg>
        <msg name="%FPAssistsKNLDescriptionAll">Certain floating point operations cannot be handled natively by the execution pipeline and must be performed by microcode (small programs injected into the execution stream).  For example, when working with very small floating point values (so-called denormals), the floating-point units are not set up to perform these operations natively. Instead, a sequence of instructions to perform the computation on the denormal is injected into the pipeline.  Since these microcode sequences might be hundreds of instructions long, these microcode assists are extremely detrimental to performance. This metric also accounts for other FP assists such as Flush-To-Zero (FTZ).</msg>
        <msg name="%FPAssistsKNLIssueTextAll">A significant portion of retired instructions is from floating point assists. Consider enabling the DAZ (Denormals Are Zero) and/or FTZ (Flush To Zero) options in your compiler to flush denormals to zero. This option may improve performance if the denormal values are not critical in your application. Also note that the DAZ and FTZ modes are not compatible with the IEEE Standard 754.</msg>
        <msg name="%LLCHitRate">LLC Hit Rate</msg>
        <msg name="%LLCHitRateDescriptionAll">The LLC (last-level cache) is the last, and longest-latency, level in the memory hierarchy before DRAM or MCDRAM.  While LLC hits are serviced much more quickly than hits in DRAM, they can still incur a significant performance penalty.  This metrics provides the ratio of demand load requests that hit in LLC to the total number of demand load requests serviced by LLC. This metric does not include instruction fetches.</msg>
        <msg name="%LLCHitRateIssueTextAll">The LLC (last-level cache) is the last, and longest-latency, level in the memory hierarchy before DRAM or MCDRAM.  While LLC hits are serviced much more quickly than hits in DRAM, they can still incur a significant performance penalty.  This metrics provides the ratio of demand load requests that hit in LLC to the total number of demand load requests serviced by LLC. This metric does not include instruction fetches.</msg>
        <msg name="%LLCHitRateKNL">L2 Hit Rate</msg>
        <msg name="%LLCHitRateKNLDescriptionAll">The L2 is the last and longest-latency level in the memory hierarchy before DRAM or MCDRAM.  While L2 hits are serviced much more quickly than hits in DRAM or MCDRAM, they can still incur a significant performance penalty. This metric provides a ratio of the demand load requests that hit the L2 to the total number of the demand load requests serviced by the L2. This metric does not include instruction fetches.</msg>
        <msg name="%LLCHitRateKNLIssueTextAll">The L2 is the last and longest-latency level in the memory hierarchy before DRAM or MCDRAM.  While L2 hits are serviced much more quickly than hits in DRAM, they can still incur a significant performance penalty. This metric provides the ratio of demand load requests that hit the L2 to the total number of the demand load requests serviced by the L2. This metric does not include instruction fetches.</msg>
        <msg name="%BACLEARS">BACLEARS</msg>
        <msg name="%BACLEARSDescriptionAll">This metric estimates a fraction of cycles lost due to the Branch Target Buffer (BTB) prediction corrected by a later branch predictor.</msg>
        <msg name="%BACLEARSIssueTextAll">A significant number of CPU cycles lost due to the Branch Target Buffer (BTB) prediction corrected by a later branch predictor. Consider reducing the amount of taken branches.</msg>
        <msg name="%MSEntry">MS Entry</msg>
        <msg name="%MSEntryDescriptionAll">This metric estimates a fraction of cycles lost due to the Microcode Sequencer entry.</msg>
        <msg name="%MSEntryIssueTextAll">A significant number of CPU cycles lost due to the Microcode Sequencer entry.</msg>
        <msg name="%ICacheLineFetch">ICache Line Fetch</msg>
        <msg name="%ICacheLineFetchDescriptionAll">This metric estimates a fraction of cycles lost due to the instruction cacheline fetching.</msg>
        <msg name="%ICacheLineFetchIssueTextAll">A significant number of CPU cycles lost due to the instruction cacheline fetching.</msg>
        <msg name="%PreDecodeWrong">Pre-Decode Wrong</msg>
        <msg name="%PreDecodeWrongDescriptionAll">This metric estimates a fraction of cycles lost due to the decoder predicting wrong instruction length.</msg>
        <msg name="%PreDecodeWrongIssueTextAll">A significant number of CPU cycles lost due to the decoder predicting wrong instruction length.</msg>
        <msg name="%L1HitRate">L1 Hit Rate</msg>
        <msg name="%L1HitRateDescriptionAll">The L1 cache is the first, and shortest-latency, level in the memory hierarchy. This metric  provides the ratio of demand load requests that hit the L1 cache to the total number of demand load requests.</msg>
        <msg name="%L1HitRateIssueTextAll">The L1 cache is the first, and shortest-latency, level in the memory hierarchy. This metric  provides the ratio of demand load requests that hit the L1 cache to the total number of demand load requests.</msg>
        <msg name="%SIMDComputeToL1AccessRatio">SIMD Compute-to-L1 Access Ratio</msg>
        <msg name="%SIMDComputeToL1AccessRatioDescriptionAll">This metric provides the ratio of SIMD compute instructions to the total number of memory loads, each of which will first access the L1 cache. On this platform, it is important that this ratio is large to ensure efficient usage of compute resources.</msg>
        <msg name="%SIMDComputeToL1AccessRatioIssueTextAll">This metric provides the ratio of SIMD compute instructions to the total number of memory loads, each of which will first access the L1 cache. On this platform, it is important that this ratio is large to ensure efficient usage of compute resources.</msg>
        <msg name="%SIMDComputeToL2AccessRatio">SIMD Compute-to-L2 Access Ratio</msg>
        <msg name="%SIMDComputeToL2AccessRatioDescriptionAll">This metric provides the ratio of SIMD compute instructions to the total number of memory loads that hit the L2 cache. On this platform, it is important that this ratio is large to ensure efficient usage of compute resources.</msg>
        <msg name="%SIMDComputeToL2AccessRatioIssueTextAll">This metric provides the ratio of SIMD compute instructions to the total number of memory loads that hit the L2 cache. On this platform, it is important that this ratio is large to ensure efficient usage of compute resources.</msg>
        <msg name="%PipelineSlotsForUnits">of Pipeline Slots</msg>
        <msg name="%PipelineSlotsDescription">A pipeline slot represents hardware resources needed to process one uOp.</msg>
        <msg name="%UOPSForUnits">of uOps</msg>
        <msg name="%UOPSDescription">uOp, or micro-op, is a low-level hardware operation. The CPU Front-End is responsible for fetching the program code represented in architectural instructions and decoding them into one or more uOps.</msg>
        <msg name="%ClksForUnit">of Clockticks</msg>
        <msg name="%ElapsedTimeForUnit">of Elapsed Time</msg>
        <msg name="%SPFPForUnits">from SP FP</msg>
        <msg name="%DPFPForUnits">from DP FP</msg>
        <msg name="%PackedFPOpsForUnits">of Packed FP Operations</msg>
        <msg name='%HTisONMetric'>Due to hardware limitations the metric is not available on this platform when Intel Hyper-Threading Technology is on. Consider disabling the Hyper-Threading option in the BIOS and re-runnning the analysis.</msg>
        <msg name="%DSB_Coverage">(Info) DSB Coverage</msg>
        <msg name="%DSB_CoverageDescriptionAll">Fraction of uOps delivered by the DSB (known as Decoded ICache or uOp Cache).</msg>
        <msg name="%DSB_CoverageIssueTextAll"><![CDATA[ <p><strong>Issue:</strong> A significant fraction of uOps was not delivered by the DSB (known as Decoded ICache or uOp Cache). This may happen if a hot code region is too large to fit into the DSB.</p><p><strong>Tips:</strong> Consider changing the code layout (for example, via profile-guided optimization) to help your hot regions fit into the DSB.</p><p>See the "Optimization for Decoded ICache" section in the Intel 64 and IA-32 Architectures Optimization Reference Manual.</p> ]]></msg>
        <msg name="%LSD_Coverage">(Info) LSD Coverage</msg>
        <msg name="%LSD_CoverageDescriptionAll">Fraction of uOps delivered by the LSD (Loop Stream Detector or Loop Cache)</msg>
        <msg name="%Mispredicts_Resteers">Mispredicts Resteers</msg>
        <msg name="%Mispredicts_ResteersDescriptionAll">This metric measures the fraction of cycles the CPU was stalled due to Branch Resteers as a result of Branch Misprediction at execution stage.</msg>
        <msg name="%Mispredicts_ResteersIssueTextAll">A significant fraction of cycles could be stalled due to Branch Resteers as a result of Branch Misprediction at execution stage.</msg>
        <msg name="%Clears_Resteers">Clears Resteers</msg>
        <msg name="%Clears_ResteersDescriptionAll">This metric measures the fraction of cycles the CPU was stalled due to Branch Resteers as a result of Machine Clears.</msg>
        <msg name="%Clears_ResteersIssueTextAll">A significant fraction of cycles could be stalled due to Branch Resteers as a result of Machine Clears.</msg>
        <msg name="%Unknown_Branches">Unknown Branches</msg>
        <msg name="%Unknown_BranchesDescriptionAll">This metric measures the fraction of cycles the CPU was stalled due to new branch address clears. These are fetched branches the Branch Prediction Unit was unable to recognize (First fetch or hitting BPU capacity limit).</msg>
        <msg name="%Unknown_BranchesIssueTextAll">A significant fraction of cycles could be stalled due to new branch address clears. These are fetched branches the Branch Prediction Unit was unable to recognize (First fetch or hitting BPU capacity limit).</msg>
        <msg name="%Store_L2">Store L2</msg>
        <msg name="%Store_L2DescriptionAll">This metric roughly estimates cycles handling demand stores accesses served by the L2 cache.</msg>
        <msg name="%Store_L2IssueTextAll">A significant fraction of cycles spent handling demand stores accesses served by the L2 cache.</msg>
        <msg name="%Store_L3">Store L3</msg>
        <msg name="%Store_L3DescriptionAll">This metric roughly estimates cycles handling demand stores accesses served by the L3 cache.</msg>
        <msg name="%Store_L3IssueTextAll">A significant fraction of cycles spent handling demand stores accesses served by the L3 cache.</msg>
        <msg name="%Store_MEM">Store Memory</msg>
        <msg name="%Store_MEMDescriptionAll">This metric roughly estimates cycles handling demand stores accesses served by external memory.</msg>
        <msg name="%Store_MEMIssueTextAll">A significant fraction of cycles spent handling demand stores accesses served by external memory.</msg>
        <msg name="%uTubeDescription"><![CDATA[ This diagram represents inefficiencies in CPU usage. Treat it as a pipe with an output flow equal to the "pipe efficiency" ratio: (Actual Instructions Retired)/(Maximum Possible <a web-link="https://software.intel.com/en-us/vtune-amplifier-help-instructions-retired-event" cli="Instruction Retired">Instruction Retired</a>). If there are pipeline stalls decreasing the pipe efficiency, the pipe shape gets more narrow. ]]></msg>
        <msg name="%uTube">ÂµPipe</msg>
        <msg name="%uTubeTooltip">This part of ÂµPipe is fraction of</msg>
        <msg name="%uTubeNotValidData">ÂµPipe cannot be shown due to multiplexing issues or a low number of collected PMU samples.</msg>
        <msg name="%Local_PMMIssueTextAll">Significant percentage of cycles is being spent on demand loads from the local Intel Optane DC Persistent Memory. Consider caching data to improve the latency and increase the performance.</msg>
        <msg name="%Remote_PMMIssueTextAll">Significant percentage of cycles is being spent on demand loads from the remote Intel Optane DC Persistent Memory. Wherever possible, try to consistently use data on the same core, or at least the same package, as it was allocated on.</msg>
        <msg name="%FPDIV">FP Divide</msg>
        <msg name="%FPDIVDescriptionAll">This metric measures the number of floating point divide uops retired (x87 and SSE, including x87 sqrt).</msg>
        <msg name="%FEBandwidthCisc">Cisc</msg>
        <msg name="%FEBandwidthCiscDescriptionAll">This metric measures the number of issue slots every cycle that were not delivered by the frontend due to microcode sequencer (MS).</msg>
        <msg name="%FEBandwidthDecode">Decode</msg>
        <msg name="%FEBandwidthDecodeDescriptionAll">This metric measures the number of issue slots every cycle that were not delivered by the frontend due to decode stall.</msg>
        <msg name="%FEBandwidthOther">Other</msg>
        <msg name="%FEBandwidthOtherDescriptionAll">This metric measures the number of issue slots every cycle that were not delivered by the frontend and are not attributed to other common frontend stalls.</msg>
        <msg name="%MachineClearsNuke">Machine Clear</msg>
        <msg name="%MachineClearsNukeDescriptionAll">This metric measures the number of issue slots every cycle that were not consumed by the backend due to a machine clear.</msg>
        <msg name="%ResourceBound">Resource Bound</msg>
        <msg name="%ResourceBoundDescriptionAll"></msg>
        <msg name="%MemoryScheduler">Memory Scheduler</msg>
        <msg name="%MemorySchedulerDescriptionAll">This metric measures the number of issue slots every cycle that were not consumed by the backend due to memory reservation stall.</msg>
        <msg name="%NonMemoryScheduler">Non-memory Scheduler</msg>
        <msg name="%NonMemorySchedulerDescriptionAll">This metric measures the number of issue slots every cycle that were not consumed by the backend due to IEC and FPC RAT stalls. This can be caused by the FIQ and IEC reservation station stall (integer, FP and SIMD scheduler not able to accept another uop).</msg>
        <msg name="%BERegister">Register</msg>
        <msg name="%BERegisterDescriptionAll">This metric measures the number of issue slots every cycle that were not consumed by the backend due to MRBL stall.</msg>
        <msg name="%BEReorderBuffer">Full Re-order Buffer (ROB)</msg>
        <msg name="%BEReorderBufferDescriptionAll">This metric measures the number of issue slots every cycle that were not consumed by the backend due to ROB full.</msg>
        <msg name="%BEStoreBuffer">Store Buffer stalls</msg>
        <msg name="%BEStoreBufferDescriptionAll">This metric measures the number of issue slots every cycle that were not consumed by the backend due to store buffers stalls.</msg>
        <msg name="%BEAllocRestriction">Allocation Restriction</msg>
        <msg name="%BEAllocRestrictionDescriptionAll">This metric measures the number of issue slots every cycle that were not consumed by the backend due to an allocation restriction.</msg>
        <msg name="%ALU_Op_Utilization">ALU Operation Utilization</msg>
        <msg name="%ALU_Op_UtilizationDescriptionAll">This metric represents Core fraction of cycles CPU dispatched uops on execution ports for ALU operations.</msg>
        <msg name="%ALU_Op_UtilizationIssueTextAll">CPU dispatched uops on execution ports for ALU operations during significant fraction of Core cycles.</msg>
        <msg name="%Load_Op_Utilization">Load Operation Utilization</msg>
        <msg name="%Load_Op_UtilizationDescriptionAll">This metric represents Core fraction of cycles CPU dispatched uops on execution port for Load operations.</msg>
        <msg name="%Load_Op_UtilizationIssueTextAll">CPU dispatched uops on execution ports for Load operations during significant fraction of Core cycles.</msg>
        <msg name="%Store_Op_Utilization">Store Operation Utilization</msg>
        <msg name="%Store_Op_UtilizationDescriptionAll">This metric represents Core fraction of cycles CPU dispatched uops on execution port for Store operations.</msg>
        <msg name="%Store_Op_UtilizationIssueTextAll">CPU dispatched uops on execution ports for Store operations during significant fraction of Core cycles.</msg>
        <msg name="%Port_Store_Address">Port Store Address</msg>
        <msg name="%Port_Store_AddressDescriptionAll">Port Store Address</msg>
        <msg name="%Port_Store_AddressIssueTextAll">Port Store Address</msg>
        <msg name="%Port_Store_Data">Port Store Data</msg>
        <msg name="%Port_Store_DataDescriptionAll">Port Store Data</msg>
        <msg name="%Port_Store_DataIssueTextAll">Port Store Data</msg>
        <msg name="%Load_STLB_Hit">Load STLB Hit</msg>
        <msg name="%Load_STLB_HitDescriptionAll">This metric roughly estimates the fraction of cycles where the (first level) DTLB was missed by load accesses, that later on hit in second-level TLB (STLB).</msg>
        <msg name="%Load_STLB_HitIssueTextAll">In significant fraction of cycles the (first level) DTLB was missed by load accesses, that later on hit in second-level TLB (STLB).</msg>
        <msg name="%Load_STLB_Miss">Load STLB Miss</msg>
        <msg name="%Load_STLB_MissDescriptionAll">This metric estimates the fraction of cycles where the Second-level TLB (STLB) was missed by load accesses, performing a hardware page walk.</msg>
        <msg name="%Load_STLB_MissIssueTextAll">In significant fraction of cycles the Second-level TLB (STLB) was missed by load accesses, performing a hardware page walk.</msg>
        <msg name="%Store_STLB_Hit">Store STLB Hit</msg>
        <msg name="%Store_STLB_HitDescriptionAll">This metric roughly estimates the fraction of cycles where the TLB was missed by store accesses, hitting in the second-level TLB (STLB).</msg>
        <msg name="%Store_STLB_HitIssueTextAll">In significant fraction of cycles where the TLB was missed by store accesses, hitting in the second-level TLB (STLB).</msg>
        <msg name="%Store_STLB_Miss">Store STLB Hit</msg>
        <msg name="%Store_STLB_MissDescriptionAll">This metric estimates the fraction of cycles where the STLB was missed by store accesses, performing a hardware page walk.</msg>
        <msg name="%Store_STLB_MissIssueTextAll">In significant fraction of cycles where the STLB was missed by store accesses, performing a hardware page walk.</msg>
    </catalog>
</xmc>
